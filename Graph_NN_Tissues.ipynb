{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eykUpghIF_1y"
      },
      "source": [
        "# Homework 3 Graph Neural Networks (GNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qz3Y9MSzF_10"
      },
      "source": [
        "Welcome to the Graph Neural Networks (GNN) programming assignment! In this assignment, you will have the opportunity to dive into the exciting world of graph analysis using deep learning techniques. Graphs are ubiquitous in the real world, and GNNs provide a powerful tool to analyze and learn from them. The objective of this assignment is to familiarize you with the basics of GNNs and to give you hands-on experience implementing them using popular deep learning frameworks. By the end of this assignment, you will have a solid understanding of how GNNs work, how they can be used to analyze graphs, and how to implement them in practice. Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfSec4EnF_10"
      },
      "source": [
        "## Environment\n",
        "To running this code, you have to prepare a python environment. We recommend python version >= 3.7. You will need to install `torch`, `torch_geometric`, `torch_scatter`, `torch_sparse`, `torchmetrics`, `networkx`, `numpy` and `jupyter` packages. And in order to run the tests locally, you also need to install `matplotlib`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXnTSOUSF_11",
        "outputId": "f59fb7bd-c3eb-4083-99f4-bef3ba072a66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.0+cu118\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install required packages.\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "!pip install -q torchmetrics\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_HNkeDnF_12"
      },
      "source": [
        "Graph Neural Networks (GNNs) have become a popular research field in the deep learning community, with a focus on applying deep learning concepts to irregular structured data found in graphs. The aim is to enable neural networks to reason about objects and their relationships. This is accomplished through a neural message passing scheme where node features are iteratively updated by gathering information from their neighbors. The tutorial will cover fundamental concepts in deep learning on graphs using GNNs and will use the PyTorch Geometric (PyG) library, which is an extension of the PyTorch framework and offers various methods and utilities to simplify GNN implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfeCQ2ggF_12"
      },
      "source": [
        "We will start with the **PPI** dataset. The protein-protein interaction networks from the [Predicting Multicellular Function through Multi-layer Tissue Networks](https://arxiv.org/abs/1707.04638) paper, containing positional gene sets, motif gene sets and immunological signatures as features (50 in total) and gene ontology sets as labels (121 in total).\n",
        "\n",
        "The task for PPI is Node Classification, and this is a [multi-label classification](https://en.wikipedia.org/wiki/Multi-label_classification) task. Multi-label classification is a type of classification task where each instance can be assigned to one or more classes. In contrast, in single-label classification, each instance can be assigned to only one class. In multi-label classification, the number of classes is not fixed and can vary from instance to instance. The PPI dataset is an example of a multi-label classification task where each instance can be assigned to one or more of the 121 classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hi_gzJS0F_13",
        "outputId": "dd370990-10a5-4254-cc74-3f198998a4ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://data.dgl.ai/dataset/ppi.zip\n",
            "Extracting data/ppi.zip\n",
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset: PPI(20):\n",
            "Number of graphs: 20\n",
            "Number of features: 50\n",
            "Number of classes: 121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.datasets import PPI\n",
        "\n",
        "# You can choose whether to normalize the node features\n",
        "train_dataset = PPI(root='data', split='train')\n",
        "val_dataset = PPI(root='data', split='val')\n",
        "print(f'Train Dataset: {train_dataset}:')\n",
        "print(f'Number of graphs: {len(train_dataset)}')\n",
        "print(f'Number of features: {train_dataset.num_features}')\n",
        "print(f'Number of classes: {train_dataset.num_classes}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDJQKVHUF_13"
      },
      "source": [
        "We can see that the train set has 20 graphs. Every node have 50 features. We can take a close look at the first graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztGpbFjkF_13",
        "outputId": "93088f4b-a641-48c6-d0b4-aa9456318a96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data(x=[1767, 50], edge_index=[2, 32318], y=[1767, 121])\n",
            "Number of nodes: 1767\n",
            "Number of edges: 32318\n",
            "Average node degree: 18.29\n",
            "Is undirected: True\n"
          ]
        }
      ],
      "source": [
        "data = train_dataset[0]  # Get the first graph object.\n",
        "\n",
        "print(data)\n",
        "\n",
        "# Gather some statistics about the graph.\n",
        "print(f'Number of nodes: {data.num_nodes}')\n",
        "print(f'Number of edges: {data.num_edges}')\n",
        "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Is undirected: {data.is_undirected()}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF397cceF_14"
      },
      "source": [
        "In PyTorch Geometric, each graph is represented by a `Data` object, which contains all the necessary information to describe the graph. By calling `print(data)`, we can obtain a summary of the object's attributes and shapes. The data object has three attributes: (1)`edge_index`, which provides the graph connectivity information as a tuple of source and destination node indices for each edge, (2) `x`, which represents node features as a matrix (tensor) where each row corresponds to a node and each column represents a feature dimension, (3) `y`, which contains node labels, where each node is assigned to 121 classes. The goal is to infer the community assignment for the remaining nodes. The Data object also offers utility functions that allow us to check basic properties of the graph, such as whether there are isolated nodes, self-loops, or whether the graph is undirected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Bmz2Ne2F_14",
        "outputId": "017b7d45-e287-4a2f-9ac3-a02de7fe82c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0.])\n"
          ]
        }
      ],
      "source": [
        "print(data.y[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LREKVa1F_14"
      },
      "source": [
        "Here we can see that for this multi-label classification, one data point can belong to multiple classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZIEpMRdF_14"
      },
      "source": [
        "We can visualize the graph using networkx."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "id": "wn5z397tF_15",
        "outputId": "015580d1-9da2-4c50-da3a-211efa5d0a73"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import networkx as nx\n",
        "\n",
        "from torch_geometric.utils import to_networkx\n",
        "\n",
        "G = to_networkx(data, to_undirected=True)\n",
        "\n",
        "def visualize_graph(G, color):\n",
        "    plt.figure(figsize=(7,7))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    nx.draw_networkx(G, pos=nx.spring_layout(G, seed=21), with_labels=False,\n",
        "                     node_color=color, cmap=\"Set2\")\n",
        "    plt.show()\n",
        "# We random choose a class if there are more than 1 class for a data point\n",
        "visualize_graph(G, color=torch.argmax(data.y, dim=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeOXxctiF_15"
      },
      "source": [
        "In some classification datasets, the graphs are usually small. Thus, it is a good practice to batch these graphs before feeding them into a Graph Neural Network to ensure that the CPU/GPU is fully utilized. In other domains such as image or language, this is typically done by resizing or padding each example to have the same shape, and then grouping examples in an additional dimension with a length equal to the number of examples in a mini-batch, referred to as the batch_size. However, these approaches are not feasible or may lead to unnecessary memory consumption in GNNs. PyTorch Geometric takes a different approach to achieve parallelization across multiple examples. Here, adjacency matrices are stacked diagonally, creating a single graph that contains multiple isolated subgraphs. Node and target features are then concatenated in the node dimension.\n",
        "\n",
        "The procedure mentioned above offers several crucial benefits over other batching procedures:\n",
        "\n",
        "GNN operators that use message passing do not require any modifications since messages are not exchanged between nodes belonging to different graphs.\n",
        "There is no computational or memory overhead as adjacency matrices are stored sparsely, containing only non-zero entries, i.e., the edges.\n",
        "\n",
        "PyTorch Geometric automatically performs the task of batching multiple graphs into a single large graph using the `torch_geometric.data.DataLoader` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDDxfwVnF_15",
        "outputId": "790b016e-b059-4249-8baf-ecae4e904457"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1:\n",
            "=======\n",
            "Number of graphs in the current batch: 8\n",
            "DataBatch(x=[20502, 50], edge_index=[2, 598262], y=[20502, 121], batch=[20502], ptr=[9])\n",
            "\n",
            "Step 2:\n",
            "=======\n",
            "Number of graphs in the current batch: 8\n",
            "DataBatch(x=[16153, 50], edge_index=[2, 424728], y=[16153, 121], batch=[16153], ptr=[9])\n",
            "\n",
            "Step 3:\n",
            "=======\n",
            "Number of graphs in the current batch: 4\n",
            "DataBatch(x=[8251, 50], edge_index=[2, 203378], y=[8251, 121], batch=[8251], ptr=[5])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "batch_size = 8\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "for step, data in enumerate(train_loader):\n",
        "    print(f'Step {step + 1}:')\n",
        "    print('=======')\n",
        "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
        "    print(data)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk5zG71nF_16"
      },
      "source": [
        "In principle, we can determine the category of a protein only based on its features representation without considering any relational information. We can confirm this by building a basic MLP that works only on input node features, using shared weights across all nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFW4gpu6F_16"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super().__init__()\n",
        "        self.lin1 = Linear(train_dataset.num_features, hidden_channels)\n",
        "        self.lin2 = Linear(hidden_channels, train_dataset.num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lin1(x)\n",
        "        x = x.relu()\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzGJAJFOGoex",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c4319a0-426e-4bc0-bc60-0ae50ae612d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjEE7hDRF_16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4023bc55-36fb-41bd-cd2c-4db98363a8f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP(\n",
            "  (lin1): Linear(in_features=50, out_features=16, bias=True)\n",
            "  (lin2): Linear(in_features=16, out_features=121, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = MLP(hidden_channels=16).to(device)\n",
        "print(model)\n",
        "# loss_fn = torch.nn.CrossEntropyLoss()  # Define loss loss_fn.\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)  # Define optimizer.\n",
        "\n",
        "def train():\n",
        "      model.train()\n",
        "      optimizer.zero_grad()  # Clear gradients.\n",
        "      for data in train_loader:\n",
        "            out = model(data.x.to(device))  # Perform a single forward pass.\n",
        "            loss = loss_fn(out, data.y.to(device))  # Compute the loss solely based on the training nodes.\n",
        "            loss.backward()  # Derive gradients.\n",
        "            optimizer.step()  # Update parameters based on gradients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEQGBENFF_17"
      },
      "source": [
        "## Task 1: Compute the ROC AUC score [2 pts]\n",
        "To evaluate the performance of our model on the PPI dataset, we will use the [ROC AUC score](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc). \n",
        "\n",
        "ROC (Receiver Operating Characteristic) AUC (Area Under the Curve) is a performance metric for binary classification problems that measures the area under the ROC curve. The ROC curve is a plot of True Positive Rate (TPR) against False Positive Rate (FPR) at different classification thresholds. A perfect classifier has a ROC AUC score of 1, while a random classifier has a ROC AUC score of 0.5.\n",
        "\n",
        "In the case of multi-label classification tasks like PPI, there are two types of averaging methods that can be used to compute the ROC AUC score:\n",
        "\n",
        "1. **Macro-average ROC AUC**: computes the ROC AUC score for each class separately and then takes the average of the scores. It treats all classes equally regardless of their frequency in the dataset.\n",
        "\n",
        "2. **Micro-average ROC AUC**: computes the ROC AUC score globally by counting the total true positives, false positives, and true negatives across all classes.\n",
        "\n",
        "In our case, since the PPI dataset is imbalanced, we will use the macro-average ROC AUC score to evaluate the performance of our model.\n",
        "\n",
        "You need to complete the `mlp_test` function to compute the ROC AUC score. You can use `MultilabelAUROC` from `torchmetrics` to compute the score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISd9qYPEF_17"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "from torchmetrics.classification import MultilabelAUROC\n",
        "import torch\n",
        "### DO NOT CHANGE ANY CODE ABOVE THIS LINE IN THIS CELL ###\n",
        "\n",
        "def mlp_test(model, loss_fn, loader, device):\n",
        "      \"\"\"\n",
        "      model: pytorch GNN model\n",
        "      loss_fn: loss function\n",
        "      loader: DataLoader\n",
        "      device: device eused to bind the model and tensor\n",
        "      \"\"\"\n",
        "      model.eval()\n",
        "      total_loss = 0.0\n",
        "      auroc_score = 0.0\n",
        "      total_nodes_n = 0\n",
        "      # TODO: calculate the auroc_score using marco average\n",
        "      auroc = MultilabelAUROC(num_labels=121)\n",
        "      \n",
        "      with torch.no_grad():\n",
        "            for data in loader:\n",
        "                  data.x, data.y = data.x.to(device), data.y.to(device)\n",
        "                  out = model(data.x)\n",
        "                  loss = loss_fn(out, data.y)\n",
        "                  total_loss += loss * data.x.shape[0]\n",
        "                  total_nodes_n += data.x.shape[0]\n",
        "                  auroc.update(out, data.y.to(torch.long))\n",
        "            \n",
        "      auroc_score = auroc.compute().mean().item()            \n",
        "      return total_loss / total_nodes_n, auroc_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXBoG67UF_17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "593731af-0772-481e-d817-1423d2425847"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Train Loss: 0.5423, Train auroc: 0.5855, Valid Loss: 0.5350, Valid Auroc: 0.5790\n",
            "Epoch: 002, Train Loss: 0.5422, Train auroc: 0.5854, Valid Loss: 0.5350, Valid Auroc: 0.5787\n",
            "Epoch: 003, Train Loss: 0.5422, Train auroc: 0.5853, Valid Loss: 0.5348, Valid Auroc: 0.5783\n",
            "Epoch: 004, Train Loss: 0.5421, Train auroc: 0.5849, Valid Loss: 0.5346, Valid Auroc: 0.5779\n",
            "Epoch: 005, Train Loss: 0.5422, Train auroc: 0.5847, Valid Loss: 0.5350, Valid Auroc: 0.5777\n",
            "Epoch: 006, Train Loss: 0.5423, Train auroc: 0.5844, Valid Loss: 0.5352, Valid Auroc: 0.5778\n",
            "Epoch: 007, Train Loss: 0.5423, Train auroc: 0.5848, Valid Loss: 0.5349, Valid Auroc: 0.5783\n",
            "Epoch: 008, Train Loss: 0.5422, Train auroc: 0.5852, Valid Loss: 0.5347, Valid Auroc: 0.5786\n",
            "Epoch: 009, Train Loss: 0.5421, Train auroc: 0.5851, Valid Loss: 0.5347, Valid Auroc: 0.5784\n",
            "Epoch: 010, Train Loss: 0.5421, Train auroc: 0.5853, Valid Loss: 0.5348, Valid Auroc: 0.5786\n",
            "Epoch: 011, Train Loss: 0.5421, Train auroc: 0.5852, Valid Loss: 0.5349, Valid Auroc: 0.5786\n",
            "Epoch: 012, Train Loss: 0.5422, Train auroc: 0.5856, Valid Loss: 0.5348, Valid Auroc: 0.5788\n",
            "Epoch: 013, Train Loss: 0.5422, Train auroc: 0.5859, Valid Loss: 0.5349, Valid Auroc: 0.5790\n",
            "Epoch: 014, Train Loss: 0.5421, Train auroc: 0.5858, Valid Loss: 0.5347, Valid Auroc: 0.5790\n",
            "Epoch: 015, Train Loss: 0.5420, Train auroc: 0.5853, Valid Loss: 0.5346, Valid Auroc: 0.5787\n",
            "Epoch: 016, Train Loss: 0.5420, Train auroc: 0.5857, Valid Loss: 0.5345, Valid Auroc: 0.5792\n",
            "Epoch: 017, Train Loss: 0.5420, Train auroc: 0.5865, Valid Loss: 0.5347, Valid Auroc: 0.5798\n",
            "Epoch: 018, Train Loss: 0.5421, Train auroc: 0.5863, Valid Loss: 0.5348, Valid Auroc: 0.5796\n",
            "Epoch: 019, Train Loss: 0.5420, Train auroc: 0.5858, Valid Loss: 0.5347, Valid Auroc: 0.5791\n",
            "Epoch: 020, Train Loss: 0.5420, Train auroc: 0.5856, Valid Loss: 0.5346, Valid Auroc: 0.5791\n",
            "Epoch: 021, Train Loss: 0.5420, Train auroc: 0.5857, Valid Loss: 0.5345, Valid Auroc: 0.5794\n",
            "Epoch: 022, Train Loss: 0.5421, Train auroc: 0.5860, Valid Loss: 0.5347, Valid Auroc: 0.5796\n",
            "Epoch: 023, Train Loss: 0.5420, Train auroc: 0.5867, Valid Loss: 0.5347, Valid Auroc: 0.5800\n",
            "Epoch: 024, Train Loss: 0.5419, Train auroc: 0.5869, Valid Loss: 0.5346, Valid Auroc: 0.5801\n",
            "Epoch: 025, Train Loss: 0.5419, Train auroc: 0.5864, Valid Loss: 0.5346, Valid Auroc: 0.5795\n",
            "Epoch: 026, Train Loss: 0.5419, Train auroc: 0.5861, Valid Loss: 0.5347, Valid Auroc: 0.5793\n",
            "Epoch: 027, Train Loss: 0.5420, Train auroc: 0.5862, Valid Loss: 0.5347, Valid Auroc: 0.5796\n",
            "Epoch: 028, Train Loss: 0.5420, Train auroc: 0.5860, Valid Loss: 0.5347, Valid Auroc: 0.5796\n",
            "Epoch: 029, Train Loss: 0.5420, Train auroc: 0.5861, Valid Loss: 0.5346, Valid Auroc: 0.5799\n",
            "Epoch: 030, Train Loss: 0.5418, Train auroc: 0.5861, Valid Loss: 0.5343, Valid Auroc: 0.5799\n",
            "Epoch: 031, Train Loss: 0.5418, Train auroc: 0.5863, Valid Loss: 0.5343, Valid Auroc: 0.5800\n",
            "Epoch: 032, Train Loss: 0.5420, Train auroc: 0.5865, Valid Loss: 0.5347, Valid Auroc: 0.5800\n",
            "Epoch: 033, Train Loss: 0.5420, Train auroc: 0.5866, Valid Loss: 0.5348, Valid Auroc: 0.5800\n",
            "Epoch: 034, Train Loss: 0.5422, Train auroc: 0.5861, Valid Loss: 0.5352, Valid Auroc: 0.5795\n",
            "Epoch: 035, Train Loss: 0.5419, Train auroc: 0.5860, Valid Loss: 0.5348, Valid Auroc: 0.5792\n",
            "Epoch: 036, Train Loss: 0.5417, Train auroc: 0.5861, Valid Loss: 0.5341, Valid Auroc: 0.5794\n",
            "Epoch: 037, Train Loss: 0.5418, Train auroc: 0.5871, Valid Loss: 0.5342, Valid Auroc: 0.5805\n",
            "Epoch: 038, Train Loss: 0.5421, Train auroc: 0.5869, Valid Loss: 0.5349, Valid Auroc: 0.5806\n",
            "Epoch: 039, Train Loss: 0.5423, Train auroc: 0.5864, Valid Loss: 0.5354, Valid Auroc: 0.5800\n",
            "Epoch: 040, Train Loss: 0.5419, Train auroc: 0.5867, Valid Loss: 0.5346, Valid Auroc: 0.5799\n",
            "Epoch: 041, Train Loss: 0.5417, Train auroc: 0.5868, Valid Loss: 0.5339, Valid Auroc: 0.5796\n",
            "Epoch: 042, Train Loss: 0.5419, Train auroc: 0.5871, Valid Loss: 0.5343, Valid Auroc: 0.5799\n",
            "Epoch: 043, Train Loss: 0.5422, Train auroc: 0.5870, Valid Loss: 0.5351, Valid Auroc: 0.5801\n",
            "Epoch: 044, Train Loss: 0.5421, Train auroc: 0.5869, Valid Loss: 0.5351, Valid Auroc: 0.5804\n",
            "Epoch: 045, Train Loss: 0.5418, Train auroc: 0.5864, Valid Loss: 0.5344, Valid Auroc: 0.5802\n",
            "Epoch: 046, Train Loss: 0.5416, Train auroc: 0.5862, Valid Loss: 0.5339, Valid Auroc: 0.5798\n",
            "Epoch: 047, Train Loss: 0.5418, Train auroc: 0.5860, Valid Loss: 0.5342, Valid Auroc: 0.5795\n",
            "Epoch: 048, Train Loss: 0.5421, Train auroc: 0.5864, Valid Loss: 0.5349, Valid Auroc: 0.5796\n",
            "Epoch: 049, Train Loss: 0.5422, Train auroc: 0.5864, Valid Loss: 0.5352, Valid Auroc: 0.5797\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import Javascript  # Restrict height of output cell.\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "# You can tune the hyperparameters like epoch_num, learning_rate, ...\n",
        "epoch_num = 50\n",
        "for epoch in range(1, epoch_num):\n",
        "    train()\n",
        "    train_loss, train_auroc = mlp_test(model, loss_fn, train_loader, device)\n",
        "    val_loss, val_auroc = mlp_test(model, loss_fn, val_loader, device)\n",
        "    print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Train auroc: {train_auroc:.4f}, Valid Loss: {val_loss:.4f}, Valid Auroc: {val_auroc:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvI5XTLGF_17"
      },
      "source": [
        "Although we also let the loss drop here, we did not use the structural relationship of the graph. Obviously we lost a lot of information. If we can make good use of these meaningful information, then our loss can be further reduced."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Abv4qEoyF_18"
      },
      "source": [
        "To convert the MLP to a GNN, we can swap the torch.nn.Linear layers with PyG's GNN operators. We can replace the linear layers with the GCNConv module.\n",
        "\n",
        "The GCN layer (Kipf et al. (2017)) is defined as:\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_v^{(\\ell + 1)} = \\mathbf{W}^{(\\ell + 1)} \\sum_{w \\in \\mathcal{N}(v) \\, \\cup \\, \\{ v \\}} \\frac{1}{c_{w,v}} \\cdot \\mathbf{x}_w^{(\\ell)}\n",
        "$$\n",
        "\n",
        " \n",
        "Here, $\\mathbf{W}^{(\\ell + 1)}$ is a trainable weight matrix of shape `[num_output_features, num_input_features]` and $c_{w,v}$ refers to a fixed normalization coefficient for each edge. On the other hand, a single Linear layer is defined as:\n",
        "\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_v^{(\\ell + 1)} = \\mathbf{W}^{(\\ell + 1)} \\mathbf{x}_v^{(\\ell)}\n",
        "$$\n",
        "\n",
        "It does not use neighboring node information.\n",
        "\n",
        "We can perform this replacement because GNN operators are designed to take into account the topology of the graph. By replacing the Linear layers with GNN operators, we can use information from neighboring nodes to make predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDLIpomCF_18"
      },
      "source": [
        "## Task 2: Node Classfication Model Design [5 pts]\n",
        "\n",
        "You need to complete the following `NodeGCN` class. We recommend that you use the `GCNConv` to finish the task.\n",
        "\n",
        "To investigate the impact of GNN layers on model performance, you may want to experiment with different types of layers. For instance, you can try substituting all GCNConv layers with GATConv layers that employ attention mechanisms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51ekeNO8F_18"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "### DO NOT CHANGE ANY CODE ABOVE THIS LINE IN THIS CELL ###\n",
        "class NodeGCN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "      self.conv1 = GCNConv(50, 16)\n",
        "      self.conv2 = GCNConv(16, 32)\n",
        "      self.conv3 = GCNConv(32, 64)\n",
        "      self.conv4 = GCNConv(64, 128)\n",
        "      self.conv5 = GCNConv(128, 256)\n",
        "      self.conv6 = GCNConv(256, 512)\n",
        "      self.conv7 = GCNConv(512, 121)\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "        x = self.conv4(x, edge_index)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "        x = self.conv5(x, edge_index)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "        x = self.conv6(x, edge_index)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "        x = self.conv7(x, edge_index)\n",
        "\n",
        "        return x\n",
        "\n",
        "######################################################################\n",
        "########## DON'T WRITE ANY CODE OUTSIDE THE CLASS! ###################\n",
        "######## IF YOU WANT TO CALL OR TEST IT CREATE A NEW CELL ############\n",
        "######################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HO9LNKIzF_19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32d0c10b-cdc0-4f4a-f47b-dd2dfb20539c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NodeGCN(\n",
            "  (conv1): GCNConv(50, 16)\n",
            "  (conv2): GCNConv(16, 32)\n",
            "  (conv3): GCNConv(32, 64)\n",
            "  (conv4): GCNConv(64, 128)\n",
            "  (conv5): GCNConv(128, 256)\n",
            "  (conv6): GCNConv(256, 512)\n",
            "  (conv7): GCNConv(512, 121)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = NodeGCN().to(device)\n",
        "print(model)\n",
        "# loss_fn = torch.nn.CrossEntropyLoss()  # Define loss loss_fn.\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "# You can tune the hyperparameters\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=5e-5)  # Define optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-3Ci3fnF_19"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "      model.train()\n",
        "      optimizer.zero_grad()  # Clear gradients.\n",
        "      total_loss = 0.0\n",
        "      for data in train_loader:\n",
        "            out = model(data.x.to(device), data.edge_index.to(device))  # Perform a single forward pass.\n",
        "            loss = loss_fn(out, data.y.to(device))  # Compute the loss solely based on the training nodes.\n",
        "            total_loss += loss\n",
        "            loss.backward()  # Derive gradients.\n",
        "            optimizer.step()  # Update parameters based on gradients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msgkx7EQF_19"
      },
      "source": [
        "## Task 3: Finish the `gnn_test` function and compute the ROC AUC score [2 pts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8R53c6YF_19"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "from torchmetrics.classification import MultilabelAUROC\n",
        "### DO NOT CHANGE ANY CODE ABOVE THIS LINE IN THIS CELL ###\n",
        "\n",
        "def gnn_test(model, loss_fn, loader, device):\n",
        "      \"\"\"\n",
        "      model: pytorch GNN model\n",
        "      loss_fn: loss function\n",
        "      loader: DataLoader\n",
        "      device: device eused to bind the model and tensor\n",
        "      \"\"\"\n",
        "      model.eval()\n",
        "      total_loss = 0.0\n",
        "      auroc_score = 0.0\n",
        "      total_nodes_n = 0\n",
        "      # TODO: Finish this function, calculate the auroc score using macro average\n",
        "      auroc = MultilabelAUROC(num_labels=121)\n",
        "\n",
        "      with torch.no_grad():\n",
        "            for data in loader:\n",
        "                  data.x, data.y = data.x.to(device), data.y.to(device)\n",
        "                  out = model(data.x, data.edge_index.to(device))\n",
        "                  loss = loss_fn(out, data.y.to(device))  # Compute the loss solely based on the training nodes.\n",
        "                  total_loss += loss * data.x.shape[0]\n",
        "                  total_nodes_n += data.x.shape[0]\n",
        "                  auroc.update(out, data.y.to(torch.long))\n",
        "      auroc_score = auroc.compute().mean().item()            \n",
        "      return total_loss / total_nodes_n, auroc_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpjvgNS0F_1-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "2c20d9c2-064a-42a8-c585-8bec4aa88a42"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Train Loss: 0.6033, Train auroc: 0.4326, Valid Loss: 0.5951, Valid Auroc: 0.4343\n",
            "Epoch: 002, Train Loss: 0.6137, Train auroc: 0.4542, Valid Loss: 0.6125, Valid Auroc: 0.4548\n",
            "Epoch: 003, Train Loss: 0.6151, Train auroc: 0.4393, Valid Loss: 0.6073, Valid Auroc: 0.4402\n",
            "Epoch: 004, Train Loss: 0.5864, Train auroc: 0.4394, Valid Loss: 0.5816, Valid Auroc: 0.4397\n",
            "Epoch: 005, Train Loss: 0.6002, Train auroc: 0.4367, Valid Loss: 0.5969, Valid Auroc: 0.4372\n",
            "Epoch: 006, Train Loss: 0.5910, Train auroc: 0.4365, Valid Loss: 0.5862, Valid Auroc: 0.4370\n",
            "Epoch: 007, Train Loss: 0.5911, Train auroc: 0.4393, Valid Loss: 0.5825, Valid Auroc: 0.4398\n",
            "Epoch: 008, Train Loss: 0.5838, Train auroc: 0.4398, Valid Loss: 0.5769, Valid Auroc: 0.4400\n",
            "Epoch: 009, Train Loss: 0.5840, Train auroc: 0.4471, Valid Loss: 0.5797, Valid Auroc: 0.4472\n",
            "Epoch: 010, Train Loss: 0.5842, Train auroc: 0.4532, Valid Loss: 0.5802, Valid Auroc: 0.4533\n",
            "Epoch: 011, Train Loss: 0.5792, Train auroc: 0.4474, Valid Loss: 0.5740, Valid Auroc: 0.4474\n",
            "Epoch: 012, Train Loss: 0.5792, Train auroc: 0.4466, Valid Loss: 0.5724, Valid Auroc: 0.4467\n",
            "Epoch: 013, Train Loss: 0.5798, Train auroc: 0.4468, Valid Loss: 0.5724, Valid Auroc: 0.4469\n",
            "Epoch: 014, Train Loss: 0.5766, Train auroc: 0.4478, Valid Loss: 0.5700, Valid Auroc: 0.4478\n",
            "Epoch: 015, Train Loss: 0.5759, Train auroc: 0.4563, Valid Loss: 0.5704, Valid Auroc: 0.4565\n",
            "Epoch: 016, Train Loss: 0.5761, Train auroc: 0.4583, Valid Loss: 0.5710, Valid Auroc: 0.4585\n",
            "Epoch: 017, Train Loss: 0.5744, Train auroc: 0.4559, Valid Loss: 0.5689, Valid Auroc: 0.4560\n",
            "Epoch: 018, Train Loss: 0.5732, Train auroc: 0.4517, Valid Loss: 0.5669, Valid Auroc: 0.4518\n",
            "Epoch: 019, Train Loss: 0.5728, Train auroc: 0.4516, Valid Loss: 0.5660, Valid Auroc: 0.4517\n",
            "Epoch: 020, Train Loss: 0.5715, Train auroc: 0.4560, Valid Loss: 0.5650, Valid Auroc: 0.4561\n",
            "Epoch: 021, Train Loss: 0.5709, Train auroc: 0.4668, Valid Loss: 0.5649, Valid Auroc: 0.4675\n",
            "Epoch: 022, Train Loss: 0.5697, Train auroc: 0.4722, Valid Loss: 0.5638, Valid Auroc: 0.4729\n",
            "Epoch: 023, Train Loss: 0.5687, Train auroc: 0.4713, Valid Loss: 0.5620, Valid Auroc: 0.4721\n",
            "Epoch: 024, Train Loss: 0.5673, Train auroc: 0.4783, Valid Loss: 0.5608, Valid Auroc: 0.4786\n",
            "Epoch: 025, Train Loss: 0.5654, Train auroc: 0.4889, Valid Loss: 0.5594, Valid Auroc: 0.4896\n",
            "Epoch: 026, Train Loss: 0.5642, Train auroc: 0.5047, Valid Loss: 0.5587, Valid Auroc: 0.5059\n",
            "Epoch: 027, Train Loss: 0.5625, Train auroc: 0.5090, Valid Loss: 0.5567, Valid Auroc: 0.5096\n",
            "Epoch: 028, Train Loss: 0.5618, Train auroc: 0.4994, Valid Loss: 0.5543, Valid Auroc: 0.5000\n",
            "Epoch: 029, Train Loss: 0.5601, Train auroc: 0.5400, Valid Loss: 0.5556, Valid Auroc: 0.5414\n",
            "Epoch: 030, Train Loss: 0.5559, Train auroc: 0.5503, Valid Loss: 0.5515, Valid Auroc: 0.5537\n",
            "Epoch: 031, Train Loss: 0.5576, Train auroc: 0.5210, Valid Loss: 0.5494, Valid Auroc: 0.5227\n",
            "Epoch: 032, Train Loss: 0.5548, Train auroc: 0.5410, Valid Loss: 0.5481, Valid Auroc: 0.5420\n",
            "Epoch: 033, Train Loss: 0.5640, Train auroc: 0.5627, Valid Loss: 0.5605, Valid Auroc: 0.5638\n",
            "Epoch: 034, Train Loss: 0.5546, Train auroc: 0.5312, Valid Loss: 0.5473, Valid Auroc: 0.5316\n",
            "Epoch: 035, Train Loss: 0.5638, Train auroc: 0.5102, Valid Loss: 0.5528, Valid Auroc: 0.5102\n",
            "Epoch: 036, Train Loss: 0.5562, Train auroc: 0.5511, Valid Loss: 0.5500, Valid Auroc: 0.5519\n",
            "Epoch: 037, Train Loss: 0.5606, Train auroc: 0.5572, Valid Loss: 0.5567, Valid Auroc: 0.5578\n",
            "Epoch: 038, Train Loss: 0.5570, Train auroc: 0.5324, Valid Loss: 0.5524, Valid Auroc: 0.5329\n",
            "Epoch: 039, Train Loss: 0.5545, Train auroc: 0.5223, Valid Loss: 0.5481, Valid Auroc: 0.5232\n",
            "Epoch: 040, Train Loss: 0.5552, Train auroc: 0.5272, Valid Loss: 0.5491, Valid Auroc: 0.5272\n",
            "Epoch: 041, Train Loss: 0.5584, Train auroc: 0.5585, Valid Loss: 0.5523, Valid Auroc: 0.5589\n",
            "Epoch: 042, Train Loss: 0.5548, Train auroc: 0.5464, Valid Loss: 0.5485, Valid Auroc: 0.5467\n",
            "Epoch: 043, Train Loss: 0.5536, Train auroc: 0.5273, Valid Loss: 0.5471, Valid Auroc: 0.5264\n",
            "Epoch: 044, Train Loss: 0.5556, Train auroc: 0.5334, Valid Loss: 0.5427, Valid Auroc: 0.5423\n",
            "Epoch: 045, Train Loss: 0.5551, Train auroc: 0.5446, Valid Loss: 0.5504, Valid Auroc: 0.5449\n",
            "Epoch: 046, Train Loss: 0.5543, Train auroc: 0.5643, Valid Loss: 0.5491, Valid Auroc: 0.5658\n",
            "Epoch: 047, Train Loss: 0.5560, Train auroc: 0.5639, Valid Loss: 0.5486, Valid Auroc: 0.5658\n",
            "Epoch: 048, Train Loss: 0.5538, Train auroc: 0.5425, Valid Loss: 0.5452, Valid Auroc: 0.5440\n",
            "Epoch: 049, Train Loss: 0.5503, Train auroc: 0.5358, Valid Loss: 0.5417, Valid Auroc: 0.5368\n",
            "Epoch: 050, Train Loss: 0.5493, Train auroc: 0.5488, Valid Loss: 0.5421, Valid Auroc: 0.5494\n",
            "Epoch: 051, Train Loss: 0.5620, Train auroc: 0.5321, Valid Loss: 0.5449, Valid Auroc: 0.5440\n",
            "Epoch: 052, Train Loss: 0.5635, Train auroc: 0.5890, Valid Loss: 0.5603, Valid Auroc: 0.5914\n",
            "Epoch: 053, Train Loss: 0.5579, Train auroc: 0.5878, Valid Loss: 0.5534, Valid Auroc: 0.5901\n",
            "Epoch: 054, Train Loss: 0.5554, Train auroc: 0.5771, Valid Loss: 0.5475, Valid Auroc: 0.5786\n",
            "Epoch: 055, Train Loss: 0.5541, Train auroc: 0.5439, Valid Loss: 0.5446, Valid Auroc: 0.5450\n",
            "Epoch: 056, Train Loss: 0.5531, Train auroc: 0.5267, Valid Loss: 0.5429, Valid Auroc: 0.5270\n",
            "Epoch: 057, Train Loss: 0.5519, Train auroc: 0.5314, Valid Loss: 0.5424, Valid Auroc: 0.5314\n",
            "Epoch: 058, Train Loss: 0.5476, Train auroc: 0.5474, Valid Loss: 0.5399, Valid Auroc: 0.5480\n",
            "Epoch: 059, Train Loss: 0.5438, Train auroc: 0.5713, Valid Loss: 0.5360, Valid Auroc: 0.5724\n",
            "Epoch: 060, Train Loss: 0.5425, Train auroc: 0.5855, Valid Loss: 0.5353, Valid Auroc: 0.5867\n",
            "Epoch: 061, Train Loss: 0.5415, Train auroc: 0.5944, Valid Loss: 0.5349, Valid Auroc: 0.5956\n",
            "Epoch: 062, Train Loss: 0.5408, Train auroc: 0.5876, Valid Loss: 0.5314, Valid Auroc: 0.5900\n",
            "Epoch: 063, Train Loss: 0.5411, Train auroc: 0.6022, Valid Loss: 0.5361, Valid Auroc: 0.6030\n",
            "Epoch: 064, Train Loss: 0.5365, Train auroc: 0.6021, Valid Loss: 0.5299, Valid Auroc: 0.6032\n",
            "Epoch: 065, Train Loss: 0.5375, Train auroc: 0.5956, Valid Loss: 0.5282, Valid Auroc: 0.5973\n",
            "Epoch: 066, Train Loss: 0.5450, Train auroc: 0.6098, Valid Loss: 0.5408, Valid Auroc: 0.6109\n",
            "Epoch: 067, Train Loss: 0.5366, Train auroc: 0.6099, Valid Loss: 0.5294, Valid Auroc: 0.6107\n",
            "Epoch: 068, Train Loss: 0.5517, Train auroc: 0.5807, Valid Loss: 0.5379, Valid Auroc: 0.5825\n",
            "Epoch: 069, Train Loss: 0.5407, Train auroc: 0.6088, Valid Loss: 0.5348, Valid Auroc: 0.6100\n",
            "Epoch: 070, Train Loss: 0.5578, Train auroc: 0.5931, Valid Loss: 0.5545, Valid Auroc: 0.5954\n",
            "Epoch: 071, Train Loss: 0.5451, Train auroc: 0.5860, Valid Loss: 0.5381, Valid Auroc: 0.5890\n",
            "Epoch: 072, Train Loss: 0.5509, Train auroc: 0.5668, Valid Loss: 0.5409, Valid Auroc: 0.5695\n",
            "Epoch: 073, Train Loss: 0.5485, Train auroc: 0.5725, Valid Loss: 0.5387, Valid Auroc: 0.5737\n",
            "Epoch: 074, Train Loss: 0.5429, Train auroc: 0.5815, Valid Loss: 0.5343, Valid Auroc: 0.5827\n",
            "Epoch: 075, Train Loss: 0.5412, Train auroc: 0.5847, Valid Loss: 0.5341, Valid Auroc: 0.5859\n",
            "Epoch: 076, Train Loss: 0.5419, Train auroc: 0.5875, Valid Loss: 0.5357, Valid Auroc: 0.5884\n",
            "Epoch: 077, Train Loss: 0.5391, Train auroc: 0.5954, Valid Loss: 0.5324, Valid Auroc: 0.5960\n",
            "Epoch: 078, Train Loss: 0.5375, Train auroc: 0.6032, Valid Loss: 0.5291, Valid Auroc: 0.6034\n",
            "Epoch: 079, Train Loss: 0.5386, Train auroc: 0.6057, Valid Loss: 0.5285, Valid Auroc: 0.6081\n",
            "Epoch: 080, Train Loss: 0.5353, Train auroc: 0.6161, Valid Loss: 0.5275, Valid Auroc: 0.6166\n",
            "Epoch: 081, Train Loss: 0.5372, Train auroc: 0.6201, Valid Loss: 0.5319, Valid Auroc: 0.6199\n",
            "Epoch: 082, Train Loss: 0.5353, Train auroc: 0.6214, Valid Loss: 0.5296, Valid Auroc: 0.6214\n",
            "Epoch: 083, Train Loss: 0.5358, Train auroc: 0.6135, Valid Loss: 0.5254, Valid Auroc: 0.6172\n",
            "Epoch: 084, Train Loss: 0.5325, Train auroc: 0.6206, Valid Loss: 0.5247, Valid Auroc: 0.6216\n",
            "Epoch: 085, Train Loss: 0.5386, Train auroc: 0.6220, Valid Loss: 0.5340, Valid Auroc: 0.6222\n",
            "Epoch: 086, Train Loss: 0.5335, Train auroc: 0.6240, Valid Loss: 0.5271, Valid Auroc: 0.6244\n",
            "Epoch: 087, Train Loss: 0.5387, Train auroc: 0.6134, Valid Loss: 0.5271, Valid Auroc: 0.6155\n",
            "Epoch: 088, Train Loss: 0.5328, Train auroc: 0.6229, Valid Loss: 0.5238, Valid Auroc: 0.6240\n",
            "Epoch: 089, Train Loss: 0.5406, Train auroc: 0.6241, Valid Loss: 0.5363, Valid Auroc: 0.6250\n",
            "Epoch: 090, Train Loss: 0.5403, Train auroc: 0.6234, Valid Loss: 0.5358, Valid Auroc: 0.6243\n",
            "Epoch: 091, Train Loss: 0.5335, Train auroc: 0.6252, Valid Loss: 0.5252, Valid Auroc: 0.6257\n",
            "Epoch: 092, Train Loss: 0.5417, Train auroc: 0.6146, Valid Loss: 0.5298, Valid Auroc: 0.6154\n",
            "Epoch: 093, Train Loss: 0.5366, Train auroc: 0.6205, Valid Loss: 0.5263, Valid Auroc: 0.6210\n",
            "Epoch: 094, Train Loss: 0.5346, Train auroc: 0.6230, Valid Loss: 0.5280, Valid Auroc: 0.6236\n",
            "Epoch: 095, Train Loss: 0.5400, Train auroc: 0.6202, Valid Loss: 0.5355, Valid Auroc: 0.6210\n",
            "Epoch: 096, Train Loss: 0.5372, Train auroc: 0.6216, Valid Loss: 0.5320, Valid Auroc: 0.6222\n",
            "Epoch: 097, Train Loss: 0.5337, Train auroc: 0.6255, Valid Loss: 0.5259, Valid Auroc: 0.6258\n",
            "Epoch: 098, Train Loss: 0.5361, Train auroc: 0.6253, Valid Loss: 0.5261, Valid Auroc: 0.6257\n",
            "Epoch: 099, Train Loss: 0.5366, Train auroc: 0.6246, Valid Loss: 0.5262, Valid Auroc: 0.6255\n",
            "Epoch: 100, Train Loss: 0.5327, Train auroc: 0.6284, Valid Loss: 0.5245, Valid Auroc: 0.6284\n",
            "Epoch: 101, Train Loss: 0.5337, Train auroc: 0.6280, Valid Loss: 0.5281, Valid Auroc: 0.6279\n",
            "Epoch: 102, Train Loss: 0.5347, Train auroc: 0.6285, Valid Loss: 0.5299, Valid Auroc: 0.6284\n",
            "Epoch: 103, Train Loss: 0.5306, Train auroc: 0.6307, Valid Loss: 0.5234, Valid Auroc: 0.6309\n",
            "Epoch: 104, Train Loss: 0.5333, Train auroc: 0.6275, Valid Loss: 0.5229, Valid Auroc: 0.6296\n",
            "Epoch: 105, Train Loss: 0.5298, Train auroc: 0.6327, Valid Loss: 0.5219, Valid Auroc: 0.6331\n",
            "Epoch: 106, Train Loss: 0.5328, Train auroc: 0.6321, Valid Loss: 0.5277, Valid Auroc: 0.6323\n",
            "Epoch: 107, Train Loss: 0.5312, Train auroc: 0.6335, Valid Loss: 0.5257, Valid Auroc: 0.6336\n",
            "Epoch: 108, Train Loss: 0.5306, Train auroc: 0.6319, Valid Loss: 0.5212, Valid Auroc: 0.6329\n",
            "Epoch: 109, Train Loss: 0.5309, Train auroc: 0.6319, Valid Loss: 0.5212, Valid Auroc: 0.6329\n",
            "Epoch: 110, Train Loss: 0.5327, Train auroc: 0.6340, Valid Loss: 0.5280, Valid Auroc: 0.6345\n",
            "Epoch: 111, Train Loss: 0.5340, Train auroc: 0.6329, Valid Loss: 0.5295, Valid Auroc: 0.6333\n",
            "Epoch: 112, Train Loss: 0.5292, Train auroc: 0.6344, Valid Loss: 0.5207, Valid Auroc: 0.6345\n",
            "Epoch: 113, Train Loss: 0.5362, Train auroc: 0.6292, Valid Loss: 0.5244, Valid Auroc: 0.6299\n",
            "Epoch: 114, Train Loss: 0.5290, Train auroc: 0.6347, Valid Loss: 0.5218, Valid Auroc: 0.6349\n",
            "Epoch: 115, Train Loss: 0.5368, Train auroc: 0.6306, Valid Loss: 0.5328, Valid Auroc: 0.6312\n",
            "Epoch: 116, Train Loss: 0.5313, Train auroc: 0.6327, Valid Loss: 0.5254, Valid Auroc: 0.6333\n",
            "Epoch: 117, Train Loss: 0.5361, Train auroc: 0.6300, Valid Loss: 0.5249, Valid Auroc: 0.6311\n",
            "Epoch: 118, Train Loss: 0.5305, Train auroc: 0.6350, Valid Loss: 0.5216, Valid Auroc: 0.6355\n",
            "Epoch: 119, Train Loss: 0.5339, Train auroc: 0.6314, Valid Loss: 0.5289, Valid Auroc: 0.6323\n",
            "Epoch: 120, Train Loss: 0.5357, Train auroc: 0.6305, Valid Loss: 0.5312, Valid Auroc: 0.6314\n",
            "Epoch: 121, Train Loss: 0.5303, Train auroc: 0.6341, Valid Loss: 0.5233, Valid Auroc: 0.6348\n",
            "Epoch: 122, Train Loss: 0.5323, Train auroc: 0.6358, Valid Loss: 0.5223, Valid Auroc: 0.6365\n",
            "Epoch: 123, Train Loss: 0.5334, Train auroc: 0.6357, Valid Loss: 0.5229, Valid Auroc: 0.6371\n",
            "Epoch: 124, Train Loss: 0.5298, Train auroc: 0.6369, Valid Loss: 0.5220, Valid Auroc: 0.6373\n",
            "Epoch: 125, Train Loss: 0.5314, Train auroc: 0.6346, Valid Loss: 0.5259, Valid Auroc: 0.6351\n",
            "Epoch: 126, Train Loss: 0.5319, Train auroc: 0.6355, Valid Loss: 0.5268, Valid Auroc: 0.6362\n",
            "Epoch: 127, Train Loss: 0.5286, Train auroc: 0.6385, Valid Loss: 0.5216, Valid Auroc: 0.6392\n",
            "Epoch: 128, Train Loss: 0.5299, Train auroc: 0.6390, Valid Loss: 0.5200, Valid Auroc: 0.6401\n",
            "Epoch: 129, Train Loss: 0.5287, Train auroc: 0.6399, Valid Loss: 0.5194, Valid Auroc: 0.6411\n",
            "Epoch: 130, Train Loss: 0.5282, Train auroc: 0.6397, Valid Loss: 0.5223, Valid Auroc: 0.6405\n",
            "Epoch: 131, Train Loss: 0.5306, Train auroc: 0.6389, Valid Loss: 0.5261, Valid Auroc: 0.6395\n",
            "Epoch: 132, Train Loss: 0.5267, Train auroc: 0.6413, Valid Loss: 0.5196, Valid Auroc: 0.6418\n",
            "Epoch: 133, Train Loss: 0.5298, Train auroc: 0.6392, Valid Loss: 0.5195, Valid Auroc: 0.6409\n",
            "Epoch: 134, Train Loss: 0.5267, Train auroc: 0.6416, Valid Loss: 0.5206, Valid Auroc: 0.6419\n",
            "Epoch: 135, Train Loss: 0.5304, Train auroc: 0.6399, Valid Loss: 0.5260, Valid Auroc: 0.6401\n",
            "Epoch: 136, Train Loss: 0.5268, Train auroc: 0.6421, Valid Loss: 0.5179, Valid Auroc: 0.6427\n",
            "Epoch: 137, Train Loss: 0.5273, Train auroc: 0.6421, Valid Loss: 0.5180, Valid Auroc: 0.6428\n",
            "Epoch: 138, Train Loss: 0.5277, Train auroc: 0.6419, Valid Loss: 0.5224, Valid Auroc: 0.6421\n",
            "Epoch: 139, Train Loss: 0.5292, Train auroc: 0.6413, Valid Loss: 0.5245, Valid Auroc: 0.6416\n",
            "Epoch: 140, Train Loss: 0.5255, Train auroc: 0.6437, Valid Loss: 0.5178, Valid Auroc: 0.6443\n",
            "Epoch: 141, Train Loss: 0.5299, Train auroc: 0.6410, Valid Loss: 0.5190, Valid Auroc: 0.6429\n",
            "Epoch: 142, Train Loss: 0.5256, Train auroc: 0.6442, Valid Loss: 0.5189, Valid Auroc: 0.6446\n",
            "Epoch: 143, Train Loss: 0.5323, Train auroc: 0.6401, Valid Loss: 0.5282, Valid Auroc: 0.6404\n",
            "Epoch: 144, Train Loss: 0.5276, Train auroc: 0.6426, Valid Loss: 0.5219, Valid Auroc: 0.6428\n",
            "Epoch: 145, Train Loss: 0.5287, Train auroc: 0.6439, Valid Loss: 0.5182, Valid Auroc: 0.6453\n",
            "Epoch: 146, Train Loss: 0.5288, Train auroc: 0.6437, Valid Loss: 0.5184, Valid Auroc: 0.6448\n",
            "Epoch: 147, Train Loss: 0.5270, Train auroc: 0.6429, Valid Loss: 0.5210, Valid Auroc: 0.6430\n",
            "Epoch: 148, Train Loss: 0.5321, Train auroc: 0.6397, Valid Loss: 0.5279, Valid Auroc: 0.6402\n",
            "Epoch: 149, Train Loss: 0.5274, Train auroc: 0.6424, Valid Loss: 0.5213, Valid Auroc: 0.6431\n",
            "Epoch: 150, Train Loss: 0.5283, Train auroc: 0.6449, Valid Loss: 0.5182, Valid Auroc: 0.6462\n",
            "Epoch: 151, Train Loss: 0.5281, Train auroc: 0.6455, Valid Loss: 0.5182, Valid Auroc: 0.6470\n",
            "Epoch: 152, Train Loss: 0.5265, Train auroc: 0.6456, Valid Loss: 0.5204, Valid Auroc: 0.6461\n",
            "Epoch: 153, Train Loss: 0.5279, Train auroc: 0.6452, Valid Loss: 0.5230, Valid Auroc: 0.6455\n",
            "Epoch: 154, Train Loss: 0.5255, Train auroc: 0.6469, Valid Loss: 0.5168, Valid Auroc: 0.6479\n",
            "Epoch: 155, Train Loss: 0.5283, Train auroc: 0.6460, Valid Loss: 0.5175, Valid Auroc: 0.6486\n",
            "Epoch: 156, Train Loss: 0.5252, Train auroc: 0.6470, Valid Loss: 0.5183, Valid Auroc: 0.6479\n",
            "Epoch: 157, Train Loss: 0.5292, Train auroc: 0.6438, Valid Loss: 0.5245, Valid Auroc: 0.6444\n",
            "Epoch: 158, Train Loss: 0.5258, Train auroc: 0.6464, Valid Loss: 0.5196, Valid Auroc: 0.6471\n",
            "Epoch: 159, Train Loss: 0.5286, Train auroc: 0.6455, Valid Loss: 0.5177, Valid Auroc: 0.6477\n",
            "Epoch: 160, Train Loss: 0.5245, Train auroc: 0.6477, Valid Loss: 0.5165, Valid Auroc: 0.6482\n",
            "Epoch: 161, Train Loss: 0.5282, Train auroc: 0.6449, Valid Loss: 0.5231, Valid Auroc: 0.6451\n",
            "Epoch: 162, Train Loss: 0.5257, Train auroc: 0.6473, Valid Loss: 0.5203, Valid Auroc: 0.6475\n",
            "Epoch: 163, Train Loss: 0.5263, Train auroc: 0.6477, Valid Loss: 0.5169, Valid Auroc: 0.6496\n",
            "Epoch: 164, Train Loss: 0.5250, Train auroc: 0.6488, Valid Loss: 0.5154, Valid Auroc: 0.6501\n",
            "Epoch: 165, Train Loss: 0.5267, Train auroc: 0.6456, Valid Loss: 0.5204, Valid Auroc: 0.6453\n",
            "Epoch: 166, Train Loss: 0.5280, Train auroc: 0.6443, Valid Loss: 0.5226, Valid Auroc: 0.6442\n",
            "Epoch: 167, Train Loss: 0.5238, Train auroc: 0.6485, Valid Loss: 0.5160, Valid Auroc: 0.6490\n",
            "Epoch: 168, Train Loss: 0.5267, Train auroc: 0.6477, Valid Loss: 0.5164, Valid Auroc: 0.6495\n",
            "Epoch: 169, Train Loss: 0.5239, Train auroc: 0.6486, Valid Loss: 0.5175, Valid Auroc: 0.6488\n",
            "Epoch: 170, Train Loss: 0.5273, Train auroc: 0.6462, Valid Loss: 0.5228, Valid Auroc: 0.6462\n",
            "Epoch: 171, Train Loss: 0.5235, Train auroc: 0.6495, Valid Loss: 0.5165, Valid Auroc: 0.6498\n",
            "Epoch: 172, Train Loss: 0.5273, Train auroc: 0.6501, Valid Loss: 0.5158, Valid Auroc: 0.6526\n",
            "Epoch: 173, Train Loss: 0.5231, Train auroc: 0.6514, Valid Loss: 0.5149, Valid Auroc: 0.6527\n",
            "Epoch: 174, Train Loss: 0.5283, Train auroc: 0.6464, Valid Loss: 0.5240, Valid Auroc: 0.6468\n",
            "Epoch: 175, Train Loss: 0.5266, Train auroc: 0.6478, Valid Loss: 0.5217, Valid Auroc: 0.6481\n",
            "Epoch: 176, Train Loss: 0.5245, Train auroc: 0.6512, Valid Loss: 0.5148, Valid Auroc: 0.6525\n",
            "Epoch: 177, Train Loss: 0.5276, Train auroc: 0.6500, Valid Loss: 0.5163, Valid Auroc: 0.6518\n",
            "Epoch: 178, Train Loss: 0.5251, Train auroc: 0.6489, Valid Loss: 0.5194, Valid Auroc: 0.6492\n",
            "Epoch: 179, Train Loss: 0.5303, Train auroc: 0.6447, Valid Loss: 0.5258, Valid Auroc: 0.6453\n",
            "Epoch: 180, Train Loss: 0.5247, Train auroc: 0.6481, Valid Loss: 0.5174, Valid Auroc: 0.6489\n",
            "Epoch: 181, Train Loss: 0.5265, Train auroc: 0.6511, Valid Loss: 0.5160, Valid Auroc: 0.6526\n",
            "Epoch: 182, Train Loss: 0.5263, Train auroc: 0.6514, Valid Loss: 0.5160, Valid Auroc: 0.6525\n",
            "Epoch: 183, Train Loss: 0.5238, Train auroc: 0.6498, Valid Loss: 0.5167, Valid Auroc: 0.6495\n",
            "Epoch: 184, Train Loss: 0.5269, Train auroc: 0.6478, Valid Loss: 0.5219, Valid Auroc: 0.6479\n",
            "Epoch: 185, Train Loss: 0.5237, Train auroc: 0.6505, Valid Loss: 0.5170, Valid Auroc: 0.6512\n",
            "Epoch: 186, Train Loss: 0.5250, Train auroc: 0.6519, Valid Loss: 0.5149, Valid Auroc: 0.6538\n",
            "Epoch: 187, Train Loss: 0.5227, Train auroc: 0.6534, Valid Loss: 0.5148, Valid Auroc: 0.6541\n",
            "Epoch: 188, Train Loss: 0.5247, Train auroc: 0.6511, Valid Loss: 0.5191, Valid Auroc: 0.6510\n",
            "Epoch: 189, Train Loss: 0.5243, Train auroc: 0.6517, Valid Loss: 0.5179, Valid Auroc: 0.6518\n",
            "Epoch: 190, Train Loss: 0.5237, Train auroc: 0.6540, Valid Loss: 0.5141, Valid Auroc: 0.6552\n",
            "Epoch: 191, Train Loss: 0.5232, Train auroc: 0.6533, Valid Loss: 0.5157, Valid Auroc: 0.6542\n",
            "Epoch: 192, Train Loss: 0.5247, Train auroc: 0.6511, Valid Loss: 0.5191, Valid Auroc: 0.6513\n",
            "Epoch: 193, Train Loss: 0.5235, Train auroc: 0.6517, Valid Loss: 0.5149, Valid Auroc: 0.6523\n",
            "Epoch: 194, Train Loss: 0.5235, Train auroc: 0.6529, Valid Loss: 0.5142, Valid Auroc: 0.6546\n",
            "Epoch: 195, Train Loss: 0.5238, Train auroc: 0.6533, Valid Loss: 0.5195, Valid Auroc: 0.6538\n",
            "Epoch: 196, Train Loss: 0.5214, Train auroc: 0.6546, Valid Loss: 0.5136, Valid Auroc: 0.6556\n",
            "Epoch: 197, Train Loss: 0.5226, Train auroc: 0.6546, Valid Loss: 0.5123, Valid Auroc: 0.6568\n",
            "Epoch: 198, Train Loss: 0.5256, Train auroc: 0.6538, Valid Loss: 0.5223, Valid Auroc: 0.6539\n",
            "Epoch: 199, Train Loss: 0.5204, Train auroc: 0.6564, Valid Loss: 0.5126, Valid Auroc: 0.6578\n",
            "Epoch: 200, Train Loss: 0.5247, Train auroc: 0.6552, Valid Loss: 0.5129, Valid Auroc: 0.6588\n",
            "Epoch: 201, Train Loss: 0.5242, Train auroc: 0.6533, Valid Loss: 0.5193, Valid Auroc: 0.6535\n",
            "Epoch: 202, Train Loss: 0.5290, Train auroc: 0.6499, Valid Loss: 0.5252, Valid Auroc: 0.6499\n",
            "Epoch: 203, Train Loss: 0.5216, Train auroc: 0.6551, Valid Loss: 0.5141, Valid Auroc: 0.6554\n",
            "Epoch: 204, Train Loss: 0.5304, Train auroc: 0.6530, Valid Loss: 0.5172, Valid Auroc: 0.6561\n",
            "Epoch: 205, Train Loss: 0.5221, Train auroc: 0.6556, Valid Loss: 0.5135, Valid Auroc: 0.6562\n",
            "Epoch: 206, Train Loss: 0.5284, Train auroc: 0.6471, Valid Loss: 0.5236, Valid Auroc: 0.6474\n",
            "Epoch: 207, Train Loss: 0.5293, Train auroc: 0.6458, Valid Loss: 0.5246, Valid Auroc: 0.6461\n",
            "Epoch: 208, Train Loss: 0.5234, Train auroc: 0.6514, Valid Loss: 0.5153, Valid Auroc: 0.6518\n",
            "Epoch: 209, Train Loss: 0.5267, Train auroc: 0.6540, Valid Loss: 0.5161, Valid Auroc: 0.6556\n",
            "Epoch: 210, Train Loss: 0.5237, Train auroc: 0.6535, Valid Loss: 0.5151, Valid Auroc: 0.6535\n",
            "Epoch: 211, Train Loss: 0.5241, Train auroc: 0.6502, Valid Loss: 0.5178, Valid Auroc: 0.6492\n",
            "Epoch: 212, Train Loss: 0.5238, Train auroc: 0.6516, Valid Loss: 0.5174, Valid Auroc: 0.6514\n",
            "Epoch: 213, Train Loss: 0.5230, Train auroc: 0.6553, Valid Loss: 0.5143, Valid Auroc: 0.6561\n",
            "Epoch: 214, Train Loss: 0.5209, Train auroc: 0.6578, Valid Loss: 0.5145, Valid Auroc: 0.6585\n",
            "Epoch: 215, Train Loss: 0.5212, Train auroc: 0.6573, Valid Loss: 0.5142, Valid Auroc: 0.6579\n",
            "Epoch: 216, Train Loss: 0.5201, Train auroc: 0.6586, Valid Loss: 0.5127, Valid Auroc: 0.6593\n",
            "Epoch: 217, Train Loss: 0.5200, Train auroc: 0.6595, Valid Loss: 0.5122, Valid Auroc: 0.6613\n",
            "Epoch: 218, Train Loss: 0.5195, Train auroc: 0.6599, Valid Loss: 0.5111, Valid Auroc: 0.6626\n",
            "Epoch: 219, Train Loss: 0.5214, Train auroc: 0.6596, Valid Loss: 0.5178, Valid Auroc: 0.6602\n",
            "Epoch: 220, Train Loss: 0.5232, Train auroc: 0.6586, Valid Loss: 0.5108, Valid Auroc: 0.6639\n",
            "Epoch: 221, Train Loss: 0.5307, Train auroc: 0.6511, Valid Loss: 0.5265, Valid Auroc: 0.6522\n",
            "Epoch: 222, Train Loss: 0.5305, Train auroc: 0.6453, Valid Loss: 0.5262, Valid Auroc: 0.6454\n",
            "Epoch: 223, Train Loss: 0.5261, Train auroc: 0.6493, Valid Loss: 0.5164, Valid Auroc: 0.6493\n",
            "Epoch: 224, Train Loss: 0.5298, Train auroc: 0.6495, Valid Loss: 0.5174, Valid Auroc: 0.6503\n",
            "Epoch: 225, Train Loss: 0.5254, Train auroc: 0.6474, Valid Loss: 0.5182, Valid Auroc: 0.6463\n",
            "Epoch: 226, Train Loss: 0.5290, Train auroc: 0.6435, Valid Loss: 0.5244, Valid Auroc: 0.6424\n",
            "Epoch: 227, Train Loss: 0.5256, Train auroc: 0.6463, Valid Loss: 0.5200, Valid Auroc: 0.6451\n",
            "Epoch: 228, Train Loss: 0.5249, Train auroc: 0.6509, Valid Loss: 0.5161, Valid Auroc: 0.6499\n",
            "Epoch: 229, Train Loss: 0.5266, Train auroc: 0.6525, Valid Loss: 0.5162, Valid Auroc: 0.6522\n",
            "Epoch: 230, Train Loss: 0.5231, Train auroc: 0.6517, Valid Loss: 0.5153, Valid Auroc: 0.6511\n",
            "Epoch: 231, Train Loss: 0.5253, Train auroc: 0.6488, Valid Loss: 0.5202, Valid Auroc: 0.6484\n",
            "Epoch: 232, Train Loss: 0.5234, Train auroc: 0.6515, Valid Loss: 0.5182, Valid Auroc: 0.6514\n",
            "Epoch: 233, Train Loss: 0.5230, Train auroc: 0.6547, Valid Loss: 0.5139, Valid Auroc: 0.6558\n",
            "Epoch: 234, Train Loss: 0.5224, Train auroc: 0.6552, Valid Loss: 0.5130, Valid Auroc: 0.6561\n",
            "Epoch: 235, Train Loss: 0.5233, Train auroc: 0.6523, Valid Loss: 0.5182, Valid Auroc: 0.6518\n",
            "Epoch: 236, Train Loss: 0.5252, Train auroc: 0.6510, Valid Loss: 0.5209, Valid Auroc: 0.6504\n",
            "Epoch: 238, Train Loss: 0.5251, Train auroc: 0.6565, Valid Loss: 0.5134, Valid Auroc: 0.6588\n",
            "Epoch: 239, Train Loss: 0.5206, Train auroc: 0.6562, Valid Loss: 0.5140, Valid Auroc: 0.6561\n",
            "Epoch: 240, Train Loss: 0.5261, Train auroc: 0.6522, Valid Loss: 0.5229, Valid Auroc: 0.6515\n",
            "Epoch: 241, Train Loss: 0.5222, Train auroc: 0.6547, Valid Loss: 0.5169, Valid Auroc: 0.6542\n",
            "Epoch: 242, Train Loss: 0.5241, Train auroc: 0.6580, Valid Loss: 0.5126, Valid Auroc: 0.6596\n",
            "Epoch: 243, Train Loss: 0.5218, Train auroc: 0.6589, Valid Loss: 0.5122, Valid Auroc: 0.6599\n",
            "Epoch: 244, Train Loss: 0.5235, Train auroc: 0.6543, Valid Loss: 0.5187, Valid Auroc: 0.6540\n",
            "Epoch: 245, Train Loss: 0.5257, Train auroc: 0.6523, Valid Loss: 0.5215, Valid Auroc: 0.6518\n",
            "Epoch: 246, Train Loss: 0.5209, Train auroc: 0.6564, Valid Loss: 0.5135, Valid Auroc: 0.6564\n",
            "Epoch: 247, Train Loss: 0.5242, Train auroc: 0.6581, Valid Loss: 0.5128, Valid Auroc: 0.6598\n",
            "Epoch: 248, Train Loss: 0.5203, Train auroc: 0.6587, Valid Loss: 0.5121, Valid Auroc: 0.6591\n",
            "Epoch: 249, Train Loss: 0.5232, Train auroc: 0.6543, Valid Loss: 0.5185, Valid Auroc: 0.6539\n",
            "Epoch: 250, Train Loss: 0.5238, Train auroc: 0.6536, Valid Loss: 0.5192, Valid Auroc: 0.6530\n",
            "Epoch: 251, Train Loss: 0.5197, Train auroc: 0.6585, Valid Loss: 0.5118, Valid Auroc: 0.6587\n",
            "Epoch: 252, Train Loss: 0.5243, Train auroc: 0.6594, Valid Loss: 0.5125, Valid Auroc: 0.6615\n",
            "Epoch: 253, Train Loss: 0.5191, Train auroc: 0.6601, Valid Loss: 0.5119, Valid Auroc: 0.6605\n",
            "Epoch: 254, Train Loss: 0.5256, Train auroc: 0.6550, Valid Loss: 0.5216, Valid Auroc: 0.6544\n",
            "Epoch: 255, Train Loss: 0.5230, Train auroc: 0.6561, Valid Loss: 0.5183, Valid Auroc: 0.6555\n",
            "Epoch: 256, Train Loss: 0.5211, Train auroc: 0.6597, Valid Loss: 0.5116, Valid Auroc: 0.6606\n",
            "Epoch: 257, Train Loss: 0.5230, Train auroc: 0.6605, Valid Loss: 0.5118, Valid Auroc: 0.6624\n",
            "Epoch: 258, Train Loss: 0.5222, Train auroc: 0.6592, Valid Loss: 0.5179, Valid Auroc: 0.6594\n",
            "Epoch: 259, Train Loss: 0.5235, Train auroc: 0.6575, Valid Loss: 0.5191, Valid Auroc: 0.6570\n",
            "Epoch: 260, Train Loss: 0.5196, Train auroc: 0.6611, Valid Loss: 0.5110, Valid Auroc: 0.6614\n",
            "Epoch: 261, Train Loss: 0.5245, Train auroc: 0.6607, Valid Loss: 0.5131, Valid Auroc: 0.6622\n",
            "Epoch: 262, Train Loss: 0.5191, Train auroc: 0.6607, Valid Loss: 0.5119, Valid Auroc: 0.6605\n",
            "Epoch: 263, Train Loss: 0.5235, Train auroc: 0.6563, Valid Loss: 0.5187, Valid Auroc: 0.6555\n",
            "Epoch: 264, Train Loss: 0.5232, Train auroc: 0.6570, Valid Loss: 0.5182, Valid Auroc: 0.6559\n",
            "Epoch: 265, Train Loss: 0.5190, Train auroc: 0.6617, Valid Loss: 0.5107, Valid Auroc: 0.6622\n",
            "Epoch: 266, Train Loss: 0.5223, Train auroc: 0.6625, Valid Loss: 0.5128, Valid Auroc: 0.6642\n",
            "Epoch: 267, Train Loss: 0.5186, Train auroc: 0.6619, Valid Loss: 0.5121, Valid Auroc: 0.6619\n",
            "Epoch: 268, Train Loss: 0.5203, Train auroc: 0.6591, Valid Loss: 0.5144, Valid Auroc: 0.6587\n",
            "Epoch: 269, Train Loss: 0.5208, Train auroc: 0.6592, Valid Loss: 0.5141, Valid Auroc: 0.6591\n",
            "Epoch: 270, Train Loss: 0.5188, Train auroc: 0.6618, Valid Loss: 0.5108, Valid Auroc: 0.6625\n",
            "Epoch: 271, Train Loss: 0.5184, Train auroc: 0.6619, Valid Loss: 0.5114, Valid Auroc: 0.6629\n",
            "Epoch: 272, Train Loss: 0.5183, Train auroc: 0.6627, Valid Loss: 0.5134, Valid Auroc: 0.6633\n",
            "Epoch: 273, Train Loss: 0.5191, Train auroc: 0.6640, Valid Loss: 0.5085, Valid Auroc: 0.6671\n",
            "Epoch: 274, Train Loss: 0.5210, Train auroc: 0.6628, Valid Loss: 0.5178, Valid Auroc: 0.6619\n",
            "Epoch: 275, Train Loss: 0.5197, Train auroc: 0.6637, Valid Loss: 0.5147, Valid Auroc: 0.6631\n",
            "Epoch: 276, Train Loss: 0.5224, Train auroc: 0.6630, Valid Loss: 0.5099, Valid Auroc: 0.6650\n",
            "Epoch: 277, Train Loss: 0.5197, Train auroc: 0.6617, Valid Loss: 0.5156, Valid Auroc: 0.6606\n",
            "Epoch: 278, Train Loss: 0.5286, Train auroc: 0.6551, Valid Loss: 0.5266, Valid Auroc: 0.6539\n",
            "Epoch: 279, Train Loss: 0.5211, Train auroc: 0.6591, Valid Loss: 0.5148, Valid Auroc: 0.6576\n",
            "Epoch: 280, Train Loss: 0.5262, Train auroc: 0.6628, Valid Loss: 0.5135, Valid Auroc: 0.6636\n",
            "Epoch: 281, Train Loss: 0.5225, Train auroc: 0.6637, Valid Loss: 0.5113, Valid Auroc: 0.6647\n",
            "Epoch: 282, Train Loss: 0.5221, Train auroc: 0.6592, Valid Loss: 0.5182, Valid Auroc: 0.6587\n",
            "Epoch: 283, Train Loss: 0.5261, Train auroc: 0.6563, Valid Loss: 0.5235, Valid Auroc: 0.6555\n",
            "Epoch: 284, Train Loss: 0.5206, Train auroc: 0.6579, Valid Loss: 0.5133, Valid Auroc: 0.6574\n",
            "Epoch: 285, Train Loss: 0.5240, Train auroc: 0.6619, Valid Loss: 0.5119, Valid Auroc: 0.6631\n",
            "Epoch: 286, Train Loss: 0.5203, Train auroc: 0.6621, Valid Loss: 0.5117, Valid Auroc: 0.6622\n",
            "Epoch: 287, Train Loss: 0.5218, Train auroc: 0.6581, Valid Loss: 0.5164, Valid Auroc: 0.6573\n",
            "Epoch: 288, Train Loss: 0.5218, Train auroc: 0.6582, Valid Loss: 0.5174, Valid Auroc: 0.6566\n",
            "Epoch: 289, Train Loss: 0.5194, Train auroc: 0.6599, Valid Loss: 0.5134, Valid Auroc: 0.6582\n",
            "Epoch: 290, Train Loss: 0.5201, Train auroc: 0.6630, Valid Loss: 0.5107, Valid Auroc: 0.6626\n",
            "Epoch: 291, Train Loss: 0.5183, Train auroc: 0.6649, Valid Loss: 0.5097, Valid Auroc: 0.6649\n",
            "Epoch: 292, Train Loss: 0.5195, Train auroc: 0.6616, Valid Loss: 0.5146, Valid Auroc: 0.6611\n",
            "Epoch: 293, Train Loss: 0.5198, Train auroc: 0.6621, Valid Loss: 0.5157, Valid Auroc: 0.6617\n",
            "Epoch: 294, Train Loss: 0.5168, Train auroc: 0.6662, Valid Loss: 0.5084, Valid Auroc: 0.6675\n",
            "Epoch: 295, Train Loss: 0.5182, Train auroc: 0.6662, Valid Loss: 0.5078, Valid Auroc: 0.6684\n",
            "Epoch: 296, Train Loss: 0.5185, Train auroc: 0.6648, Valid Loss: 0.5148, Valid Auroc: 0.6640\n",
            "Epoch: 297, Train Loss: 0.5199, Train auroc: 0.6639, Valid Loss: 0.5163, Valid Auroc: 0.6630\n",
            "Epoch: 298, Train Loss: 0.5172, Train auroc: 0.6678, Valid Loss: 0.5086, Valid Auroc: 0.6688\n",
            "Epoch: 299, Train Loss: 0.5190, Train auroc: 0.6678, Valid Loss: 0.5078, Valid Auroc: 0.6689\n",
            "Epoch: 300, Train Loss: 0.5188, Train auroc: 0.6649, Valid Loss: 0.5135, Valid Auroc: 0.6635\n",
            "Epoch: 301, Train Loss: 0.5184, Train auroc: 0.6653, Valid Loss: 0.5144, Valid Auroc: 0.6646\n",
            "Epoch: 302, Train Loss: 0.5198, Train auroc: 0.6667, Valid Loss: 0.5100, Valid Auroc: 0.6680\n",
            "Epoch: 303, Train Loss: 0.5176, Train auroc: 0.6652, Valid Loss: 0.5116, Valid Auroc: 0.6649\n",
            "Epoch: 304, Train Loss: 0.5220, Train auroc: 0.6624, Valid Loss: 0.5192, Valid Auroc: 0.6619\n",
            "Epoch: 305, Train Loss: 0.5175, Train auroc: 0.6669, Valid Loss: 0.5085, Valid Auroc: 0.6686\n",
            "Epoch: 306, Train Loss: 0.5163, Train auroc: 0.6683, Valid Loss: 0.5081, Valid Auroc: 0.6698\n",
            "Epoch: 307, Train Loss: 0.5212, Train auroc: 0.6636, Valid Loss: 0.5158, Valid Auroc: 0.6641\n",
            "Epoch: 308, Train Loss: 0.5198, Train auroc: 0.6634, Valid Loss: 0.5149, Valid Auroc: 0.6631\n",
            "Epoch: 309, Train Loss: 0.5176, Train auroc: 0.6653, Valid Loss: 0.5094, Valid Auroc: 0.6651\n",
            "Epoch: 310, Train Loss: 0.5186, Train auroc: 0.6648, Valid Loss: 0.5103, Valid Auroc: 0.6644\n",
            "Epoch: 311, Train Loss: 0.5173, Train auroc: 0.6652, Valid Loss: 0.5117, Valid Auroc: 0.6644\n",
            "Epoch: 312, Train Loss: 0.5178, Train auroc: 0.6648, Valid Loss: 0.5127, Valid Auroc: 0.6643\n",
            "Epoch: 313, Train Loss: 0.5166, Train auroc: 0.6674, Valid Loss: 0.5083, Valid Auroc: 0.6685\n",
            "Epoch: 314, Train Loss: 0.5171, Train auroc: 0.6682, Valid Loss: 0.5075, Valid Auroc: 0.6701\n",
            "Epoch: 315, Train Loss: 0.5168, Train auroc: 0.6672, Valid Loss: 0.5129, Valid Auroc: 0.6669\n",
            "Epoch: 316, Train Loss: 0.5178, Train auroc: 0.6671, Valid Loss: 0.5145, Valid Auroc: 0.6665\n",
            "Epoch: 317, Train Loss: 0.5184, Train auroc: 0.6681, Valid Loss: 0.5071, Valid Auroc: 0.6712\n",
            "Epoch: 318, Train Loss: 0.5149, Train auroc: 0.6704, Valid Loss: 0.5081, Valid Auroc: 0.6713\n",
            "Epoch: 319, Train Loss: 0.5241, Train auroc: 0.6610, Valid Loss: 0.5208, Valid Auroc: 0.6601\n",
            "Epoch: 320, Train Loss: 0.5195, Train auroc: 0.6620, Valid Loss: 0.5129, Valid Auroc: 0.6623\n",
            "Epoch: 321, Train Loss: 0.5224, Train auroc: 0.6626, Valid Loss: 0.5117, Valid Auroc: 0.6640\n",
            "Epoch: 322, Train Loss: 0.5186, Train auroc: 0.6646, Valid Loss: 0.5113, Valid Auroc: 0.6640\n",
            "Epoch: 323, Train Loss: 0.5191, Train auroc: 0.6651, Valid Loss: 0.5150, Valid Auroc: 0.6639\n",
            "Epoch: 324, Train Loss: 0.5165, Train auroc: 0.6682, Valid Loss: 0.5109, Valid Auroc: 0.6680\n",
            "Epoch: 325, Train Loss: 0.5186, Train auroc: 0.6691, Valid Loss: 0.5079, Valid Auroc: 0.6703\n",
            "Epoch: 326, Train Loss: 0.5169, Train auroc: 0.6692, Valid Loss: 0.5082, Valid Auroc: 0.6689\n",
            "Epoch: 327, Train Loss: 0.5196, Train auroc: 0.6657, Valid Loss: 0.5153, Valid Auroc: 0.6643\n",
            "Epoch: 328, Train Loss: 0.5170, Train auroc: 0.6689, Valid Loss: 0.5131, Valid Auroc: 0.6687\n",
            "Epoch: 329, Train Loss: 0.5196, Train auroc: 0.6705, Valid Loss: 0.5083, Valid Auroc: 0.6732\n",
            "Epoch: 330, Train Loss: 0.5156, Train auroc: 0.6708, Valid Loss: 0.5082, Valid Auroc: 0.6719\n",
            "Epoch: 331, Train Loss: 0.5234, Train auroc: 0.6629, Valid Loss: 0.5195, Valid Auroc: 0.6611\n",
            "Epoch: 332, Train Loss: 0.5206, Train auroc: 0.6649, Valid Loss: 0.5160, Valid Auroc: 0.6638\n",
            "Epoch: 333, Train Loss: 0.5166, Train auroc: 0.6703, Valid Loss: 0.5080, Valid Auroc: 0.6708\n",
            "Epoch: 334, Train Loss: 0.5201, Train auroc: 0.6700, Valid Loss: 0.5091, Valid Auroc: 0.6718\n",
            "Epoch: 335, Train Loss: 0.5171, Train auroc: 0.6665, Valid Loss: 0.5112, Valid Auroc: 0.6662\n",
            "Epoch: 336, Train Loss: 0.5192, Train auroc: 0.6645, Valid Loss: 0.5153, Valid Auroc: 0.6631\n",
            "Epoch: 337, Train Loss: 0.5163, Train auroc: 0.6674, Valid Loss: 0.5098, Valid Auroc: 0.6664\n",
            "Epoch: 338, Train Loss: 0.5163, Train auroc: 0.6708, Valid Loss: 0.5066, Valid Auroc: 0.6712\n",
            "Epoch: 339, Train Loss: 0.5172, Train auroc: 0.6709, Valid Loss: 0.5092, Valid Auroc: 0.6716\n",
            "Epoch: 340, Train Loss: 0.5146, Train auroc: 0.6698, Valid Loss: 0.5091, Valid Auroc: 0.6690\n",
            "Epoch: 341, Train Loss: 0.5183, Train auroc: 0.6672, Valid Loss: 0.5129, Valid Auroc: 0.6657\n",
            "Epoch: 342, Train Loss: 0.5153, Train auroc: 0.6710, Valid Loss: 0.5092, Valid Auroc: 0.6701\n",
            "Epoch: 343, Train Loss: 0.5145, Train auroc: 0.6724, Valid Loss: 0.5069, Valid Auroc: 0.6734\n",
            "Epoch: 344, Train Loss: 0.5158, Train auroc: 0.6724, Valid Loss: 0.5085, Valid Auroc: 0.6737\n",
            "Epoch: 345, Train Loss: 0.5127, Train auroc: 0.6740, Valid Loss: 0.5067, Valid Auroc: 0.6741\n",
            "Epoch: 346, Train Loss: 0.5150, Train auroc: 0.6720, Valid Loss: 0.5089, Valid Auroc: 0.6720\n",
            "Epoch: 347, Train Loss: 0.5138, Train auroc: 0.6735, Valid Loss: 0.5101, Valid Auroc: 0.6727\n",
            "Epoch: 348, Train Loss: 0.5137, Train auroc: 0.6758, Valid Loss: 0.5058, Valid Auroc: 0.6770\n",
            "Epoch: 349, Train Loss: 0.5131, Train auroc: 0.6765, Valid Loss: 0.5075, Valid Auroc: 0.6765\n",
            "Epoch: 350, Train Loss: 0.5137, Train auroc: 0.6756, Valid Loss: 0.5100, Valid Auroc: 0.6738\n",
            "Epoch: 351, Train Loss: 0.5170, Train auroc: 0.6755, Valid Loss: 0.5046, Valid Auroc: 0.6787\n",
            "Epoch: 352, Train Loss: 0.5206, Train auroc: 0.6731, Valid Loss: 0.5197, Valid Auroc: 0.6726\n",
            "Epoch: 353, Train Loss: 0.5229, Train auroc: 0.6648, Valid Loss: 0.5187, Valid Auroc: 0.6629\n",
            "Epoch: 354, Train Loss: 0.5290, Train auroc: 0.6638, Valid Loss: 0.5156, Valid Auroc: 0.6650\n",
            "Epoch: 355, Train Loss: 0.5183, Train auroc: 0.6676, Valid Loss: 0.5096, Valid Auroc: 0.6663\n",
            "Epoch: 356, Train Loss: 0.5245, Train auroc: 0.6633, Valid Loss: 0.5225, Valid Auroc: 0.6608\n",
            "Epoch: 357, Train Loss: 0.5214, Train auroc: 0.6648, Valid Loss: 0.5178, Valid Auroc: 0.6625\n",
            "Epoch: 358, Train Loss: 0.5209, Train auroc: 0.6673, Valid Loss: 0.5101, Valid Auroc: 0.6655\n",
            "Epoch: 359, Train Loss: 0.5187, Train auroc: 0.6661, Valid Loss: 0.5104, Valid Auroc: 0.6637\n",
            "Epoch: 360, Train Loss: 0.5200, Train auroc: 0.6661, Valid Loss: 0.5191, Valid Auroc: 0.6638\n",
            "Epoch: 361, Train Loss: 0.5169, Train auroc: 0.6693, Valid Loss: 0.5093, Valid Auroc: 0.6686\n",
            "Epoch: 362, Train Loss: 0.5175, Train auroc: 0.6707, Valid Loss: 0.5073, Valid Auroc: 0.6703\n",
            "Epoch: 363, Train Loss: 0.5193, Train auroc: 0.6656, Valid Loss: 0.5157, Valid Auroc: 0.6636\n",
            "Epoch: 364, Train Loss: 0.5216, Train auroc: 0.6635, Valid Loss: 0.5187, Valid Auroc: 0.6612\n",
            "Epoch: 365, Train Loss: 0.5161, Train auroc: 0.6709, Valid Loss: 0.5080, Valid Auroc: 0.6697\n",
            "Epoch: 366, Train Loss: 0.5194, Train auroc: 0.6733, Valid Loss: 0.5089, Valid Auroc: 0.6735\n",
            "Epoch: 367, Train Loss: 0.5164, Train auroc: 0.6703, Valid Loss: 0.5121, Valid Auroc: 0.6690\n",
            "Epoch: 368, Train Loss: 0.5195, Train auroc: 0.6662, Valid Loss: 0.5168, Valid Auroc: 0.6645\n",
            "Epoch: 369, Train Loss: 0.5171, Train auroc: 0.6681, Valid Loss: 0.5104, Valid Auroc: 0.6669\n",
            "Epoch: 370, Train Loss: 0.5186, Train auroc: 0.6714, Valid Loss: 0.5074, Valid Auroc: 0.6721\n",
            "Epoch: 371, Train Loss: 0.5157, Train auroc: 0.6728, Valid Loss: 0.5115, Valid Auroc: 0.6716\n",
            "Epoch: 372, Train Loss: 0.5173, Train auroc: 0.6697, Valid Loss: 0.5140, Valid Auroc: 0.6673\n",
            "Epoch: 373, Train Loss: 0.5149, Train auroc: 0.6723, Valid Loss: 0.5074, Valid Auroc: 0.6703\n",
            "Epoch: 374, Train Loss: 0.5167, Train auroc: 0.6736, Valid Loss: 0.5064, Valid Auroc: 0.6730\n",
            "Epoch: 375, Train Loss: 0.5138, Train auroc: 0.6741, Valid Loss: 0.5096, Valid Auroc: 0.6721\n",
            "Epoch: 376, Train Loss: 0.5165, Train auroc: 0.6719, Valid Loss: 0.5149, Valid Auroc: 0.6693\n",
            "Epoch: 377, Train Loss: 0.5126, Train auroc: 0.6755, Valid Loss: 0.5064, Valid Auroc: 0.6741\n",
            "Epoch: 378, Train Loss: 0.5159, Train auroc: 0.6757, Valid Loss: 0.5049, Valid Auroc: 0.6765\n",
            "Epoch: 379, Train Loss: 0.5125, Train auroc: 0.6762, Valid Loss: 0.5081, Valid Auroc: 0.6742\n",
            "Epoch: 380, Train Loss: 0.5181, Train auroc: 0.6722, Valid Loss: 0.5165, Valid Auroc: 0.6691\n",
            "Epoch: 381, Train Loss: 0.5117, Train auroc: 0.6776, Valid Loss: 0.5066, Valid Auroc: 0.6763\n",
            "Epoch: 382, Train Loss: 0.5184, Train auroc: 0.6758, Valid Loss: 0.5055, Valid Auroc: 0.6780\n",
            "Epoch: 383, Train Loss: 0.5129, Train auroc: 0.6773, Valid Loss: 0.5072, Valid Auroc: 0.6750\n",
            "Epoch: 384, Train Loss: 0.5215, Train auroc: 0.6687, Valid Loss: 0.5204, Valid Auroc: 0.6653\n",
            "Epoch: 385, Train Loss: 0.5165, Train auroc: 0.6732, Valid Loss: 0.5130, Valid Auroc: 0.6716\n",
            "Epoch: 386, Train Loss: 0.5152, Train auroc: 0.6772, Valid Loss: 0.5052, Valid Auroc: 0.6775\n",
            "Epoch: 387, Train Loss: 0.5193, Train auroc: 0.6748, Valid Loss: 0.5078, Valid Auroc: 0.6741\n",
            "Epoch: 388, Train Loss: 0.5167, Train auroc: 0.6714, Valid Loss: 0.5119, Valid Auroc: 0.6673\n",
            "Epoch: 389, Train Loss: 0.5212, Train auroc: 0.6680, Valid Loss: 0.5202, Valid Auroc: 0.6636\n",
            "Epoch: 390, Train Loss: 0.5155, Train auroc: 0.6731, Valid Loss: 0.5121, Valid Auroc: 0.6699\n",
            "Epoch: 391, Train Loss: 0.5171, Train auroc: 0.6764, Valid Loss: 0.5075, Valid Auroc: 0.6761\n",
            "Epoch: 392, Train Loss: 0.5182, Train auroc: 0.6765, Valid Loss: 0.5075, Valid Auroc: 0.6763\n",
            "Epoch: 393, Train Loss: 0.5143, Train auroc: 0.6751, Valid Loss: 0.5088, Valid Auroc: 0.6718\n",
            "Epoch: 394, Train Loss: 0.5181, Train auroc: 0.6716, Valid Loss: 0.5165, Valid Auroc: 0.6684\n",
            "Epoch: 395, Train Loss: 0.5168, Train auroc: 0.6744, Valid Loss: 0.5139, Valid Auroc: 0.6723\n",
            "Epoch: 396, Train Loss: 0.5130, Train auroc: 0.6792, Valid Loss: 0.5047, Valid Auroc: 0.6792\n",
            "Epoch: 397, Train Loss: 0.5161, Train auroc: 0.6791, Valid Loss: 0.5064, Valid Auroc: 0.6793\n",
            "Epoch: 398, Train Loss: 0.5160, Train auroc: 0.6765, Valid Loss: 0.5100, Valid Auroc: 0.6739\n",
            "Epoch: 399, Train Loss: 0.5145, Train auroc: 0.6767, Valid Loss: 0.5117, Valid Auroc: 0.6737\n",
            "Epoch: 400, Train Loss: 0.5134, Train auroc: 0.6796, Valid Loss: 0.5083, Valid Auroc: 0.6780\n",
            "Epoch: 401, Train Loss: 0.5149, Train auroc: 0.6794, Valid Loss: 0.5055, Valid Auroc: 0.6794\n",
            "Epoch: 402, Train Loss: 0.5127, Train auroc: 0.6783, Valid Loss: 0.5076, Valid Auroc: 0.6761\n",
            "Epoch: 403, Train Loss: 0.5159, Train auroc: 0.6747, Valid Loss: 0.5116, Valid Auroc: 0.6727\n",
            "Epoch: 404, Train Loss: 0.5162, Train auroc: 0.6762, Valid Loss: 0.5096, Valid Auroc: 0.6763\n",
            "Epoch: 405, Train Loss: 0.5116, Train auroc: 0.6801, Valid Loss: 0.5041, Valid Auroc: 0.6788\n",
            "Epoch: 406, Train Loss: 0.5150, Train auroc: 0.6760, Valid Loss: 0.5080, Valid Auroc: 0.6733\n",
            "Epoch: 407, Train Loss: 0.5150, Train auroc: 0.6758, Valid Loss: 0.5118, Valid Auroc: 0.6720\n",
            "Epoch: 408, Train Loss: 0.5127, Train auroc: 0.6795, Valid Loss: 0.5098, Valid Auroc: 0.6761\n",
            "Epoch: 409, Train Loss: 0.5126, Train auroc: 0.6812, Valid Loss: 0.5054, Valid Auroc: 0.6801\n",
            "Epoch: 410, Train Loss: 0.5114, Train auroc: 0.6802, Valid Loss: 0.5060, Valid Auroc: 0.6774\n",
            "Epoch: 411, Train Loss: 0.5144, Train auroc: 0.6773, Valid Loss: 0.5095, Valid Auroc: 0.6741\n",
            "Epoch: 412, Train Loss: 0.5110, Train auroc: 0.6815, Valid Loss: 0.5034, Valid Auroc: 0.6806\n",
            "Epoch: 413, Train Loss: 0.5113, Train auroc: 0.6830, Valid Loss: 0.5040, Valid Auroc: 0.6828\n",
            "Epoch: 414, Train Loss: 0.5124, Train auroc: 0.6814, Valid Loss: 0.5117, Valid Auroc: 0.6783\n",
            "Epoch: 415, Train Loss: 0.5112, Train auroc: 0.6808, Valid Loss: 0.5083, Valid Auroc: 0.6778\n",
            "Epoch: 416, Train Loss: 0.5148, Train auroc: 0.6815, Valid Loss: 0.5023, Valid Auroc: 0.6832\n",
            "Epoch: 417, Train Loss: 0.5120, Train auroc: 0.6818, Valid Loss: 0.5106, Valid Auroc: 0.6783\n",
            "Epoch: 418, Train Loss: 0.5197, Train auroc: 0.6748, Valid Loss: 0.5190, Valid Auroc: 0.6713\n",
            "Epoch: 419, Train Loss: 0.5137, Train auroc: 0.6807, Valid Loss: 0.5040, Valid Auroc: 0.6816\n",
            "Epoch: 420, Train Loss: 0.5156, Train auroc: 0.6814, Valid Loss: 0.5048, Valid Auroc: 0.6808\n",
            "Epoch: 421, Train Loss: 0.5168, Train auroc: 0.6760, Valid Loss: 0.5128, Valid Auroc: 0.6716\n",
            "Epoch: 422, Train Loss: 0.5172, Train auroc: 0.6755, Valid Loss: 0.5160, Valid Auroc: 0.6710\n",
            "Epoch: 423, Train Loss: 0.5154, Train auroc: 0.6809, Valid Loss: 0.5076, Valid Auroc: 0.6796\n",
            "Epoch: 424, Train Loss: 0.5131, Train auroc: 0.6824, Valid Loss: 0.5034, Valid Auroc: 0.6821\n",
            "Epoch: 425, Train Loss: 0.5152, Train auroc: 0.6766, Valid Loss: 0.5103, Valid Auroc: 0.6728\n",
            "Epoch: 426, Train Loss: 0.5189, Train auroc: 0.6729, Valid Loss: 0.5152, Valid Auroc: 0.6692\n",
            "Epoch: 427, Train Loss: 0.5115, Train auroc: 0.6810, Valid Loss: 0.5052, Valid Auroc: 0.6802\n",
            "Epoch: 428, Train Loss: 0.5171, Train auroc: 0.6791, Valid Loss: 0.5063, Valid Auroc: 0.6820\n",
            "Epoch: 429, Train Loss: 0.5106, Train auroc: 0.6810, Valid Loss: 0.5064, Valid Auroc: 0.6793\n",
            "Epoch: 430, Train Loss: 0.5167, Train auroc: 0.6745, Valid Loss: 0.5149, Valid Auroc: 0.6707\n",
            "Epoch: 431, Train Loss: 0.5131, Train auroc: 0.6766, Valid Loss: 0.5087, Valid Auroc: 0.6726\n",
            "Epoch: 432, Train Loss: 0.5119, Train auroc: 0.6826, Valid Loss: 0.5033, Valid Auroc: 0.6809\n",
            "Epoch: 433, Train Loss: 0.5122, Train auroc: 0.6832, Valid Loss: 0.5044, Valid Auroc: 0.6829\n",
            "Epoch: 434, Train Loss: 0.5121, Train auroc: 0.6811, Valid Loss: 0.5092, Valid Auroc: 0.6782\n",
            "Epoch: 435, Train Loss: 0.5127, Train auroc: 0.6799, Valid Loss: 0.5087, Valid Auroc: 0.6759\n",
            "Epoch: 436, Train Loss: 0.5128, Train auroc: 0.6819, Valid Loss: 0.5044, Valid Auroc: 0.6795\n",
            "Epoch: 437, Train Loss: 0.5084, Train auroc: 0.6853, Valid Loss: 0.5033, Valid Auroc: 0.6824\n",
            "Epoch: 438, Train Loss: 0.5113, Train auroc: 0.6849, Valid Loss: 0.5082, Valid Auroc: 0.6823\n",
            "Epoch: 439, Train Loss: 0.5082, Train auroc: 0.6866, Valid Loss: 0.5035, Valid Auroc: 0.6841\n",
            "Epoch: 440, Train Loss: 0.5106, Train auroc: 0.6841, Valid Loss: 0.5019, Valid Auroc: 0.6829\n",
            "Epoch: 441, Train Loss: 0.5098, Train auroc: 0.6838, Valid Loss: 0.5055, Valid Auroc: 0.6801\n",
            "Epoch: 442, Train Loss: 0.5098, Train auroc: 0.6846, Valid Loss: 0.5093, Valid Auroc: 0.6797\n",
            "Epoch: 443, Train Loss: 0.5108, Train auroc: 0.6876, Valid Loss: 0.5028, Valid Auroc: 0.6867\n",
            "Epoch: 444, Train Loss: 0.5072, Train auroc: 0.6887, Valid Loss: 0.5004, Valid Auroc: 0.6868\n",
            "Epoch: 445, Train Loss: 0.5155, Train auroc: 0.6810, Valid Loss: 0.5163, Valid Auroc: 0.6748\n",
            "Epoch: 446, Train Loss: 0.5113, Train auroc: 0.6835, Valid Loss: 0.5090, Valid Auroc: 0.6781\n",
            "Epoch: 447, Train Loss: 0.5149, Train auroc: 0.6856, Valid Loss: 0.5018, Valid Auroc: 0.6866\n",
            "Epoch: 448, Train Loss: 0.5114, Train auroc: 0.6866, Valid Loss: 0.5062, Valid Auroc: 0.6848\n",
            "Epoch: 449, Train Loss: 0.5212, Train auroc: 0.6768, Valid Loss: 0.5227, Valid Auroc: 0.6705\n",
            "Epoch: 450, Train Loss: 0.5157, Train auroc: 0.6775, Valid Loss: 0.5126, Valid Auroc: 0.6717\n",
            "Epoch: 451, Train Loss: 0.5183, Train auroc: 0.6818, Valid Loss: 0.5068, Valid Auroc: 0.6796\n",
            "Epoch: 452, Train Loss: 0.5182, Train auroc: 0.6824, Valid Loss: 0.5058, Valid Auroc: 0.6811\n",
            "Epoch: 453, Train Loss: 0.5117, Train auroc: 0.6818, Valid Loss: 0.5084, Valid Auroc: 0.6776\n",
            "Epoch: 454, Train Loss: 0.5194, Train auroc: 0.6774, Valid Loss: 0.5208, Valid Auroc: 0.6723\n",
            "Epoch: 455, Train Loss: 0.5116, Train auroc: 0.6815, Valid Loss: 0.5087, Valid Auroc: 0.6774\n",
            "Epoch: 456, Train Loss: 0.5155, Train auroc: 0.6854, Valid Loss: 0.5052, Valid Auroc: 0.6843\n",
            "Epoch: 457, Train Loss: 0.5114, Train auroc: 0.6869, Valid Loss: 0.5021, Valid Auroc: 0.6856\n",
            "Epoch: 458, Train Loss: 0.5088, Train auroc: 0.6854, Valid Loss: 0.5061, Valid Auroc: 0.6806\n",
            "Epoch: 459, Train Loss: 0.5129, Train auroc: 0.6832, Valid Loss: 0.5133, Valid Auroc: 0.6772\n",
            "Epoch: 460, Train Loss: 0.5075, Train auroc: 0.6878, Valid Loss: 0.5039, Valid Auroc: 0.6844\n",
            "Epoch: 461, Train Loss: 0.5111, Train auroc: 0.6871, Valid Loss: 0.5005, Valid Auroc: 0.6882\n",
            "Epoch: 462, Train Loss: 0.5079, Train auroc: 0.6898, Valid Loss: 0.5006, Valid Auroc: 0.6878\n",
            "Epoch: 463, Train Loss: 0.5108, Train auroc: 0.6857, Valid Loss: 0.5095, Valid Auroc: 0.6799\n",
            "Epoch: 464, Train Loss: 0.5095, Train auroc: 0.6874, Valid Loss: 0.5081, Valid Auroc: 0.6829\n",
            "Epoch: 465, Train Loss: 0.5098, Train auroc: 0.6901, Valid Loss: 0.5012, Valid Auroc: 0.6898\n",
            "Epoch: 466, Train Loss: 0.5094, Train auroc: 0.6897, Valid Loss: 0.5001, Valid Auroc: 0.6887\n",
            "Epoch: 467, Train Loss: 0.5123, Train auroc: 0.6855, Valid Loss: 0.5092, Valid Auroc: 0.6797\n",
            "Epoch: 468, Train Loss: 0.5126, Train auroc: 0.6844, Valid Loss: 0.5118, Valid Auroc: 0.6788\n",
            "Epoch: 469, Train Loss: 0.5109, Train auroc: 0.6885, Valid Loss: 0.5041, Valid Auroc: 0.6876\n",
            "Epoch: 470, Train Loss: 0.5103, Train auroc: 0.6900, Valid Loss: 0.5006, Valid Auroc: 0.6890\n",
            "Epoch: 471, Train Loss: 0.5120, Train auroc: 0.6843, Valid Loss: 0.5083, Valid Auroc: 0.6789\n",
            "Epoch: 472, Train Loss: 0.5137, Train auroc: 0.6819, Valid Loss: 0.5138, Valid Auroc: 0.6747\n",
            "Epoch: 473, Train Loss: 0.5080, Train auroc: 0.6870, Valid Loss: 0.5032, Valid Auroc: 0.6819\n",
            "Epoch: 474, Train Loss: 0.5102, Train auroc: 0.6890, Valid Loss: 0.5010, Valid Auroc: 0.6872\n",
            "Epoch: 475, Train Loss: 0.5081, Train auroc: 0.6890, Valid Loss: 0.5015, Valid Auroc: 0.6872\n",
            "Epoch: 476, Train Loss: 0.5097, Train auroc: 0.6861, Valid Loss: 0.5081, Valid Auroc: 0.6813\n",
            "Epoch: 477, Train Loss: 0.5090, Train auroc: 0.6859, Valid Loss: 0.5068, Valid Auroc: 0.6807\n",
            "Epoch: 478, Train Loss: 0.5088, Train auroc: 0.6881, Valid Loss: 0.5009, Valid Auroc: 0.6854\n",
            "Epoch: 479, Train Loss: 0.5071, Train auroc: 0.6911, Valid Loss: 0.4993, Valid Auroc: 0.6886\n",
            "Epoch: 480, Train Loss: 0.5079, Train auroc: 0.6897, Valid Loss: 0.5053, Valid Auroc: 0.6853\n",
            "Epoch: 481, Train Loss: 0.5095, Train auroc: 0.6881, Valid Loss: 0.5087, Valid Auroc: 0.6823\n",
            "Epoch: 482, Train Loss: 0.5055, Train auroc: 0.6921, Valid Loss: 0.5001, Valid Auroc: 0.6876\n",
            "Epoch: 483, Train Loss: 0.5110, Train auroc: 0.6910, Valid Loss: 0.5001, Valid Auroc: 0.6894\n",
            "Epoch: 484, Train Loss: 0.5048, Train auroc: 0.6926, Valid Loss: 0.5018, Valid Auroc: 0.6872\n",
            "Epoch: 485, Train Loss: 0.5128, Train auroc: 0.6875, Valid Loss: 0.5152, Valid Auroc: 0.6809\n",
            "Epoch: 486, Train Loss: 0.5070, Train auroc: 0.6913, Valid Loss: 0.5014, Valid Auroc: 0.6887\n",
            "Epoch: 487, Train Loss: 0.5110, Train auroc: 0.6924, Valid Loss: 0.5002, Valid Auroc: 0.6916\n",
            "Epoch: 488, Train Loss: 0.5072, Train auroc: 0.6921, Valid Loss: 0.5029, Valid Auroc: 0.6862\n",
            "Epoch: 489, Train Loss: 0.5077, Train auroc: 0.6903, Valid Loss: 0.5085, Valid Auroc: 0.6828\n",
            "Epoch: 490, Train Loss: 0.5085, Train auroc: 0.6927, Valid Loss: 0.5042, Valid Auroc: 0.6887\n",
            "Epoch: 491, Train Loss: 0.5056, Train auroc: 0.6940, Valid Loss: 0.4979, Valid Auroc: 0.6916\n",
            "Epoch: 492, Train Loss: 0.5098, Train auroc: 0.6907, Valid Loss: 0.5057, Valid Auroc: 0.6849\n",
            "Epoch: 493, Train Loss: 0.5093, Train auroc: 0.6904, Valid Loss: 0.5062, Valid Auroc: 0.6851\n",
            "Epoch: 494, Train Loss: 0.5063, Train auroc: 0.6922, Valid Loss: 0.4983, Valid Auroc: 0.6924\n",
            "Epoch: 495, Train Loss: 0.5091, Train auroc: 0.6919, Valid Loss: 0.5029, Valid Auroc: 0.6918\n",
            "Epoch: 496, Train Loss: 0.5077, Train auroc: 0.6917, Valid Loss: 0.5086, Valid Auroc: 0.6863\n",
            "Epoch: 497, Train Loss: 0.5085, Train auroc: 0.6898, Valid Loss: 0.5054, Valid Auroc: 0.6841\n",
            "Epoch: 498, Train Loss: 0.5106, Train auroc: 0.6905, Valid Loss: 0.4997, Valid Auroc: 0.6885\n",
            "Epoch: 499, Train Loss: 0.5088, Train auroc: 0.6920, Valid Loss: 0.5090, Valid Auroc: 0.6866\n",
            "Epoch: 500, Train Loss: 0.5098, Train auroc: 0.6899, Valid Loss: 0.5093, Valid Auroc: 0.6854\n",
            "Epoch: 501, Train Loss: 0.5084, Train auroc: 0.6924, Valid Loss: 0.4986, Valid Auroc: 0.6925\n",
            "Epoch: 502, Train Loss: 0.5083, Train auroc: 0.6941, Valid Loss: 0.4997, Valid Auroc: 0.6911\n",
            "Epoch: 503, Train Loss: 0.5093, Train auroc: 0.6896, Valid Loss: 0.5097, Valid Auroc: 0.6822\n",
            "Epoch: 504, Train Loss: 0.5082, Train auroc: 0.6902, Valid Loss: 0.5081, Valid Auroc: 0.6843\n",
            "Epoch: 505, Train Loss: 0.5103, Train auroc: 0.6917, Valid Loss: 0.4996, Valid Auroc: 0.6920\n",
            "Epoch: 506, Train Loss: 0.5056, Train auroc: 0.6945, Valid Loss: 0.4976, Valid Auroc: 0.6920\n",
            "Epoch: 507, Train Loss: 0.5119, Train auroc: 0.6880, Valid Loss: 0.5106, Valid Auroc: 0.6815\n",
            "Epoch: 508, Train Loss: 0.5091, Train auroc: 0.6893, Valid Loss: 0.5067, Valid Auroc: 0.6843\n",
            "Epoch: 509, Train Loss: 0.5092, Train auroc: 0.6902, Valid Loss: 0.4995, Valid Auroc: 0.6910\n",
            "Epoch: 510, Train Loss: 0.5088, Train auroc: 0.6907, Valid Loss: 0.5029, Valid Auroc: 0.6906\n",
            "Epoch: 511, Train Loss: 0.5111, Train auroc: 0.6889, Valid Loss: 0.5116, Valid Auroc: 0.6838\n",
            "Epoch: 512, Train Loss: 0.5094, Train auroc: 0.6874, Valid Loss: 0.5074, Valid Auroc: 0.6810\n",
            "Epoch: 513, Train Loss: 0.5098, Train auroc: 0.6904, Valid Loss: 0.5006, Valid Auroc: 0.6873\n",
            "Epoch: 514, Train Loss: 0.5051, Train auroc: 0.6939, Valid Loss: 0.4985, Valid Auroc: 0.6900\n",
            "Epoch: 515, Train Loss: 0.5086, Train auroc: 0.6894, Valid Loss: 0.5075, Valid Auroc: 0.6841\n",
            "Epoch: 516, Train Loss: 0.5078, Train auroc: 0.6896, Valid Loss: 0.5063, Valid Auroc: 0.6846\n",
            "Epoch: 517, Train Loss: 0.5056, Train auroc: 0.6931, Valid Loss: 0.4979, Valid Auroc: 0.6913\n",
            "Epoch: 518, Train Loss: 0.5077, Train auroc: 0.6935, Valid Loss: 0.4980, Valid Auroc: 0.6921\n",
            "Epoch: 519, Train Loss: 0.5053, Train auroc: 0.6927, Valid Loss: 0.5043, Valid Auroc: 0.6865\n",
            "Epoch: 520, Train Loss: 0.5098, Train auroc: 0.6897, Valid Loss: 0.5114, Valid Auroc: 0.6826\n",
            "Epoch: 521, Train Loss: 0.5050, Train auroc: 0.6942, Valid Loss: 0.4988, Valid Auroc: 0.6915\n",
            "Epoch: 522, Train Loss: 0.5099, Train auroc: 0.6940, Valid Loss: 0.4983, Valid Auroc: 0.6934\n",
            "Epoch: 523, Train Loss: 0.5047, Train auroc: 0.6955, Valid Loss: 0.5000, Valid Auroc: 0.6901\n",
            "Epoch: 524, Train Loss: 0.5120, Train auroc: 0.6890, Valid Loss: 0.5139, Valid Auroc: 0.6812\n",
            "Epoch: 525, Train Loss: 0.5047, Train auroc: 0.6941, Valid Loss: 0.5034, Valid Auroc: 0.6884\n",
            "Epoch: 526, Train Loss: 0.5092, Train auroc: 0.6935, Valid Loss: 0.4983, Valid Auroc: 0.6943\n",
            "Epoch: 527, Train Loss: 0.5067, Train auroc: 0.6945, Valid Loss: 0.4966, Valid Auroc: 0.6948\n",
            "Epoch: 528, Train Loss: 0.5070, Train auroc: 0.6921, Valid Loss: 0.5064, Valid Auroc: 0.6852\n",
            "Epoch: 529, Train Loss: 0.5126, Train auroc: 0.6876, Valid Loss: 0.5147, Valid Auroc: 0.6794\n",
            "Epoch: 530, Train Loss: 0.5044, Train auroc: 0.6939, Valid Loss: 0.5005, Valid Auroc: 0.6885\n",
            "Epoch: 531, Train Loss: 0.5097, Train auroc: 0.6941, Valid Loss: 0.4984, Valid Auroc: 0.6936\n",
            "Epoch: 532, Train Loss: 0.5098, Train auroc: 0.6934, Valid Loss: 0.4995, Valid Auroc: 0.6933\n",
            "Epoch: 533, Train Loss: 0.5066, Train auroc: 0.6918, Valid Loss: 0.5039, Valid Auroc: 0.6869\n",
            "Epoch: 534, Train Loss: 0.5079, Train auroc: 0.6900, Valid Loss: 0.5053, Valid Auroc: 0.6838\n",
            "Epoch: 535, Train Loss: 0.5073, Train auroc: 0.6910, Valid Loss: 0.5008, Valid Auroc: 0.6868\n",
            "Epoch: 536, Train Loss: 0.5035, Train auroc: 0.6944, Valid Loss: 0.4978, Valid Auroc: 0.6909\n",
            "Epoch: 537, Train Loss: 0.5055, Train auroc: 0.6938, Valid Loss: 0.5010, Valid Auroc: 0.6901\n",
            "Epoch: 538, Train Loss: 0.5035, Train auroc: 0.6948, Valid Loss: 0.4993, Valid Auroc: 0.6900\n",
            "Epoch: 539, Train Loss: 0.5038, Train auroc: 0.6952, Valid Loss: 0.4977, Valid Auroc: 0.6910\n",
            "Epoch: 540, Train Loss: 0.5030, Train auroc: 0.6962, Valid Loss: 0.4984, Valid Auroc: 0.6916\n",
            "Epoch: 541, Train Loss: 0.5022, Train auroc: 0.6971, Valid Loss: 0.4998, Valid Auroc: 0.6919\n",
            "Epoch: 542, Train Loss: 0.5016, Train auroc: 0.6980, Valid Loss: 0.4977, Valid Auroc: 0.6937\n",
            "Epoch: 543, Train Loss: 0.5014, Train auroc: 0.6988, Valid Loss: 0.4963, Valid Auroc: 0.6950\n",
            "Epoch: 544, Train Loss: 0.5001, Train auroc: 0.6997, Valid Loss: 0.4971, Valid Auroc: 0.6946\n",
            "Epoch: 545, Train Loss: 0.5002, Train auroc: 0.7001, Valid Loss: 0.4975, Valid Auroc: 0.6945\n",
            "Epoch: 546, Train Loss: 0.4996, Train auroc: 0.7008, Valid Loss: 0.4961, Valid Auroc: 0.6957\n",
            "Epoch: 547, Train Loss: 0.4994, Train auroc: 0.7011, Valid Loss: 0.4960, Valid Auroc: 0.6961\n",
            "Epoch: 548, Train Loss: 0.4992, Train auroc: 0.7016, Valid Loss: 0.4974, Valid Auroc: 0.6960\n",
            "Epoch: 549, Train Loss: 0.4988, Train auroc: 0.7023, Valid Loss: 0.4955, Valid Auroc: 0.6976\n",
            "Epoch: 550, Train Loss: 0.4985, Train auroc: 0.7027, Valid Loss: 0.4944, Valid Auroc: 0.6981\n",
            "Epoch: 551, Train Loss: 0.4985, Train auroc: 0.7028, Valid Loss: 0.4980, Valid Auroc: 0.6966\n",
            "Epoch: 552, Train Loss: 0.4982, Train auroc: 0.7037, Valid Loss: 0.4939, Valid Auroc: 0.6993\n",
            "Epoch: 553, Train Loss: 0.4977, Train auroc: 0.7039, Valid Loss: 0.4942, Valid Auroc: 0.6989\n",
            "Epoch: 554, Train Loss: 0.5001, Train auroc: 0.7024, Valid Loss: 0.5020, Valid Auroc: 0.6947\n",
            "Epoch: 555, Train Loss: 0.4997, Train auroc: 0.7040, Valid Loss: 0.4916, Valid Auroc: 0.7014\n",
            "Epoch: 556, Train Loss: 0.4974, Train auroc: 0.7047, Valid Loss: 0.4960, Valid Auroc: 0.6987\n",
            "Epoch: 557, Train Loss: 0.5051, Train auroc: 0.6995, Valid Loss: 0.5100, Valid Auroc: 0.6909\n",
            "Epoch: 558, Train Loss: 0.5042, Train auroc: 0.7013, Valid Loss: 0.4921, Valid Auroc: 0.7020\n",
            "Epoch: 559, Train Loss: 0.5002, Train auroc: 0.7038, Valid Loss: 0.4916, Valid Auroc: 0.7022\n",
            "Epoch: 560, Train Loss: 0.5157, Train auroc: 0.6932, Valid Loss: 0.5236, Valid Auroc: 0.6834\n",
            "Epoch: 561, Train Loss: 0.5031, Train auroc: 0.6991, Valid Loss: 0.5036, Valid Auroc: 0.6924\n",
            "Epoch: 562, Train Loss: 0.5187, Train auroc: 0.6916, Valid Loss: 0.5019, Valid Auroc: 0.6954\n",
            "Epoch: 563, Train Loss: 0.5058, Train auroc: 0.6987, Valid Loss: 0.4955, Valid Auroc: 0.6987\n",
            "Epoch: 564, Train Loss: 0.5108, Train auroc: 0.6909, Valid Loss: 0.5101, Valid Auroc: 0.6854\n",
            "Epoch: 565, Train Loss: 0.5168, Train auroc: 0.6850, Valid Loss: 0.5157, Valid Auroc: 0.6799\n",
            "Epoch: 566, Train Loss: 0.5098, Train auroc: 0.6850, Valid Loss: 0.5006, Valid Auroc: 0.6912\n",
            "Epoch: 567, Train Loss: 0.5086, Train auroc: 0.6905, Valid Loss: 0.4992, Valid Auroc: 0.6929\n",
            "Epoch: 568, Train Loss: 0.5079, Train auroc: 0.6892, Valid Loss: 0.5027, Valid Auroc: 0.6864\n",
            "Epoch: 569, Train Loss: 0.5095, Train auroc: 0.6868, Valid Loss: 0.5058, Valid Auroc: 0.6821\n",
            "Epoch: 570, Train Loss: 0.5095, Train auroc: 0.6876, Valid Loss: 0.5052, Valid Auroc: 0.6817\n",
            "Epoch: 571, Train Loss: 0.5071, Train auroc: 0.6902, Valid Loss: 0.5015, Valid Auroc: 0.6862\n",
            "Epoch: 572, Train Loss: 0.5081, Train auroc: 0.6900, Valid Loss: 0.5006, Valid Auroc: 0.6884\n",
            "Epoch: 573, Train Loss: 0.5065, Train auroc: 0.6908, Valid Loss: 0.4999, Valid Auroc: 0.6885\n",
            "Epoch: 574, Train Loss: 0.5059, Train auroc: 0.6913, Valid Loss: 0.5021, Valid Auroc: 0.6866\n",
            "Epoch: 575, Train Loss: 0.5052, Train auroc: 0.6920, Valid Loss: 0.5026, Valid Auroc: 0.6873\n",
            "Epoch: 576, Train Loss: 0.5041, Train auroc: 0.6942, Valid Loss: 0.4987, Valid Auroc: 0.6912\n",
            "Epoch: 577, Train Loss: 0.5036, Train auroc: 0.6962, Valid Loss: 0.4958, Valid Auroc: 0.6944\n",
            "Epoch: 578, Train Loss: 0.5027, Train auroc: 0.6977, Valid Loss: 0.4973, Valid Auroc: 0.6944\n",
            "Epoch: 579, Train Loss: 0.5035, Train auroc: 0.6972, Valid Loss: 0.5019, Valid Auroc: 0.6920\n",
            "Epoch: 580, Train Loss: 0.5022, Train auroc: 0.6982, Valid Loss: 0.4996, Valid Auroc: 0.6935\n",
            "Epoch: 581, Train Loss: 0.5031, Train auroc: 0.6992, Valid Loss: 0.4956, Valid Auroc: 0.6964\n",
            "Epoch: 582, Train Loss: 0.5009, Train auroc: 0.7001, Valid Loss: 0.4948, Valid Auroc: 0.6972\n",
            "Epoch: 583, Train Loss: 0.5032, Train auroc: 0.6988, Valid Loss: 0.5023, Valid Auroc: 0.6932\n",
            "Epoch: 584, Train Loss: 0.5017, Train auroc: 0.6994, Valid Loss: 0.5023, Valid Auroc: 0.6917\n",
            "Epoch: 585, Train Loss: 0.5024, Train auroc: 0.7012, Valid Loss: 0.4950, Valid Auroc: 0.6976\n",
            "Epoch: 586, Train Loss: 0.5018, Train auroc: 0.7015, Valid Loss: 0.4929, Valid Auroc: 0.6998\n",
            "Epoch: 587, Train Loss: 0.5026, Train auroc: 0.6996, Valid Loss: 0.5035, Valid Auroc: 0.6922\n",
            "Epoch: 588, Train Loss: 0.5053, Train auroc: 0.6974, Valid Loss: 0.5091, Valid Auroc: 0.6876\n",
            "Epoch: 589, Train Loss: 0.5014, Train auroc: 0.7025, Valid Loss: 0.4926, Valid Auroc: 0.7006\n",
            "Epoch: 590, Train Loss: 0.5076, Train auroc: 0.6993, Valid Loss: 0.4946, Valid Auroc: 0.7004\n",
            "Epoch: 591, Train Loss: 0.5013, Train auroc: 0.7010, Valid Loss: 0.5015, Valid Auroc: 0.6926\n",
            "Epoch: 592, Train Loss: 0.5120, Train auroc: 0.6937, Valid Loss: 0.5168, Valid Auroc: 0.6836\n",
            "Epoch: 593, Train Loss: 0.5012, Train auroc: 0.7013, Valid Loss: 0.4968, Valid Auroc: 0.6979\n",
            "Epoch: 594, Train Loss: 0.5114, Train auroc: 0.6964, Valid Loss: 0.4971, Valid Auroc: 0.6986\n",
            "Epoch: 595, Train Loss: 0.5030, Train auroc: 0.7024, Valid Loss: 0.4941, Valid Auroc: 0.7006\n",
            "Epoch: 596, Train Loss: 0.5066, Train auroc: 0.6963, Valid Loss: 0.5069, Valid Auroc: 0.6885\n",
            "Epoch: 597, Train Loss: 0.5131, Train auroc: 0.6918, Valid Loss: 0.5148, Valid Auroc: 0.6836\n",
            "Epoch: 598, Train Loss: 0.5016, Train auroc: 0.6993, Valid Loss: 0.4984, Valid Auroc: 0.6931\n",
            "Epoch: 599, Train Loss: 0.5079, Train auroc: 0.7000, Valid Loss: 0.4970, Valid Auroc: 0.6985\n",
            "Epoch: 600, Train Loss: 0.5075, Train auroc: 0.6998, Valid Loss: 0.4966, Valid Auroc: 0.6986\n",
            "Epoch: 601, Train Loss: 0.5016, Train auroc: 0.6994, Valid Loss: 0.4975, Valid Auroc: 0.6945\n",
            "Epoch: 602, Train Loss: 0.5076, Train auroc: 0.6940, Valid Loss: 0.5086, Valid Auroc: 0.6850\n",
            "Epoch: 603, Train Loss: 0.5062, Train auroc: 0.6951, Valid Loss: 0.5066, Valid Auroc: 0.6856\n",
            "Epoch: 604, Train Loss: 0.5010, Train auroc: 0.7009, Valid Loss: 0.4955, Valid Auroc: 0.6971\n",
            "Epoch: 605, Train Loss: 0.5059, Train auroc: 0.6991, Valid Loss: 0.4957, Valid Auroc: 0.6989\n",
            "Epoch: 606, Train Loss: 0.5026, Train auroc: 0.7007, Valid Loss: 0.4947, Valid Auroc: 0.6984\n",
            "Epoch: 607, Train Loss: 0.5011, Train auroc: 0.7004, Valid Loss: 0.4998, Valid Auroc: 0.6931\n",
            "Epoch: 608, Train Loss: 0.5055, Train auroc: 0.6977, Valid Loss: 0.5057, Valid Auroc: 0.6896\n",
            "Epoch: 609, Train Loss: 0.5013, Train auroc: 0.7008, Valid Loss: 0.4988, Valid Auroc: 0.6941\n",
            "Epoch: 610, Train Loss: 0.5013, Train auroc: 0.7040, Valid Loss: 0.4943, Valid Auroc: 0.6992\n",
            "Epoch: 611, Train Loss: 0.5014, Train auroc: 0.7032, Valid Loss: 0.4945, Valid Auroc: 0.6991\n",
            "Epoch: 612, Train Loss: 0.5004, Train auroc: 0.7035, Valid Loss: 0.4993, Valid Auroc: 0.6978\n",
            "Epoch: 613, Train Loss: 0.4996, Train auroc: 0.7031, Valid Loss: 0.4971, Valid Auroc: 0.6964\n",
            "Epoch: 614, Train Loss: 0.4998, Train auroc: 0.7044, Valid Loss: 0.4947, Valid Auroc: 0.6982\n",
            "Epoch: 615, Train Loss: 0.5016, Train auroc: 0.7041, Valid Loss: 0.4957, Valid Auroc: 0.6996\n",
            "Epoch: 616, Train Loss: 0.4985, Train auroc: 0.7048, Valid Loss: 0.4971, Valid Auroc: 0.6975\n",
            "Epoch: 617, Train Loss: 0.5035, Train auroc: 0.7018, Valid Loss: 0.5020, Valid Auroc: 0.6929\n",
            "Epoch: 618, Train Loss: 0.4983, Train auroc: 0.7055, Valid Loss: 0.4938, Valid Auroc: 0.7008\n",
            "Epoch: 619, Train Loss: 0.5033, Train auroc: 0.7040, Valid Loss: 0.4981, Valid Auroc: 0.7011\n",
            "Epoch: 620, Train Loss: 0.4994, Train auroc: 0.7046, Valid Loss: 0.4957, Valid Auroc: 0.6989\n",
            "Epoch: 621, Train Loss: 0.5063, Train auroc: 0.6998, Valid Loss: 0.5058, Valid Auroc: 0.6909\n",
            "Epoch: 622, Train Loss: 0.4991, Train auroc: 0.7042, Valid Loss: 0.4998, Valid Auroc: 0.6964\n",
            "Epoch: 623, Train Loss: 0.5054, Train auroc: 0.7023, Valid Loss: 0.4993, Valid Auroc: 0.6998\n",
            "Epoch: 624, Train Loss: 0.5016, Train auroc: 0.7038, Valid Loss: 0.4929, Valid Auroc: 0.7020\n",
            "Epoch: 625, Train Loss: 0.5041, Train auroc: 0.7006, Valid Loss: 0.5054, Valid Auroc: 0.6916\n",
            "Epoch: 626, Train Loss: 0.5032, Train auroc: 0.7010, Valid Loss: 0.5032, Valid Auroc: 0.6929\n",
            "Epoch: 627, Train Loss: 0.5003, Train auroc: 0.7029, Valid Loss: 0.4940, Valid Auroc: 0.6989\n",
            "Epoch: 628, Train Loss: 0.5018, Train auroc: 0.7025, Valid Loss: 0.4984, Valid Auroc: 0.6986\n",
            "Epoch: 629, Train Loss: 0.5021, Train auroc: 0.7025, Valid Loss: 0.5005, Valid Auroc: 0.6972\n",
            "Epoch: 630, Train Loss: 0.4997, Train auroc: 0.7036, Valid Loss: 0.4958, Valid Auroc: 0.6973\n",
            "Epoch: 631, Train Loss: 0.5030, Train auroc: 0.7015, Valid Loss: 0.4974, Valid Auroc: 0.6953\n",
            "Epoch: 632, Train Loss: 0.5010, Train auroc: 0.7022, Valid Loss: 0.5021, Valid Auroc: 0.6936\n",
            "Epoch: 633, Train Loss: 0.5002, Train auroc: 0.7043, Valid Loss: 0.5021, Valid Auroc: 0.6960\n",
            "Epoch: 634, Train Loss: 0.4998, Train auroc: 0.7053, Valid Loss: 0.4936, Valid Auroc: 0.7012\n",
            "Epoch: 635, Train Loss: 0.5018, Train auroc: 0.7050, Valid Loss: 0.4936, Valid Auroc: 0.7016\n",
            "Epoch: 636, Train Loss: 0.4989, Train auroc: 0.7052, Valid Loss: 0.5015, Valid Auroc: 0.6955\n",
            "Epoch: 637, Train Loss: 0.5024, Train auroc: 0.7035, Valid Loss: 0.5073, Valid Auroc: 0.6936\n",
            "Epoch: 638, Train Loss: 0.5019, Train auroc: 0.7059, Valid Loss: 0.4937, Valid Auroc: 0.7017\n",
            "Epoch: 639, Train Loss: 0.4982, Train auroc: 0.7083, Valid Loss: 0.4919, Valid Auroc: 0.7033\n",
            "Epoch: 640, Train Loss: 0.5068, Train auroc: 0.7003, Valid Loss: 0.5079, Valid Auroc: 0.6917\n",
            "Epoch: 641, Train Loss: 0.5044, Train auroc: 0.7003, Valid Loss: 0.5068, Valid Auroc: 0.6904\n",
            "Epoch: 642, Train Loss: 0.4985, Train auroc: 0.7060, Valid Loss: 0.4936, Valid Auroc: 0.6997\n",
            "Epoch: 643, Train Loss: 0.5063, Train auroc: 0.7045, Valid Loss: 0.4961, Valid Auroc: 0.7010\n",
            "Epoch: 644, Train Loss: 0.4993, Train auroc: 0.7062, Valid Loss: 0.4974, Valid Auroc: 0.6995\n",
            "Epoch: 645, Train Loss: 0.5028, Train auroc: 0.7016, Valid Loss: 0.5062, Valid Auroc: 0.6915\n",
            "Epoch: 646, Train Loss: 0.5014, Train auroc: 0.7027, Valid Loss: 0.5003, Valid Auroc: 0.6934\n",
            "Epoch: 647, Train Loss: 0.5029, Train auroc: 0.7053, Valid Loss: 0.4939, Valid Auroc: 0.7021\n",
            "Epoch: 648, Train Loss: 0.5003, Train auroc: 0.7062, Valid Loss: 0.4928, Valid Auroc: 0.7033\n",
            "Epoch: 649, Train Loss: 0.4989, Train auroc: 0.7051, Valid Loss: 0.4986, Valid Auroc: 0.6979\n",
            "Epoch: 650, Train Loss: 0.5040, Train auroc: 0.7020, Valid Loss: 0.5068, Valid Auroc: 0.6923\n",
            "Epoch: 651, Train Loss: 0.4972, Train auroc: 0.7066, Valid Loss: 0.4951, Valid Auroc: 0.6995\n",
            "Epoch: 652, Train Loss: 0.5005, Train auroc: 0.7057, Valid Loss: 0.4927, Valid Auroc: 0.7030\n",
            "Epoch: 653, Train Loss: 0.4997, Train auroc: 0.7065, Valid Loss: 0.4948, Valid Auroc: 0.7027\n",
            "Epoch: 654, Train Loss: 0.4978, Train auroc: 0.7065, Valid Loss: 0.4994, Valid Auroc: 0.6971\n",
            "Epoch: 655, Train Loss: 0.5007, Train auroc: 0.7041, Valid Loss: 0.5020, Valid Auroc: 0.6931\n",
            "Epoch: 656, Train Loss: 0.4982, Train auroc: 0.7070, Valid Loss: 0.4931, Valid Auroc: 0.7010\n",
            "Epoch: 657, Train Loss: 0.4995, Train auroc: 0.7078, Valid Loss: 0.4973, Valid Auroc: 0.7023\n",
            "Epoch: 658, Train Loss: 0.4983, Train auroc: 0.7077, Valid Loss: 0.4971, Valid Auroc: 0.7020\n",
            "Epoch: 659, Train Loss: 0.5001, Train auroc: 0.7075, Valid Loss: 0.4955, Valid Auroc: 0.7000\n",
            "Epoch: 660, Train Loss: 0.5014, Train auroc: 0.7069, Valid Loss: 0.4982, Valid Auroc: 0.6982\n",
            "Epoch: 661, Train Loss: 0.4974, Train auroc: 0.7075, Valid Loss: 0.4994, Valid Auroc: 0.6987\n",
            "Epoch: 662, Train Loss: 0.5009, Train auroc: 0.7077, Valid Loss: 0.5007, Valid Auroc: 0.7011\n",
            "Epoch: 663, Train Loss: 0.4986, Train auroc: 0.7073, Valid Loss: 0.4917, Valid Auroc: 0.7039\n",
            "Epoch: 664, Train Loss: 0.4979, Train auroc: 0.7068, Valid Loss: 0.4948, Valid Auroc: 0.7006\n",
            "Epoch: 665, Train Loss: 0.5037, Train auroc: 0.7036, Valid Loss: 0.5086, Valid Auroc: 0.6933\n",
            "Epoch: 666, Train Loss: 0.4970, Train auroc: 0.7079, Valid Loss: 0.4980, Valid Auroc: 0.7001\n",
            "Epoch: 667, Train Loss: 0.5026, Train auroc: 0.7075, Valid Loss: 0.4914, Valid Auroc: 0.7060\n",
            "Epoch: 668, Train Loss: 0.4966, Train auroc: 0.7103, Valid Loss: 0.4916, Valid Auroc: 0.7056\n",
            "Epoch: 669, Train Loss: 0.5027, Train auroc: 0.7049, Valid Loss: 0.5079, Valid Auroc: 0.6943\n",
            "Epoch: 670, Train Loss: 0.5057, Train auroc: 0.7014, Valid Loss: 0.5073, Valid Auroc: 0.6885\n",
            "Epoch: 671, Train Loss: 0.4991, Train auroc: 0.7083, Valid Loss: 0.4925, Valid Auroc: 0.7028\n",
            "Epoch: 672, Train Loss: 0.5010, Train auroc: 0.7061, Valid Loss: 0.4956, Valid Auroc: 0.7031\n",
            "Epoch: 673, Train Loss: 0.5005, Train auroc: 0.7068, Valid Loss: 0.4984, Valid Auroc: 0.7022\n",
            "Epoch: 674, Train Loss: 0.4998, Train auroc: 0.7065, Valid Loss: 0.4958, Valid Auroc: 0.7005\n",
            "Epoch: 675, Train Loss: 0.4992, Train auroc: 0.7068, Valid Loss: 0.5000, Valid Auroc: 0.6981\n",
            "Epoch: 676, Train Loss: 0.4984, Train auroc: 0.7067, Valid Loss: 0.4986, Valid Auroc: 0.6998\n",
            "Epoch: 677, Train Loss: 0.5000, Train auroc: 0.7077, Valid Loss: 0.4914, Valid Auroc: 0.7038\n",
            "Epoch: 678, Train Loss: 0.4954, Train auroc: 0.7095, Valid Loss: 0.4950, Valid Auroc: 0.7009\n",
            "Epoch: 679, Train Loss: 0.5015, Train auroc: 0.7064, Valid Loss: 0.5075, Valid Auroc: 0.6959\n",
            "Epoch: 680, Train Loss: 0.4938, Train auroc: 0.7112, Valid Loss: 0.4910, Valid Auroc: 0.7049\n",
            "Epoch: 681, Train Loss: 0.5028, Train auroc: 0.7087, Valid Loss: 0.4906, Valid Auroc: 0.7073\n",
            "Epoch: 682, Train Loss: 0.4946, Train auroc: 0.7113, Valid Loss: 0.4938, Valid Auroc: 0.7042\n",
            "Epoch: 683, Train Loss: 0.5052, Train auroc: 0.7049, Valid Loss: 0.5116, Valid Auroc: 0.6940\n",
            "Epoch: 684, Train Loss: 0.4959, Train auroc: 0.7101, Valid Loss: 0.4960, Valid Auroc: 0.6995\n",
            "Epoch: 685, Train Loss: 0.5056, Train auroc: 0.7097, Valid Loss: 0.4939, Valid Auroc: 0.7056\n",
            "Epoch: 686, Train Loss: 0.4961, Train auroc: 0.7119, Valid Loss: 0.4909, Valid Auroc: 0.7060\n",
            "Epoch: 687, Train Loss: 0.5060, Train auroc: 0.7048, Valid Loss: 0.5073, Valid Auroc: 0.6968\n",
            "Epoch: 688, Train Loss: 0.5006, Train auroc: 0.7049, Valid Loss: 0.5024, Valid Auroc: 0.6952\n",
            "Epoch: 689, Train Loss: 0.5072, Train auroc: 0.7042, Valid Loss: 0.4985, Valid Auroc: 0.7006\n",
            "Epoch: 690, Train Loss: 0.5010, Train auroc: 0.7085, Valid Loss: 0.4920, Valid Auroc: 0.7061\n",
            "Epoch: 691, Train Loss: 0.5010, Train auroc: 0.7072, Valid Loss: 0.5003, Valid Auroc: 0.7004\n",
            "Epoch: 692, Train Loss: 0.5059, Train auroc: 0.7038, Valid Loss: 0.5089, Valid Auroc: 0.6951\n",
            "Epoch: 693, Train Loss: 0.4985, Train auroc: 0.7056, Valid Loss: 0.4980, Valid Auroc: 0.6953\n",
            "Epoch: 694, Train Loss: 0.5048, Train auroc: 0.7067, Valid Loss: 0.4962, Valid Auroc: 0.7004\n",
            "Epoch: 695, Train Loss: 0.4981, Train auroc: 0.7061, Valid Loss: 0.4958, Valid Auroc: 0.7000\n",
            "Epoch: 696, Train Loss: 0.5011, Train auroc: 0.7042, Valid Loss: 0.5043, Valid Auroc: 0.6952\n",
            "Epoch: 697, Train Loss: 0.4992, Train auroc: 0.7077, Valid Loss: 0.4951, Valid Auroc: 0.7003\n",
            "Epoch: 698, Train Loss: 0.5020, Train auroc: 0.7077, Valid Loss: 0.4919, Valid Auroc: 0.7049\n",
            "Epoch: 699, Train Loss: 0.4959, Train auroc: 0.7097, Valid Loss: 0.4934, Valid Auroc: 0.7033\n",
            "Epoch: 700, Train Loss: 0.5025, Train auroc: 0.7071, Valid Loss: 0.5074, Valid Auroc: 0.6974\n",
            "Epoch: 701, Train Loss: 0.4954, Train auroc: 0.7095, Valid Loss: 0.4978, Valid Auroc: 0.6991\n",
            "Epoch: 702, Train Loss: 0.5009, Train auroc: 0.7105, Valid Loss: 0.4920, Valid Auroc: 0.7054\n",
            "Epoch: 703, Train Loss: 0.4981, Train auroc: 0.7125, Valid Loss: 0.4890, Valid Auroc: 0.7083\n",
            "Epoch: 704, Train Loss: 0.4971, Train auroc: 0.7101, Valid Loss: 0.4989, Valid Auroc: 0.7012\n",
            "Epoch: 705, Train Loss: 0.5016, Train auroc: 0.7080, Valid Loss: 0.5072, Valid Auroc: 0.6973\n",
            "Epoch: 706, Train Loss: 0.4934, Train auroc: 0.7131, Valid Loss: 0.4908, Valid Auroc: 0.7058\n",
            "Epoch: 707, Train Loss: 0.5019, Train auroc: 0.7101, Valid Loss: 0.4903, Valid Auroc: 0.7084\n",
            "Epoch: 708, Train Loss: 0.4938, Train auroc: 0.7133, Valid Loss: 0.4912, Valid Auroc: 0.7056\n",
            "Epoch: 709, Train Loss: 0.5018, Train auroc: 0.7071, Valid Loss: 0.5095, Valid Auroc: 0.6949\n",
            "Epoch: 710, Train Loss: 0.4939, Train auroc: 0.7119, Valid Loss: 0.4943, Valid Auroc: 0.7028\n",
            "Epoch: 711, Train Loss: 0.4982, Train auroc: 0.7132, Valid Loss: 0.4888, Valid Auroc: 0.7093\n",
            "Epoch: 712, Train Loss: 0.4954, Train auroc: 0.7145, Valid Loss: 0.4884, Valid Auroc: 0.7095\n",
            "Epoch: 713, Train Loss: 0.4950, Train auroc: 0.7121, Valid Loss: 0.4972, Valid Auroc: 0.7017\n",
            "Epoch: 714, Train Loss: 0.4979, Train auroc: 0.7108, Valid Loss: 0.5007, Valid Auroc: 0.7015\n",
            "Epoch: 715, Train Loss: 0.4929, Train auroc: 0.7137, Valid Loss: 0.4889, Valid Auroc: 0.7080\n",
            "Epoch: 716, Train Loss: 0.4964, Train auroc: 0.7134, Valid Loss: 0.4897, Valid Auroc: 0.7082\n",
            "Epoch: 717, Train Loss: 0.4922, Train auroc: 0.7150, Valid Loss: 0.4916, Valid Auroc: 0.7066\n",
            "Epoch: 718, Train Loss: 0.4962, Train auroc: 0.7136, Valid Loss: 0.4977, Valid Auroc: 0.7049\n",
            "Epoch: 719, Train Loss: 0.4921, Train auroc: 0.7153, Valid Loss: 0.4922, Valid Auroc: 0.7063\n",
            "Epoch: 720, Train Loss: 0.4960, Train auroc: 0.7149, Valid Loss: 0.4920, Valid Auroc: 0.7067\n",
            "Epoch: 721, Train Loss: 0.4943, Train auroc: 0.7158, Valid Loss: 0.4890, Valid Auroc: 0.7085\n",
            "Epoch: 722, Train Loss: 0.4954, Train auroc: 0.7147, Valid Loss: 0.4936, Valid Auroc: 0.7073\n",
            "Epoch: 723, Train Loss: 0.4964, Train auroc: 0.7143, Valid Loss: 0.4979, Valid Auroc: 0.7058\n",
            "Epoch: 724, Train Loss: 0.4932, Train auroc: 0.7149, Valid Loss: 0.4917, Valid Auroc: 0.7068\n",
            "Epoch: 725, Train Loss: 0.4977, Train auroc: 0.7140, Valid Loss: 0.4912, Valid Auroc: 0.7078\n",
            "Epoch: 726, Train Loss: 0.4923, Train auroc: 0.7149, Valid Loss: 0.4912, Valid Auroc: 0.7063\n",
            "Epoch: 727, Train Loss: 0.4982, Train auroc: 0.7114, Valid Loss: 0.5021, Valid Auroc: 0.7016\n",
            "Epoch: 728, Train Loss: 0.4931, Train auroc: 0.7142, Valid Loss: 0.4967, Valid Auroc: 0.7033\n",
            "Epoch: 729, Train Loss: 0.4961, Train auroc: 0.7146, Valid Loss: 0.4899, Valid Auroc: 0.7074\n",
            "Epoch: 730, Train Loss: 0.4946, Train auroc: 0.7159, Valid Loss: 0.4894, Valid Auroc: 0.7082\n",
            "Epoch: 731, Train Loss: 0.4953, Train auroc: 0.7129, Valid Loss: 0.5013, Valid Auroc: 0.7008\n",
            "Epoch: 732, Train Loss: 0.4992, Train auroc: 0.7127, Valid Loss: 0.4973, Valid Auroc: 0.7063\n",
            "Epoch: 733, Train Loss: 0.4910, Train auroc: 0.7165, Valid Loss: 0.4882, Valid Auroc: 0.7092\n",
            "Epoch: 734, Train Loss: 0.5009, Train auroc: 0.7115, Valid Loss: 0.4987, Valid Auroc: 0.7012\n",
            "Epoch: 735, Train Loss: 0.4952, Train auroc: 0.7143, Valid Loss: 0.4946, Valid Auroc: 0.7036\n",
            "Epoch: 736, Train Loss: 0.4955, Train auroc: 0.7147, Valid Loss: 0.4941, Valid Auroc: 0.7070\n",
            "Epoch: 737, Train Loss: 0.4962, Train auroc: 0.7141, Valid Loss: 0.4941, Valid Auroc: 0.7077\n",
            "Epoch: 738, Train Loss: 0.4918, Train auroc: 0.7150, Valid Loss: 0.4931, Valid Auroc: 0.7057\n",
            "Epoch: 739, Train Loss: 0.4960, Train auroc: 0.7135, Valid Loss: 0.4960, Valid Auroc: 0.7036\n",
            "Epoch: 740, Train Loss: 0.4939, Train auroc: 0.7154, Valid Loss: 0.4900, Valid Auroc: 0.7084\n",
            "Epoch: 741, Train Loss: 0.4913, Train auroc: 0.7169, Valid Loss: 0.4913, Valid Auroc: 0.7092\n",
            "Epoch: 742, Train Loss: 0.4955, Train auroc: 0.7157, Valid Loss: 0.4983, Valid Auroc: 0.7071\n",
            "Epoch: 743, Train Loss: 0.4911, Train auroc: 0.7169, Valid Loss: 0.4898, Valid Auroc: 0.7094\n",
            "Epoch: 744, Train Loss: 0.4942, Train auroc: 0.7165, Valid Loss: 0.4894, Valid Auroc: 0.7092\n",
            "Epoch: 745, Train Loss: 0.4926, Train auroc: 0.7167, Valid Loss: 0.4943, Valid Auroc: 0.7056\n",
            "Epoch: 746, Train Loss: 0.4924, Train auroc: 0.7167, Valid Loss: 0.4983, Valid Auroc: 0.7051\n",
            "Epoch: 747, Train Loss: 0.4933, Train auroc: 0.7178, Valid Loss: 0.4916, Valid Auroc: 0.7106\n",
            "Epoch: 748, Train Loss: 0.4919, Train auroc: 0.7179, Valid Loss: 0.4870, Valid Auroc: 0.7122\n",
            "Epoch: 749, Train Loss: 0.4939, Train auroc: 0.7155, Valid Loss: 0.4969, Valid Auroc: 0.7044\n",
            "Epoch: 750, Train Loss: 0.4936, Train auroc: 0.7161, Valid Loss: 0.4988, Valid Auroc: 0.7044\n",
            "Epoch: 751, Train Loss: 0.4934, Train auroc: 0.7179, Valid Loss: 0.4878, Valid Auroc: 0.7121\n",
            "Epoch: 752, Train Loss: 0.4939, Train auroc: 0.7174, Valid Loss: 0.4902, Valid Auroc: 0.7119\n",
            "Epoch: 753, Train Loss: 0.4946, Train auroc: 0.7164, Valid Loss: 0.4999, Valid Auroc: 0.7058\n",
            "Epoch: 754, Train Loss: 0.4950, Train auroc: 0.7152, Valid Loss: 0.4986, Valid Auroc: 0.7034\n",
            "Epoch: 755, Train Loss: 0.4962, Train auroc: 0.7169, Valid Loss: 0.4897, Valid Auroc: 0.7100\n",
            "Epoch: 756, Train Loss: 0.4910, Train auroc: 0.7186, Valid Loss: 0.4894, Valid Auroc: 0.7109\n",
            "Epoch: 757, Train Loss: 0.5005, Train auroc: 0.7147, Valid Loss: 0.5051, Valid Auroc: 0.7057\n",
            "Epoch: 758, Train Loss: 0.4909, Train auroc: 0.7173, Valid Loss: 0.4898, Valid Auroc: 0.7099\n",
            "Epoch: 759, Train Loss: 0.5007, Train auroc: 0.7151, Valid Loss: 0.4901, Valid Auroc: 0.7113\n",
            "Epoch: 760, Train Loss: 0.4948, Train auroc: 0.7165, Valid Loss: 0.4949, Valid Auroc: 0.7057\n",
            "Epoch: 761, Train Loss: 0.4971, Train auroc: 0.7134, Valid Loss: 0.5070, Valid Auroc: 0.7002\n",
            "Epoch: 762, Train Loss: 0.4969, Train auroc: 0.7156, Valid Loss: 0.4983, Valid Auroc: 0.7075\n",
            "Epoch: 763, Train Loss: 0.4986, Train auroc: 0.7152, Valid Loss: 0.4898, Valid Auroc: 0.7114\n",
            "Epoch: 764, Train Loss: 0.4931, Train auroc: 0.7177, Valid Loss: 0.4888, Valid Auroc: 0.7109\n",
            "Epoch: 765, Train Loss: 0.5013, Train auroc: 0.7105, Valid Loss: 0.5068, Valid Auroc: 0.6978\n",
            "Epoch: 766, Train Loss: 0.4942, Train auroc: 0.7145, Valid Loss: 0.4963, Valid Auroc: 0.7056\n",
            "Epoch: 767, Train Loss: 0.4979, Train auroc: 0.7123, Valid Loss: 0.4898, Valid Auroc: 0.7103\n",
            "Epoch: 768, Train Loss: 0.4963, Train auroc: 0.7142, Valid Loss: 0.4913, Valid Auroc: 0.7104\n",
            "Epoch: 769, Train Loss: 0.4941, Train auroc: 0.7152, Valid Loss: 0.4970, Valid Auroc: 0.7069\n",
            "Epoch: 770, Train Loss: 0.4963, Train auroc: 0.7123, Valid Loss: 0.4983, Valid Auroc: 0.7025\n",
            "Epoch: 771, Train Loss: 0.4970, Train auroc: 0.7147, Valid Loss: 0.4935, Valid Auroc: 0.7057\n",
            "Epoch: 772, Train Loss: 0.4923, Train auroc: 0.7175, Valid Loss: 0.4893, Valid Auroc: 0.7094\n",
            "Epoch: 773, Train Loss: 0.4960, Train auroc: 0.7137, Valid Loss: 0.4970, Valid Auroc: 0.7061\n",
            "Epoch: 774, Train Loss: 0.4945, Train auroc: 0.7145, Valid Loss: 0.4952, Valid Auroc: 0.7074\n",
            "Epoch: 775, Train Loss: 0.4935, Train auroc: 0.7147, Valid Loss: 0.4880, Valid Auroc: 0.7094\n",
            "Epoch: 776, Train Loss: 0.4947, Train auroc: 0.7142, Valid Loss: 0.4915, Valid Auroc: 0.7072\n",
            "Epoch: 777, Train Loss: 0.4944, Train auroc: 0.7145, Valid Loss: 0.4989, Valid Auroc: 0.7040\n",
            "Epoch: 778, Train Loss: 0.4921, Train auroc: 0.7170, Valid Loss: 0.4926, Valid Auroc: 0.7092\n",
            "Epoch: 779, Train Loss: 0.4956, Train auroc: 0.7169, Valid Loss: 0.4882, Valid Auroc: 0.7124\n",
            "Epoch: 780, Train Loss: 0.4898, Train auroc: 0.7199, Valid Loss: 0.4873, Valid Auroc: 0.7123\n",
            "Epoch: 781, Train Loss: 0.4961, Train auroc: 0.7145, Valid Loss: 0.5030, Valid Auroc: 0.7015\n",
            "Epoch: 782, Train Loss: 0.4911, Train auroc: 0.7176, Valid Loss: 0.4930, Valid Auroc: 0.7074\n",
            "Epoch: 783, Train Loss: 0.4939, Train auroc: 0.7181, Valid Loss: 0.4855, Valid Auroc: 0.7143\n",
            "Epoch: 784, Train Loss: 0.4920, Train auroc: 0.7194, Valid Loss: 0.4878, Valid Auroc: 0.7136\n",
            "Epoch: 785, Train Loss: 0.4931, Train auroc: 0.7189, Valid Loss: 0.4981, Valid Auroc: 0.7086\n",
            "Epoch: 786, Train Loss: 0.4929, Train auroc: 0.7180, Valid Loss: 0.4957, Valid Auroc: 0.7073\n",
            "Epoch: 787, Train Loss: 0.4977, Train auroc: 0.7183, Valid Loss: 0.4904, Valid Auroc: 0.7117\n",
            "Epoch: 788, Train Loss: 0.4895, Train auroc: 0.7212, Valid Loss: 0.4867, Valid Auroc: 0.7139\n",
            "Epoch: 789, Train Loss: 0.5004, Train auroc: 0.7162, Valid Loss: 0.5032, Valid Auroc: 0.7081\n",
            "Epoch: 790, Train Loss: 0.4902, Train auroc: 0.7192, Valid Loss: 0.4912, Valid Auroc: 0.7106\n",
            "Epoch: 791, Train Loss: 0.4988, Train auroc: 0.7164, Valid Loss: 0.4917, Valid Auroc: 0.7102\n",
            "Epoch: 792, Train Loss: 0.4938, Train auroc: 0.7176, Valid Loss: 0.4930, Valid Auroc: 0.7071\n",
            "Epoch: 793, Train Loss: 0.4970, Train auroc: 0.7142, Valid Loss: 0.5031, Valid Auroc: 0.7021\n",
            "Epoch: 794, Train Loss: 0.4938, Train auroc: 0.7165, Valid Loss: 0.4936, Valid Auroc: 0.7082\n",
            "Epoch: 795, Train Loss: 0.4924, Train auroc: 0.7189, Valid Loss: 0.4861, Valid Auroc: 0.7131\n",
            "Epoch: 796, Train Loss: 0.4927, Train auroc: 0.7170, Valid Loss: 0.4958, Valid Auroc: 0.7049\n",
            "Epoch: 797, Train Loss: 0.4898, Train auroc: 0.7192, Valid Loss: 0.4925, Valid Auroc: 0.7082\n",
            "Epoch: 798, Train Loss: 0.4919, Train auroc: 0.7194, Valid Loss: 0.4851, Valid Auroc: 0.7143\n",
            "Epoch: 799, Train Loss: 0.4894, Train auroc: 0.7204, Valid Loss: 0.4879, Valid Auroc: 0.7132\n",
            "Epoch: 800, Train Loss: 0.4908, Train auroc: 0.7206, Valid Loss: 0.4959, Valid Auroc: 0.7099\n",
            "Epoch: 801, Train Loss: 0.4877, Train auroc: 0.7224, Valid Loss: 0.4857, Valid Auroc: 0.7137\n",
            "Epoch: 802, Train Loss: 0.4897, Train auroc: 0.7228, Valid Loss: 0.4838, Valid Auroc: 0.7158\n",
            "Epoch: 803, Train Loss: 0.4870, Train auroc: 0.7231, Valid Loss: 0.4889, Valid Auroc: 0.7128\n",
            "Epoch: 804, Train Loss: 0.4905, Train auroc: 0.7214, Valid Loss: 0.4956, Valid Auroc: 0.7098\n",
            "Epoch: 805, Train Loss: 0.4868, Train auroc: 0.7239, Valid Loss: 0.4839, Valid Auroc: 0.7163\n",
            "Epoch: 806, Train Loss: 0.4915, Train auroc: 0.7230, Valid Loss: 0.4840, Valid Auroc: 0.7175\n",
            "Epoch: 807, Train Loss: 0.4863, Train auroc: 0.7243, Valid Loss: 0.4903, Valid Auroc: 0.7126\n",
            "Epoch: 808, Train Loss: 0.4937, Train auroc: 0.7216, Valid Loss: 0.5013, Valid Auroc: 0.7087\n",
            "Epoch: 809, Train Loss: 0.4862, Train auroc: 0.7256, Valid Loss: 0.4829, Valid Auroc: 0.7175\n",
            "Epoch: 810, Train Loss: 0.4966, Train auroc: 0.7225, Valid Loss: 0.4860, Valid Auroc: 0.7181\n",
            "Epoch: 811, Train Loss: 0.4864, Train auroc: 0.7245, Valid Loss: 0.4891, Valid Auroc: 0.7130\n",
            "Epoch: 812, Train Loss: 0.5004, Train auroc: 0.7176, Valid Loss: 0.5082, Valid Auroc: 0.7049\n",
            "Epoch: 813, Train Loss: 0.4871, Train auroc: 0.7235, Valid Loss: 0.4866, Valid Auroc: 0.7148\n",
            "Epoch: 814, Train Loss: 0.5002, Train auroc: 0.7199, Valid Loss: 0.4888, Valid Auroc: 0.7170\n",
            "Epoch: 815, Train Loss: 0.4905, Train auroc: 0.7249, Valid Loss: 0.4862, Valid Auroc: 0.7167\n",
            "Epoch: 816, Train Loss: 0.4942, Train auroc: 0.7189, Valid Loss: 0.5012, Valid Auroc: 0.7046\n",
            "Epoch: 817, Train Loss: 0.4990, Train auroc: 0.7177, Valid Loss: 0.5052, Valid Auroc: 0.7048\n",
            "Epoch: 818, Train Loss: 0.4896, Train auroc: 0.7227, Valid Loss: 0.4856, Valid Auroc: 0.7137\n",
            "Epoch: 819, Train Loss: 0.4998, Train auroc: 0.7212, Valid Loss: 0.4909, Valid Auroc: 0.7143\n",
            "Epoch: 820, Train Loss: 0.4899, Train auroc: 0.7224, Valid Loss: 0.4883, Valid Auroc: 0.7123\n",
            "Epoch: 821, Train Loss: 0.4948, Train auroc: 0.7170, Valid Loss: 0.4964, Valid Auroc: 0.7082\n",
            "Epoch: 822, Train Loss: 0.4973, Train auroc: 0.7162, Valid Loss: 0.4971, Valid Auroc: 0.7097\n",
            "Epoch: 823, Train Loss: 0.4902, Train auroc: 0.7185, Valid Loss: 0.4860, Valid Auroc: 0.7134\n",
            "Epoch: 824, Train Loss: 0.4979, Train auroc: 0.7171, Valid Loss: 0.4920, Valid Auroc: 0.7110\n",
            "Epoch: 825, Train Loss: 0.4900, Train auroc: 0.7208, Valid Loss: 0.4897, Valid Auroc: 0.7107\n",
            "Epoch: 826, Train Loss: 0.4922, Train auroc: 0.7190, Valid Loss: 0.4940, Valid Auroc: 0.7091\n",
            "Epoch: 827, Train Loss: 0.4938, Train auroc: 0.7191, Valid Loss: 0.4957, Valid Auroc: 0.7093\n",
            "Epoch: 828, Train Loss: 0.4881, Train auroc: 0.7215, Valid Loss: 0.4884, Valid Auroc: 0.7105\n",
            "Epoch: 829, Train Loss: 0.4932, Train auroc: 0.7207, Valid Loss: 0.4909, Valid Auroc: 0.7098\n",
            "Epoch: 830, Train Loss: 0.4892, Train auroc: 0.7237, Valid Loss: 0.4878, Valid Auroc: 0.7135\n",
            "Epoch: 831, Train Loss: 0.4880, Train auroc: 0.7235, Valid Loss: 0.4889, Valid Auroc: 0.7137\n",
            "Epoch: 832, Train Loss: 0.4911, Train auroc: 0.7228, Valid Loss: 0.4930, Valid Auroc: 0.7131\n",
            "Epoch: 833, Train Loss: 0.4860, Train auroc: 0.7250, Valid Loss: 0.4856, Valid Auroc: 0.7157\n",
            "Epoch: 834, Train Loss: 0.4898, Train auroc: 0.7242, Valid Loss: 0.4866, Valid Auroc: 0.7158\n",
            "Epoch: 835, Train Loss: 0.4881, Train auroc: 0.7251, Valid Loss: 0.4875, Valid Auroc: 0.7157\n",
            "Epoch: 836, Train Loss: 0.4861, Train auroc: 0.7262, Valid Loss: 0.4891, Valid Auroc: 0.7157\n",
            "Epoch: 837, Train Loss: 0.4898, Train auroc: 0.7249, Valid Loss: 0.4917, Valid Auroc: 0.7150\n",
            "Epoch: 838, Train Loss: 0.4850, Train auroc: 0.7267, Valid Loss: 0.4845, Valid Auroc: 0.7169\n",
            "Epoch: 839, Train Loss: 0.4881, Train auroc: 0.7257, Valid Loss: 0.4873, Valid Auroc: 0.7153\n",
            "Epoch: 840, Train Loss: 0.4878, Train auroc: 0.7256, Valid Loss: 0.4894, Valid Auroc: 0.7144\n",
            "Epoch: 841, Train Loss: 0.4853, Train auroc: 0.7271, Valid Loss: 0.4871, Valid Auroc: 0.7168\n",
            "Epoch: 842, Train Loss: 0.4884, Train auroc: 0.7263, Valid Loss: 0.4880, Valid Auroc: 0.7176\n",
            "Epoch: 843, Train Loss: 0.4855, Train auroc: 0.7273, Valid Loss: 0.4842, Valid Auroc: 0.7183\n",
            "Epoch: 844, Train Loss: 0.4866, Train auroc: 0.7267, Valid Loss: 0.4877, Valid Auroc: 0.7158\n",
            "Epoch: 845, Train Loss: 0.4871, Train auroc: 0.7263, Valid Loss: 0.4902, Valid Auroc: 0.7150\n",
            "Epoch: 846, Train Loss: 0.4862, Train auroc: 0.7275, Valid Loss: 0.4857, Valid Auroc: 0.7184\n",
            "Epoch: 847, Train Loss: 0.4843, Train auroc: 0.7285, Valid Loss: 0.4844, Valid Auroc: 0.7189\n",
            "Epoch: 848, Train Loss: 0.4829, Train auroc: 0.7282, Valid Loss: 0.4841, Valid Auroc: 0.7186\n",
            "Epoch: 849, Train Loss: 0.4831, Train auroc: 0.7288, Valid Loss: 0.4837, Valid Auroc: 0.7190\n",
            "Epoch: 850, Train Loss: 0.4820, Train auroc: 0.7305, Valid Loss: 0.4834, Valid Auroc: 0.7201\n",
            "Epoch: 851, Train Loss: 0.4823, Train auroc: 0.7309, Valid Loss: 0.4848, Valid Auroc: 0.7204\n",
            "Epoch: 852, Train Loss: 0.4824, Train auroc: 0.7315, Valid Loss: 0.4803, Valid Auroc: 0.7227\n",
            "Epoch: 853, Train Loss: 0.4811, Train auroc: 0.7320, Valid Loss: 0.4819, Valid Auroc: 0.7215\n",
            "Epoch: 854, Train Loss: 0.4827, Train auroc: 0.7308, Valid Loss: 0.4883, Valid Auroc: 0.7182\n",
            "Epoch: 855, Train Loss: 0.4808, Train auroc: 0.7324, Valid Loss: 0.4807, Valid Auroc: 0.7228\n",
            "Epoch: 856, Train Loss: 0.4829, Train auroc: 0.7318, Valid Loss: 0.4798, Valid Auroc: 0.7236\n",
            "Epoch: 857, Train Loss: 0.4826, Train auroc: 0.7317, Valid Loss: 0.4889, Valid Auroc: 0.7185\n",
            "Epoch: 858, Train Loss: 0.4833, Train auroc: 0.7311, Valid Loss: 0.4881, Valid Auroc: 0.7178\n",
            "Epoch: 859, Train Loss: 0.4847, Train auroc: 0.7323, Valid Loss: 0.4800, Valid Auroc: 0.7241\n",
            "Epoch: 860, Train Loss: 0.4860, Train auroc: 0.7316, Valid Loss: 0.4891, Valid Auroc: 0.7216\n",
            "Epoch: 861, Train Loss: 0.4832, Train auroc: 0.7322, Valid Loss: 0.4900, Valid Auroc: 0.7192\n",
            "Epoch: 862, Train Loss: 0.4935, Train auroc: 0.7272, Valid Loss: 0.4896, Valid Auroc: 0.7185\n",
            "Epoch: 863, Train Loss: 0.4858, Train auroc: 0.7309, Valid Loss: 0.4862, Valid Auroc: 0.7203\n",
            "Epoch: 864, Train Loss: 0.4943, Train auroc: 0.7245, Valid Loss: 0.5004, Valid Auroc: 0.7129\n",
            "Epoch: 865, Train Loss: 0.4899, Train auroc: 0.7267, Valid Loss: 0.4934, Valid Auroc: 0.7158\n",
            "Epoch: 866, Train Loss: 0.4931, Train auroc: 0.7260, Valid Loss: 0.4850, Valid Auroc: 0.7189\n",
            "Epoch: 867, Train Loss: 0.4897, Train auroc: 0.7253, Valid Loss: 0.4884, Valid Auroc: 0.7144\n",
            "Epoch: 868, Train Loss: 0.4947, Train auroc: 0.7193, Valid Loss: 0.5046, Valid Auroc: 0.7039\n",
            "Epoch: 869, Train Loss: 0.4912, Train auroc: 0.7224, Valid Loss: 0.4907, Valid Auroc: 0.7132\n",
            "Epoch: 870, Train Loss: 0.4935, Train auroc: 0.7228, Valid Loss: 0.4850, Valid Auroc: 0.7177\n",
            "Epoch: 871, Train Loss: 0.4877, Train auroc: 0.7251, Valid Loss: 0.4881, Valid Auroc: 0.7152\n",
            "Epoch: 872, Train Loss: 0.4916, Train auroc: 0.7225, Valid Loss: 0.4976, Valid Auroc: 0.7091\n",
            "Epoch: 873, Train Loss: 0.4854, Train auroc: 0.7273, Valid Loss: 0.4845, Valid Auroc: 0.7176\n",
            "Epoch: 874, Train Loss: 0.4917, Train auroc: 0.7263, Valid Loss: 0.4841, Valid Auroc: 0.7207\n",
            "Epoch: 875, Train Loss: 0.4844, Train auroc: 0.7292, Valid Loss: 0.4843, Valid Auroc: 0.7201\n",
            "Epoch: 876, Train Loss: 0.4909, Train auroc: 0.7260, Valid Loss: 0.4983, Valid Auroc: 0.7132\n",
            "Epoch: 877, Train Loss: 0.4858, Train auroc: 0.7278, Valid Loss: 0.4875, Valid Auroc: 0.7172\n",
            "Epoch: 878, Train Loss: 0.4911, Train auroc: 0.7273, Valid Loss: 0.4823, Valid Auroc: 0.7222\n",
            "Epoch: 879, Train Loss: 0.4863, Train auroc: 0.7290, Valid Loss: 0.4826, Valid Auroc: 0.7217\n",
            "Epoch: 880, Train Loss: 0.4931, Train auroc: 0.7256, Valid Loss: 0.5002, Valid Auroc: 0.7132\n",
            "Epoch: 881, Train Loss: 0.4875, Train auroc: 0.7266, Valid Loss: 0.4945, Valid Auroc: 0.7123\n",
            "Epoch: 882, Train Loss: 0.4932, Train auroc: 0.7270, Valid Loss: 0.4862, Valid Auroc: 0.7189\n",
            "Epoch: 883, Train Loss: 0.4877, Train auroc: 0.7284, Valid Loss: 0.4817, Valid Auroc: 0.7215\n",
            "Epoch: 884, Train Loss: 0.4923, Train auroc: 0.7263, Valid Loss: 0.4973, Valid Auroc: 0.7165\n",
            "Epoch: 885, Train Loss: 0.4858, Train auroc: 0.7274, Valid Loss: 0.4885, Valid Auroc: 0.7179\n",
            "Epoch: 886, Train Loss: 0.4914, Train auroc: 0.7271, Valid Loss: 0.4847, Valid Auroc: 0.7221\n",
            "Epoch: 887, Train Loss: 0.4852, Train auroc: 0.7301, Valid Loss: 0.4808, Valid Auroc: 0.7228\n",
            "Epoch: 888, Train Loss: 0.4899, Train auroc: 0.7272, Valid Loss: 0.4950, Valid Auroc: 0.7147\n",
            "Epoch: 889, Train Loss: 0.4863, Train auroc: 0.7278, Valid Loss: 0.4924, Valid Auroc: 0.7144\n",
            "Epoch: 890, Train Loss: 0.4882, Train auroc: 0.7289, Valid Loss: 0.4858, Valid Auroc: 0.7195\n",
            "Epoch: 891, Train Loss: 0.4879, Train auroc: 0.7303, Valid Loss: 0.4825, Valid Auroc: 0.7222\n",
            "Epoch: 892, Train Loss: 0.4860, Train auroc: 0.7300, Valid Loss: 0.4888, Valid Auroc: 0.7187\n",
            "Epoch: 893, Train Loss: 0.4856, Train auroc: 0.7296, Valid Loss: 0.4904, Valid Auroc: 0.7179\n",
            "Epoch: 894, Train Loss: 0.4842, Train auroc: 0.7302, Valid Loss: 0.4813, Valid Auroc: 0.7222\n",
            "Epoch: 895, Train Loss: 0.4856, Train auroc: 0.7310, Valid Loss: 0.4804, Valid Auroc: 0.7238\n",
            "Epoch: 896, Train Loss: 0.4816, Train auroc: 0.7321, Valid Loss: 0.4849, Valid Auroc: 0.7202\n",
            "Epoch: 897, Train Loss: 0.4872, Train auroc: 0.7300, Valid Loss: 0.4946, Valid Auroc: 0.7158\n",
            "Epoch: 898, Train Loss: 0.4808, Train auroc: 0.7335, Valid Loss: 0.4814, Valid Auroc: 0.7222\n",
            "Epoch: 899, Train Loss: 0.4870, Train auroc: 0.7327, Valid Loss: 0.4804, Valid Auroc: 0.7249\n",
            "Epoch: 900, Train Loss: 0.4808, Train auroc: 0.7348, Valid Loss: 0.4806, Valid Auroc: 0.7241\n",
            "Epoch: 901, Train Loss: 0.4871, Train auroc: 0.7309, Valid Loss: 0.4935, Valid Auroc: 0.7181\n",
            "Epoch: 902, Train Loss: 0.4819, Train auroc: 0.7340, Valid Loss: 0.4848, Valid Auroc: 0.7227\n",
            "Epoch: 903, Train Loss: 0.4866, Train auroc: 0.7329, Valid Loss: 0.4799, Valid Auroc: 0.7257\n",
            "Epoch: 904, Train Loss: 0.4841, Train auroc: 0.7345, Valid Loss: 0.4816, Valid Auroc: 0.7247\n",
            "Epoch: 905, Train Loss: 0.4850, Train auroc: 0.7313, Valid Loss: 0.4933, Valid Auroc: 0.7166\n",
            "Epoch: 906, Train Loss: 0.4899, Train auroc: 0.7296, Valid Loss: 0.4979, Valid Auroc: 0.7152\n",
            "Epoch: 907, Train Loss: 0.4819, Train auroc: 0.7341, Valid Loss: 0.4808, Valid Auroc: 0.7228\n",
            "Epoch: 908, Train Loss: 0.4921, Train auroc: 0.7318, Valid Loss: 0.4866, Valid Auroc: 0.7224\n",
            "Epoch: 909, Train Loss: 0.4827, Train auroc: 0.7344, Valid Loss: 0.4831, Valid Auroc: 0.7228\n",
            "Epoch: 910, Train Loss: 0.4889, Train auroc: 0.7302, Valid Loss: 0.4918, Valid Auroc: 0.7196\n",
            "Epoch: 911, Train Loss: 0.4854, Train auroc: 0.7304, Valid Loss: 0.4880, Valid Auroc: 0.7196\n",
            "Epoch: 912, Train Loss: 0.4876, Train auroc: 0.7310, Valid Loss: 0.4862, Valid Auroc: 0.7210\n",
            "Epoch: 913, Train Loss: 0.4842, Train auroc: 0.7331, Valid Loss: 0.4816, Valid Auroc: 0.7234\n",
            "Epoch: 914, Train Loss: 0.4840, Train auroc: 0.7327, Valid Loss: 0.4879, Valid Auroc: 0.7202\n",
            "Epoch: 915, Train Loss: 0.4837, Train auroc: 0.7320, Valid Loss: 0.4884, Valid Auroc: 0.7191\n",
            "Epoch: 916, Train Loss: 0.4805, Train auroc: 0.7339, Valid Loss: 0.4811, Valid Auroc: 0.7229\n",
            "Epoch: 917, Train Loss: 0.4842, Train auroc: 0.7341, Valid Loss: 0.4807, Valid Auroc: 0.7249\n",
            "Epoch: 918, Train Loss: 0.4799, Train auroc: 0.7358, Valid Loss: 0.4801, Valid Auroc: 0.7249\n",
            "Epoch: 919, Train Loss: 0.4832, Train auroc: 0.7346, Valid Loss: 0.4883, Valid Auroc: 0.7221\n",
            "Epoch: 920, Train Loss: 0.4812, Train auroc: 0.7354, Valid Loss: 0.4833, Valid Auroc: 0.7242\n",
            "Epoch: 921, Train Loss: 0.4817, Train auroc: 0.7356, Valid Loss: 0.4805, Valid Auroc: 0.7258\n",
            "Epoch: 922, Train Loss: 0.4823, Train auroc: 0.7354, Valid Loss: 0.4831, Valid Auroc: 0.7240\n",
            "Epoch: 923, Train Loss: 0.4812, Train auroc: 0.7360, Valid Loss: 0.4852, Valid Auroc: 0.7236\n",
            "Epoch: 924, Train Loss: 0.4832, Train auroc: 0.7348, Valid Loss: 0.4858, Valid Auroc: 0.7235\n",
            "Epoch: 925, Train Loss: 0.4812, Train auroc: 0.7360, Valid Loss: 0.4826, Valid Auroc: 0.7247\n",
            "Epoch: 926, Train Loss: 0.4860, Train auroc: 0.7337, Valid Loss: 0.4890, Valid Auroc: 0.7213\n",
            "Epoch: 927, Train Loss: 0.4812, Train auroc: 0.7361, Valid Loss: 0.4825, Valid Auroc: 0.7248\n",
            "Epoch: 928, Train Loss: 0.4860, Train auroc: 0.7346, Valid Loss: 0.4882, Valid Auroc: 0.7241\n",
            "Epoch: 929, Train Loss: 0.4828, Train auroc: 0.7346, Valid Loss: 0.4833, Valid Auroc: 0.7241\n",
            "Epoch: 930, Train Loss: 0.4841, Train auroc: 0.7336, Valid Loss: 0.4864, Valid Auroc: 0.7208\n",
            "Epoch: 931, Train Loss: 0.4863, Train auroc: 0.7325, Valid Loss: 0.4883, Valid Auroc: 0.7202\n",
            "Epoch: 932, Train Loss: 0.4835, Train auroc: 0.7340, Valid Loss: 0.4841, Valid Auroc: 0.7229\n",
            "Epoch: 933, Train Loss: 0.4834, Train auroc: 0.7333, Valid Loss: 0.4854, Valid Auroc: 0.7225\n",
            "Epoch: 934, Train Loss: 0.4849, Train auroc: 0.7318, Valid Loss: 0.4865, Valid Auroc: 0.7209\n",
            "Epoch: 935, Train Loss: 0.4836, Train auroc: 0.7333, Valid Loss: 0.4832, Valid Auroc: 0.7225\n",
            "Epoch: 936, Train Loss: 0.4816, Train auroc: 0.7345, Valid Loss: 0.4814, Valid Auroc: 0.7241\n",
            "Epoch: 937, Train Loss: 0.4839, Train auroc: 0.7337, Valid Loss: 0.4862, Valid Auroc: 0.7227\n",
            "Epoch: 938, Train Loss: 0.4814, Train auroc: 0.7350, Valid Loss: 0.4850, Valid Auroc: 0.7235\n",
            "Epoch: 939, Train Loss: 0.4822, Train auroc: 0.7336, Valid Loss: 0.4811, Valid Auroc: 0.7239\n",
            "Epoch: 940, Train Loss: 0.4825, Train auroc: 0.7343, Valid Loss: 0.4818, Valid Auroc: 0.7235\n",
            "Epoch: 941, Train Loss: 0.4814, Train auroc: 0.7351, Valid Loss: 0.4856, Valid Auroc: 0.7225\n",
            "Epoch: 942, Train Loss: 0.4805, Train auroc: 0.7361, Valid Loss: 0.4802, Valid Auroc: 0.7255\n",
            "Epoch: 943, Train Loss: 0.4775, Train auroc: 0.7381, Valid Loss: 0.4793, Valid Auroc: 0.7265\n",
            "Epoch: 944, Train Loss: 0.4788, Train auroc: 0.7374, Valid Loss: 0.4829, Valid Auroc: 0.7250\n",
            "Epoch: 945, Train Loss: 0.4792, Train auroc: 0.7376, Valid Loss: 0.4774, Valid Auroc: 0.7280\n",
            "Epoch: 946, Train Loss: 0.4764, Train auroc: 0.7398, Valid Loss: 0.4796, Valid Auroc: 0.7282\n",
            "Epoch: 947, Train Loss: 0.4774, Train auroc: 0.7392, Valid Loss: 0.4835, Valid Auroc: 0.7260\n",
            "Epoch: 948, Train Loss: 0.4783, Train auroc: 0.7393, Valid Loss: 0.4758, Valid Auroc: 0.7296\n",
            "Epoch: 949, Train Loss: 0.4751, Train auroc: 0.7417, Valid Loss: 0.4773, Valid Auroc: 0.7297\n",
            "Epoch: 950, Train Loss: 0.4800, Train auroc: 0.7390, Valid Loss: 0.4883, Valid Auroc: 0.7245\n",
            "Epoch: 951, Train Loss: 0.4777, Train auroc: 0.7399, Valid Loss: 0.4757, Valid Auroc: 0.7304\n",
            "Epoch: 952, Train Loss: 0.4792, Train auroc: 0.7404, Valid Loss: 0.4760, Valid Auroc: 0.7311\n",
            "Epoch: 953, Train Loss: 0.4817, Train auroc: 0.7386, Valid Loss: 0.4944, Valid Auroc: 0.7222\n",
            "Epoch: 954, Train Loss: 0.4807, Train auroc: 0.7377, Valid Loss: 0.4876, Valid Auroc: 0.7215\n",
            "Epoch: 955, Train Loss: 0.4847, Train auroc: 0.7383, Valid Loss: 0.4768, Valid Auroc: 0.7304\n",
            "Epoch: 956, Train Loss: 0.4862, Train auroc: 0.7372, Valid Loss: 0.4879, Valid Auroc: 0.7274\n",
            "Epoch: 957, Train Loss: 0.4879, Train auroc: 0.7333, Valid Loss: 0.4970, Valid Auroc: 0.7186\n",
            "Epoch: 958, Train Loss: 0.4896, Train auroc: 0.7323, Valid Loss: 0.4905, Valid Auroc: 0.7208\n",
            "Epoch: 959, Train Loss: 0.4830, Train auroc: 0.7363, Valid Loss: 0.4804, Valid Auroc: 0.7272\n",
            "Epoch: 960, Train Loss: 0.4903, Train auroc: 0.7327, Valid Loss: 0.4964, Valid Auroc: 0.7209\n",
            "Epoch: 961, Train Loss: 0.4821, Train auroc: 0.7324, Valid Loss: 0.4855, Valid Auroc: 0.7201\n",
            "Epoch: 962, Train Loss: 0.4904, Train auroc: 0.7284, Valid Loss: 0.4897, Valid Auroc: 0.7169\n",
            "Epoch: 963, Train Loss: 0.4869, Train auroc: 0.7312, Valid Loss: 0.4892, Valid Auroc: 0.7175\n",
            "Epoch: 964, Train Loss: 0.4827, Train auroc: 0.7320, Valid Loss: 0.4867, Valid Auroc: 0.7196\n",
            "Epoch: 965, Train Loss: 0.4870, Train auroc: 0.7296, Valid Loss: 0.4862, Valid Auroc: 0.7206\n",
            "Epoch: 966, Train Loss: 0.4813, Train auroc: 0.7336, Valid Loss: 0.4806, Valid Auroc: 0.7243\n",
            "Epoch: 967, Train Loss: 0.4818, Train auroc: 0.7336, Valid Loss: 0.4863, Valid Auroc: 0.7214\n",
            "Epoch: 968, Train Loss: 0.4799, Train auroc: 0.7345, Valid Loss: 0.4825, Valid Auroc: 0.7228\n",
            "Epoch: 969, Train Loss: 0.4794, Train auroc: 0.7368, Valid Loss: 0.4794, Valid Auroc: 0.7264\n",
            "Epoch: 970, Train Loss: 0.4780, Train auroc: 0.7382, Valid Loss: 0.4799, Valid Auroc: 0.7269\n",
            "Epoch: 971, Train Loss: 0.4785, Train auroc: 0.7379, Valid Loss: 0.4830, Valid Auroc: 0.7249\n",
            "Epoch: 972, Train Loss: 0.4763, Train auroc: 0.7400, Valid Loss: 0.4786, Valid Auroc: 0.7287\n",
            "Epoch: 973, Train Loss: 0.4769, Train auroc: 0.7406, Valid Loss: 0.4768, Valid Auroc: 0.7304\n",
            "Epoch: 974, Train Loss: 0.4755, Train auroc: 0.7412, Valid Loss: 0.4800, Valid Auroc: 0.7288\n",
            "Epoch: 975, Train Loss: 0.4762, Train auroc: 0.7411, Valid Loss: 0.4824, Valid Auroc: 0.7273\n",
            "Epoch: 976, Train Loss: 0.4765, Train auroc: 0.7417, Valid Loss: 0.4750, Valid Auroc: 0.7313\n",
            "Epoch: 977, Train Loss: 0.4741, Train auroc: 0.7438, Valid Loss: 0.4753, Valid Auroc: 0.7324\n",
            "Epoch: 978, Train Loss: 0.4796, Train auroc: 0.7409, Valid Loss: 0.4896, Valid Auroc: 0.7262\n",
            "Epoch: 979, Train Loss: 0.4746, Train auroc: 0.7434, Valid Loss: 0.4777, Valid Auroc: 0.7305\n",
            "Epoch: 980, Train Loss: 0.4807, Train auroc: 0.7418, Valid Loss: 0.4752, Valid Auroc: 0.7330\n",
            "Epoch: 981, Train Loss: 0.4783, Train auroc: 0.7433, Valid Loss: 0.4855, Valid Auroc: 0.7297\n",
            "Epoch: 982, Train Loss: 0.4783, Train auroc: 0.7413, Valid Loss: 0.4864, Valid Auroc: 0.7267\n",
            "Epoch: 983, Train Loss: 0.4878, Train auroc: 0.7389, Valid Loss: 0.4851, Valid Auroc: 0.7297\n",
            "Epoch: 984, Train Loss: 0.4797, Train auroc: 0.7416, Valid Loss: 0.4754, Valid Auroc: 0.7329\n",
            "Epoch: 985, Train Loss: 0.4898, Train auroc: 0.7386, Valid Loss: 0.5010, Valid Auroc: 0.7242\n",
            "Epoch: 986, Train Loss: 0.4798, Train auroc: 0.7395, Valid Loss: 0.4880, Valid Auroc: 0.7247\n",
            "Epoch: 987, Train Loss: 0.4913, Train auroc: 0.7350, Valid Loss: 0.4850, Valid Auroc: 0.7267\n",
            "Epoch: 988, Train Loss: 0.4885, Train auroc: 0.7363, Valid Loss: 0.4840, Valid Auroc: 0.7270\n",
            "Epoch: 989, Train Loss: 0.4868, Train auroc: 0.7317, Valid Loss: 0.4933, Valid Auroc: 0.7180\n",
            "Epoch: 990, Train Loss: 0.4921, Train auroc: 0.7307, Valid Loss: 0.5005, Valid Auroc: 0.7168\n",
            "Epoch: 991, Train Loss: 0.4852, Train auroc: 0.7330, Valid Loss: 0.4805, Valid Auroc: 0.7258\n",
            "Epoch: 992, Train Loss: 0.4919, Train auroc: 0.7317, Valid Loss: 0.4826, Valid Auroc: 0.7264\n",
            "Epoch: 993, Train Loss: 0.4821, Train auroc: 0.7342, Valid Loss: 0.4849, Valid Auroc: 0.7204\n",
            "Epoch: 994, Train Loss: 0.4934, Train auroc: 0.7266, Valid Loss: 0.5044, Valid Auroc: 0.7097\n",
            "Epoch: 995, Train Loss: 0.4852, Train auroc: 0.7311, Valid Loss: 0.4904, Valid Auroc: 0.7169\n",
            "Epoch: 996, Train Loss: 0.4874, Train auroc: 0.7328, Valid Loss: 0.4825, Valid Auroc: 0.7234\n",
            "Epoch: 997, Train Loss: 0.4867, Train auroc: 0.7350, Valid Loss: 0.4833, Valid Auroc: 0.7241\n",
            "Epoch: 998, Train Loss: 0.4824, Train auroc: 0.7348, Valid Loss: 0.4853, Valid Auroc: 0.7219\n",
            "Epoch: 999, Train Loss: 0.4865, Train auroc: 0.7327, Valid Loss: 0.4900, Valid Auroc: 0.7222\n",
            "Epoch: 1000, Train Loss: 0.4834, Train auroc: 0.7338, Valid Loss: 0.4819, Valid Auroc: 0.7257\n",
            "Epoch: 1001, Train Loss: 0.4844, Train auroc: 0.7356, Valid Loss: 0.4843, Valid Auroc: 0.7255\n",
            "Epoch: 1002, Train Loss: 0.4816, Train auroc: 0.7358, Valid Loss: 0.4836, Valid Auroc: 0.7239\n",
            "Epoch: 1003, Train Loss: 0.4824, Train auroc: 0.7360, Valid Loss: 0.4857, Valid Auroc: 0.7243\n",
            "Epoch: 1004, Train Loss: 0.4806, Train auroc: 0.7380, Valid Loss: 0.4829, Valid Auroc: 0.7262\n",
            "Epoch: 1005, Train Loss: 0.4811, Train auroc: 0.7387, Valid Loss: 0.4803, Valid Auroc: 0.7272\n",
            "Epoch: 1006, Train Loss: 0.4806, Train auroc: 0.7393, Valid Loss: 0.4824, Valid Auroc: 0.7266\n",
            "Epoch: 1007, Train Loss: 0.4802, Train auroc: 0.7390, Valid Loss: 0.4831, Valid Auroc: 0.7269\n",
            "Epoch: 1008, Train Loss: 0.4796, Train auroc: 0.7400, Valid Loss: 0.4831, Valid Auroc: 0.7286\n",
            "Epoch: 1009, Train Loss: 0.4790, Train auroc: 0.7406, Valid Loss: 0.4786, Valid Auroc: 0.7305\n",
            "Epoch: 1010, Train Loss: 0.4808, Train auroc: 0.7402, Valid Loss: 0.4794, Valid Auroc: 0.7298\n",
            "Epoch: 1011, Train Loss: 0.4770, Train auroc: 0.7407, Valid Loss: 0.4814, Valid Auroc: 0.7276\n",
            "Epoch: 1012, Train Loss: 0.4832, Train auroc: 0.7379, Valid Loss: 0.4875, Valid Auroc: 0.7251\n",
            "Epoch: 1013, Train Loss: 0.4764, Train auroc: 0.7414, Valid Loss: 0.4782, Valid Auroc: 0.7289\n",
            "Epoch: 1014, Train Loss: 0.4804, Train auroc: 0.7408, Valid Loss: 0.4809, Valid Auroc: 0.7285\n",
            "Epoch: 1015, Train Loss: 0.4787, Train auroc: 0.7415, Valid Loss: 0.4816, Valid Auroc: 0.7284\n",
            "Epoch: 1016, Train Loss: 0.4775, Train auroc: 0.7421, Valid Loss: 0.4818, Valid Auroc: 0.7297\n",
            "Epoch: 1017, Train Loss: 0.4785, Train auroc: 0.7414, Valid Loss: 0.4781, Valid Auroc: 0.7313\n",
            "Epoch: 1018, Train Loss: 0.4795, Train auroc: 0.7421, Valid Loss: 0.4804, Valid Auroc: 0.7306\n",
            "Epoch: 1019, Train Loss: 0.4792, Train auroc: 0.7406, Valid Loss: 0.4861, Valid Auroc: 0.7257\n",
            "Epoch: 1020, Train Loss: 0.4781, Train auroc: 0.7431, Valid Loss: 0.4834, Valid Auroc: 0.7303\n",
            "Epoch: 1021, Train Loss: 0.4810, Train auroc: 0.7401, Valid Loss: 0.4814, Valid Auroc: 0.7297\n",
            "Epoch: 1022, Train Loss: 0.4777, Train auroc: 0.7409, Valid Loss: 0.4761, Valid Auroc: 0.7307\n",
            "Epoch: 1023, Train Loss: 0.4798, Train auroc: 0.7399, Valid Loss: 0.4851, Valid Auroc: 0.7264\n",
            "Epoch: 1024, Train Loss: 0.4795, Train auroc: 0.7399, Valid Loss: 0.4859, Valid Auroc: 0.7264\n",
            "Epoch: 1025, Train Loss: 0.4789, Train auroc: 0.7420, Valid Loss: 0.4772, Valid Auroc: 0.7326\n",
            "Epoch: 1026, Train Loss: 0.4777, Train auroc: 0.7427, Valid Loss: 0.4771, Valid Auroc: 0.7330\n",
            "Epoch: 1027, Train Loss: 0.4801, Train auroc: 0.7402, Valid Loss: 0.4858, Valid Auroc: 0.7263\n",
            "Epoch: 1028, Train Loss: 0.4799, Train auroc: 0.7392, Valid Loss: 0.4851, Valid Auroc: 0.7243\n",
            "Epoch: 1029, Train Loss: 0.4780, Train auroc: 0.7427, Valid Loss: 0.4785, Valid Auroc: 0.7312\n",
            "Epoch: 1030, Train Loss: 0.4800, Train auroc: 0.7426, Valid Loss: 0.4806, Valid Auroc: 0.7318\n",
            "Epoch: 1031, Train Loss: 0.4776, Train auroc: 0.7431, Valid Loss: 0.4825, Valid Auroc: 0.7313\n",
            "Epoch: 1032, Train Loss: 0.4795, Train auroc: 0.7408, Valid Loss: 0.4803, Valid Auroc: 0.7292\n",
            "Epoch: 1033, Train Loss: 0.4818, Train auroc: 0.7402, Valid Loss: 0.4826, Valid Auroc: 0.7280\n",
            "Epoch: 1034, Train Loss: 0.4760, Train auroc: 0.7432, Valid Loss: 0.4823, Valid Auroc: 0.7298\n",
            "Epoch: 1035, Train Loss: 0.4835, Train auroc: 0.7406, Valid Loss: 0.4889, Valid Auroc: 0.7286\n",
            "Epoch: 1036, Train Loss: 0.4781, Train auroc: 0.7426, Valid Loss: 0.4754, Valid Auroc: 0.7326\n",
            "Epoch: 1037, Train Loss: 0.4808, Train auroc: 0.7417, Valid Loss: 0.4831, Valid Auroc: 0.7275\n",
            "Epoch: 1038, Train Loss: 0.4835, Train auroc: 0.7387, Valid Loss: 0.4916, Valid Auroc: 0.7216\n",
            "Epoch: 1039, Train Loss: 0.4759, Train auroc: 0.7437, Valid Loss: 0.4789, Valid Auroc: 0.7320\n",
            "Epoch: 1040, Train Loss: 0.4878, Train auroc: 0.7386, Valid Loss: 0.4821, Valid Auroc: 0.7313\n",
            "Epoch: 1041, Train Loss: 0.4771, Train auroc: 0.7436, Valid Loss: 0.4792, Valid Auroc: 0.7323\n",
            "Epoch: 1042, Train Loss: 0.4877, Train auroc: 0.7367, Valid Loss: 0.4982, Valid Auroc: 0.7213\n",
            "Epoch: 1043, Train Loss: 0.4816, Train auroc: 0.7383, Valid Loss: 0.4863, Valid Auroc: 0.7243\n",
            "Epoch: 1044, Train Loss: 0.4846, Train auroc: 0.7388, Valid Loss: 0.4776, Valid Auroc: 0.7315\n",
            "Epoch: 1045, Train Loss: 0.4826, Train auroc: 0.7403, Valid Loss: 0.4850, Valid Auroc: 0.7304\n",
            "Epoch: 1046, Train Loss: 0.4826, Train auroc: 0.7400, Valid Loss: 0.4883, Valid Auroc: 0.7285\n",
            "Epoch: 1047, Train Loss: 0.4846, Train auroc: 0.7372, Valid Loss: 0.4866, Valid Auroc: 0.7257\n",
            "Epoch: 1048, Train Loss: 0.4856, Train auroc: 0.7360, Valid Loss: 0.4859, Valid Auroc: 0.7250\n",
            "Epoch: 1049, Train Loss: 0.4871, Train auroc: 0.7362, Valid Loss: 0.4905, Valid Auroc: 0.7245\n",
            "Epoch: 1050, Train Loss: 0.4785, Train auroc: 0.7395, Valid Loss: 0.4771, Valid Auroc: 0.7297\n",
            "Epoch: 1051, Train Loss: 0.4826, Train auroc: 0.7402, Valid Loss: 0.4791, Valid Auroc: 0.7312\n",
            "Epoch: 1052, Train Loss: 0.4783, Train auroc: 0.7412, Valid Loss: 0.4834, Valid Auroc: 0.7281\n",
            "Epoch: 1053, Train Loss: 0.4800, Train auroc: 0.7411, Valid Loss: 0.4870, Valid Auroc: 0.7279\n",
            "Epoch: 1054, Train Loss: 0.4775, Train auroc: 0.7422, Valid Loss: 0.4751, Valid Auroc: 0.7338\n",
            "Epoch: 1055, Train Loss: 0.4802, Train auroc: 0.7414, Valid Loss: 0.4751, Valid Auroc: 0.7339\n",
            "Epoch: 1056, Train Loss: 0.4776, Train auroc: 0.7428, Valid Loss: 0.4861, Valid Auroc: 0.7285\n",
            "Epoch: 1057, Train Loss: 0.4784, Train auroc: 0.7409, Valid Loss: 0.4881, Valid Auroc: 0.7244\n",
            "Epoch: 1058, Train Loss: 0.4777, Train auroc: 0.7437, Valid Loss: 0.4768, Valid Auroc: 0.7326\n",
            "Epoch: 1059, Train Loss: 0.4791, Train auroc: 0.7441, Valid Loss: 0.4751, Valid Auroc: 0.7350\n",
            "Epoch: 1060, Train Loss: 0.4760, Train auroc: 0.7453, Valid Loss: 0.4811, Valid Auroc: 0.7329\n",
            "Epoch: 1061, Train Loss: 0.4786, Train auroc: 0.7435, Valid Loss: 0.4860, Valid Auroc: 0.7299\n",
            "Epoch: 1062, Train Loss: 0.4828, Train auroc: 0.7411, Valid Loss: 0.4811, Valid Auroc: 0.7309\n",
            "Epoch: 1063, Train Loss: 0.4754, Train auroc: 0.7460, Valid Loss: 0.4748, Valid Auroc: 0.7348\n",
            "Epoch: 1064, Train Loss: 0.4798, Train auroc: 0.7427, Valid Loss: 0.4892, Valid Auroc: 0.7281\n",
            "Epoch: 1065, Train Loss: 0.4766, Train auroc: 0.7422, Valid Loss: 0.4800, Valid Auroc: 0.7286\n",
            "Epoch: 1066, Train Loss: 0.4783, Train auroc: 0.7440, Valid Loss: 0.4783, Valid Auroc: 0.7313\n",
            "Epoch: 1067, Train Loss: 0.4735, Train auroc: 0.7463, Valid Loss: 0.4780, Valid Auroc: 0.7320\n",
            "Epoch: 1068, Train Loss: 0.4757, Train auroc: 0.7457, Valid Loss: 0.4811, Valid Auroc: 0.7324\n",
            "Epoch: 1069, Train Loss: 0.4734, Train auroc: 0.7459, Valid Loss: 0.4732, Valid Auroc: 0.7355\n",
            "Epoch: 1070, Train Loss: 0.4761, Train auroc: 0.7456, Valid Loss: 0.4749, Valid Auroc: 0.7352\n",
            "Epoch: 1071, Train Loss: 0.4734, Train auroc: 0.7470, Valid Loss: 0.4800, Valid Auroc: 0.7335\n",
            "Epoch: 1072, Train Loss: 0.4765, Train auroc: 0.7459, Valid Loss: 0.4845, Valid Auroc: 0.7314\n",
            "Epoch: 1073, Train Loss: 0.4747, Train auroc: 0.7472, Valid Loss: 0.4748, Valid Auroc: 0.7348\n",
            "Epoch: 1074, Train Loss: 0.4793, Train auroc: 0.7460, Valid Loss: 0.4782, Valid Auroc: 0.7342\n",
            "Epoch: 1075, Train Loss: 0.4729, Train auroc: 0.7483, Valid Loss: 0.4791, Valid Auroc: 0.7338\n",
            "Epoch: 1076, Train Loss: 0.4792, Train auroc: 0.7461, Valid Loss: 0.4865, Valid Auroc: 0.7324\n",
            "Epoch: 1077, Train Loss: 0.4756, Train auroc: 0.7472, Valid Loss: 0.4743, Valid Auroc: 0.7368\n",
            "Epoch: 1078, Train Loss: 0.4838, Train auroc: 0.7436, Valid Loss: 0.4825, Valid Auroc: 0.7326\n",
            "Epoch: 1079, Train Loss: 0.4741, Train auroc: 0.7467, Valid Loss: 0.4819, Valid Auroc: 0.7305\n",
            "Epoch: 1080, Train Loss: 0.4854, Train auroc: 0.7438, Valid Loss: 0.4948, Valid Auroc: 0.7278\n",
            "Epoch: 1081, Train Loss: 0.4788, Train auroc: 0.7434, Valid Loss: 0.4784, Valid Auroc: 0.7315\n",
            "Epoch: 1082, Train Loss: 0.4845, Train auroc: 0.7439, Valid Loss: 0.4811, Valid Auroc: 0.7334\n",
            "Epoch: 1083, Train Loss: 0.4841, Train auroc: 0.7410, Valid Loss: 0.4896, Valid Auroc: 0.7254\n",
            "Epoch: 1084, Train Loss: 0.4835, Train auroc: 0.7396, Valid Loss: 0.4922, Valid Auroc: 0.7238\n",
            "Epoch: 1085, Train Loss: 0.4824, Train auroc: 0.7416, Valid Loss: 0.4844, Valid Auroc: 0.7313\n",
            "Epoch: 1086, Train Loss: 0.4855, Train auroc: 0.7382, Valid Loss: 0.4798, Valid Auroc: 0.7311\n",
            "Epoch: 1087, Train Loss: 0.4803, Train auroc: 0.7420, Valid Loss: 0.4779, Valid Auroc: 0.7317\n",
            "Epoch: 1088, Train Loss: 0.4825, Train auroc: 0.7370, Valid Loss: 0.4913, Valid Auroc: 0.7202\n",
            "Epoch: 1089, Train Loss: 0.4851, Train auroc: 0.7366, Valid Loss: 0.4963, Valid Auroc: 0.7194\n",
            "Epoch: 1090, Train Loss: 0.4792, Train auroc: 0.7402, Valid Loss: 0.4791, Valid Auroc: 0.7293\n",
            "Epoch: 1091, Train Loss: 0.4839, Train auroc: 0.7391, Valid Loss: 0.4787, Valid Auroc: 0.7307\n",
            "Epoch: 1092, Train Loss: 0.4783, Train auroc: 0.7421, Valid Loss: 0.4817, Valid Auroc: 0.7294\n",
            "Epoch: 1093, Train Loss: 0.4778, Train auroc: 0.7424, Valid Loss: 0.4839, Valid Auroc: 0.7289\n",
            "Epoch: 1094, Train Loss: 0.4771, Train auroc: 0.7429, Valid Loss: 0.4768, Valid Auroc: 0.7322\n",
            "Epoch: 1095, Train Loss: 0.4751, Train auroc: 0.7455, Valid Loss: 0.4750, Valid Auroc: 0.7346\n",
            "Epoch: 1096, Train Loss: 0.4744, Train auroc: 0.7460, Valid Loss: 0.4792, Valid Auroc: 0.7321\n",
            "Epoch: 1097, Train Loss: 0.4738, Train auroc: 0.7463, Valid Loss: 0.4798, Valid Auroc: 0.7318\n",
            "Epoch: 1098, Train Loss: 0.4741, Train auroc: 0.7472, Valid Loss: 0.4760, Valid Auroc: 0.7344\n",
            "Epoch: 1099, Train Loss: 0.4734, Train auroc: 0.7481, Valid Loss: 0.4742, Valid Auroc: 0.7364\n",
            "Epoch: 1100, Train Loss: 0.4732, Train auroc: 0.7485, Valid Loss: 0.4783, Valid Auroc: 0.7353\n",
            "Epoch: 1101, Train Loss: 0.4733, Train auroc: 0.7481, Valid Loss: 0.4797, Valid Auroc: 0.7335\n",
            "Epoch: 1102, Train Loss: 0.4749, Train auroc: 0.7476, Valid Loss: 0.4772, Valid Auroc: 0.7344\n",
            "Epoch: 1103, Train Loss: 0.4724, Train auroc: 0.7499, Valid Loss: 0.4730, Valid Auroc: 0.7379\n",
            "Epoch: 1104, Train Loss: 0.4765, Train auroc: 0.7489, Valid Loss: 0.4845, Valid Auroc: 0.7348\n",
            "Epoch: 1105, Train Loss: 0.4729, Train auroc: 0.7491, Valid Loss: 0.4803, Valid Auroc: 0.7341\n",
            "Epoch: 1106, Train Loss: 0.4800, Train auroc: 0.7473, Valid Loss: 0.4837, Valid Auroc: 0.7333\n",
            "Epoch: 1107, Train Loss: 0.4728, Train auroc: 0.7502, Valid Loss: 0.4728, Valid Auroc: 0.7388\n",
            "Epoch: 1108, Train Loss: 0.4832, Train auroc: 0.7482, Valid Loss: 0.4919, Valid Auroc: 0.7347\n",
            "Epoch: 1109, Train Loss: 0.4702, Train auroc: 0.7503, Valid Loss: 0.4749, Valid Auroc: 0.7369\n",
            "Epoch: 1110, Train Loss: 0.4859, Train auroc: 0.7434, Valid Loss: 0.4869, Valid Auroc: 0.7316\n",
            "Epoch: 1111, Train Loss: 0.4788, Train auroc: 0.7459, Valid Loss: 0.4820, Valid Auroc: 0.7324\n",
            "Epoch: 1112, Train Loss: 0.4767, Train auroc: 0.7450, Valid Loss: 0.4839, Valid Auroc: 0.7304\n",
            "Epoch: 1113, Train Loss: 0.4814, Train auroc: 0.7439, Valid Loss: 0.4883, Valid Auroc: 0.7303\n",
            "Epoch: 1114, Train Loss: 0.4772, Train auroc: 0.7439, Valid Loss: 0.4764, Valid Auroc: 0.7330\n",
            "Epoch: 1115, Train Loss: 0.4786, Train auroc: 0.7445, Valid Loss: 0.4772, Valid Auroc: 0.7331\n",
            "Epoch: 1116, Train Loss: 0.4769, Train auroc: 0.7434, Valid Loss: 0.4851, Valid Auroc: 0.7275\n",
            "Epoch: 1117, Train Loss: 0.4804, Train auroc: 0.7427, Valid Loss: 0.4885, Valid Auroc: 0.7279\n",
            "Epoch: 1118, Train Loss: 0.4772, Train auroc: 0.7457, Valid Loss: 0.4755, Valid Auroc: 0.7359\n",
            "Epoch: 1119, Train Loss: 0.4774, Train auroc: 0.7466, Valid Loss: 0.4735, Valid Auroc: 0.7374\n",
            "Epoch: 1120, Train Loss: 0.4781, Train auroc: 0.7440, Valid Loss: 0.4861, Valid Auroc: 0.7272\n",
            "Epoch: 1121, Train Loss: 0.4785, Train auroc: 0.7437, Valid Loss: 0.4903, Valid Auroc: 0.7257\n",
            "Epoch: 1122, Train Loss: 0.4737, Train auroc: 0.7471, Valid Loss: 0.4759, Valid Auroc: 0.7350\n",
            "Epoch: 1123, Train Loss: 0.4801, Train auroc: 0.7464, Valid Loss: 0.4780, Valid Auroc: 0.7363\n",
            "Epoch: 1124, Train Loss: 0.4712, Train auroc: 0.7495, Valid Loss: 0.4747, Valid Auroc: 0.7364\n",
            "Epoch: 1125, Train Loss: 0.4776, Train auroc: 0.7447, Valid Loss: 0.4849, Valid Auroc: 0.7291\n",
            "Epoch: 1126, Train Loss: 0.4735, Train auroc: 0.7457, Valid Loss: 0.4754, Valid Auroc: 0.7330\n",
            "Epoch: 1127, Train Loss: 0.4729, Train auroc: 0.7478, Valid Loss: 0.4731, Valid Auroc: 0.7365\n",
            "Epoch: 1128, Train Loss: 0.4702, Train auroc: 0.7508, Valid Loss: 0.4770, Valid Auroc: 0.7366\n",
            "Epoch: 1129, Train Loss: 0.4703, Train auroc: 0.7515, Valid Loss: 0.4788, Valid Auroc: 0.7358\n",
            "Epoch: 1130, Train Loss: 0.4705, Train auroc: 0.7515, Valid Loss: 0.4728, Valid Auroc: 0.7383\n",
            "Epoch: 1131, Train Loss: 0.4690, Train auroc: 0.7528, Valid Loss: 0.4704, Valid Auroc: 0.7403\n",
            "Epoch: 1132, Train Loss: 0.4693, Train auroc: 0.7534, Valid Loss: 0.4766, Valid Auroc: 0.7385\n",
            "Epoch: 1133, Train Loss: 0.4683, Train auroc: 0.7529, Valid Loss: 0.4736, Valid Auroc: 0.7382\n",
            "Epoch: 1134, Train Loss: 0.4678, Train auroc: 0.7539, Valid Loss: 0.4698, Valid Auroc: 0.7411\n",
            "Epoch: 1135, Train Loss: 0.4696, Train auroc: 0.7537, Valid Loss: 0.4760, Valid Auroc: 0.7400\n",
            "Epoch: 1136, Train Loss: 0.4662, Train auroc: 0.7558, Valid Loss: 0.4731, Valid Auroc: 0.7410\n",
            "Epoch: 1137, Train Loss: 0.4730, Train auroc: 0.7532, Valid Loss: 0.4795, Valid Auroc: 0.7377\n",
            "Epoch: 1138, Train Loss: 0.4675, Train auroc: 0.7551, Valid Loss: 0.4724, Valid Auroc: 0.7411\n",
            "Epoch: 1139, Train Loss: 0.4741, Train auroc: 0.7529, Valid Loss: 0.4839, Valid Auroc: 0.7383\n",
            "Epoch: 1140, Train Loss: 0.4727, Train auroc: 0.7529, Valid Loss: 0.4775, Valid Auroc: 0.7379\n",
            "Epoch: 1141, Train Loss: 0.4788, Train auroc: 0.7491, Valid Loss: 0.4858, Valid Auroc: 0.7323\n",
            "Epoch: 1142, Train Loss: 0.4720, Train auroc: 0.7519, Valid Loss: 0.4804, Valid Auroc: 0.7361\n",
            "Epoch: 1143, Train Loss: 0.4830, Train auroc: 0.7478, Valid Loss: 0.4904, Valid Auroc: 0.7346\n",
            "Epoch: 1144, Train Loss: 0.4732, Train auroc: 0.7515, Valid Loss: 0.4716, Valid Auroc: 0.7403\n",
            "Epoch: 1145, Train Loss: 0.4833, Train auroc: 0.7458, Valid Loss: 0.4882, Valid Auroc: 0.7297\n",
            "Epoch: 1146, Train Loss: 0.4829, Train auroc: 0.7426, Valid Loss: 0.4978, Valid Auroc: 0.7209\n",
            "Epoch: 1147, Train Loss: 0.4731, Train auroc: 0.7494, Valid Loss: 0.4804, Valid Auroc: 0.7335\n",
            "Epoch: 1148, Train Loss: 0.4872, Train auroc: 0.7418, Valid Loss: 0.4852, Valid Auroc: 0.7320\n",
            "Epoch: 1149, Train Loss: 0.4775, Train auroc: 0.7459, Valid Loss: 0.4767, Valid Auroc: 0.7351\n",
            "Epoch: 1150, Train Loss: 0.4776, Train auroc: 0.7452, Valid Loss: 0.4865, Valid Auroc: 0.7293\n",
            "Epoch: 1151, Train Loss: 0.4825, Train auroc: 0.7402, Valid Loss: 0.4923, Valid Auroc: 0.7215\n",
            "Epoch: 1152, Train Loss: 0.4796, Train auroc: 0.7441, Valid Loss: 0.4840, Valid Auroc: 0.7284\n",
            "Epoch: 1153, Train Loss: 0.4749, Train auroc: 0.7478, Valid Loss: 0.4756, Valid Auroc: 0.7357\n",
            "Epoch: 1154, Train Loss: 0.4786, Train auroc: 0.7465, Valid Loss: 0.4850, Valid Auroc: 0.7337\n",
            "Epoch: 1155, Train Loss: 0.4759, Train auroc: 0.7474, Valid Loss: 0.4839, Valid Auroc: 0.7342\n",
            "Epoch: 1156, Train Loss: 0.4758, Train auroc: 0.7464, Valid Loss: 0.4757, Valid Auroc: 0.7350\n",
            "Epoch: 1157, Train Loss: 0.4771, Train auroc: 0.7467, Valid Loss: 0.4805, Valid Auroc: 0.7329\n",
            "Epoch: 1158, Train Loss: 0.4762, Train auroc: 0.7467, Valid Loss: 0.4875, Valid Auroc: 0.7295\n",
            "Epoch: 1159, Train Loss: 0.4748, Train auroc: 0.7486, Valid Loss: 0.4838, Valid Auroc: 0.7334\n",
            "Epoch: 1160, Train Loss: 0.4756, Train auroc: 0.7490, Valid Loss: 0.4737, Valid Auroc: 0.7392\n",
            "Epoch: 1161, Train Loss: 0.4754, Train auroc: 0.7488, Valid Loss: 0.4726, Valid Auroc: 0.7389\n",
            "Epoch: 1162, Train Loss: 0.4751, Train auroc: 0.7476, Valid Loss: 0.4817, Valid Auroc: 0.7328\n",
            "Epoch: 1163, Train Loss: 0.4759, Train auroc: 0.7467, Valid Loss: 0.4847, Valid Auroc: 0.7297\n",
            "Epoch: 1164, Train Loss: 0.4763, Train auroc: 0.7484, Valid Loss: 0.4789, Valid Auroc: 0.7358\n",
            "Epoch: 1165, Train Loss: 0.4755, Train auroc: 0.7500, Valid Loss: 0.4756, Valid Auroc: 0.7388\n",
            "Epoch: 1166, Train Loss: 0.4738, Train auroc: 0.7500, Valid Loss: 0.4795, Valid Auroc: 0.7357\n",
            "Epoch: 1167, Train Loss: 0.4761, Train auroc: 0.7489, Valid Loss: 0.4880, Valid Auroc: 0.7317\n",
            "Epoch: 1168, Train Loss: 0.4733, Train auroc: 0.7496, Valid Loss: 0.4784, Valid Auroc: 0.7342\n",
            "Epoch: 1169, Train Loss: 0.4792, Train auroc: 0.7487, Valid Loss: 0.4797, Valid Auroc: 0.7361\n",
            "Epoch: 1170, Train Loss: 0.4711, Train auroc: 0.7511, Valid Loss: 0.4745, Valid Auroc: 0.7381\n",
            "Epoch: 1171, Train Loss: 0.4805, Train auroc: 0.7480, Valid Loss: 0.4884, Valid Auroc: 0.7336\n",
            "Epoch: 1172, Train Loss: 0.4692, Train auroc: 0.7523, Valid Loss: 0.4722, Valid Auroc: 0.7393\n",
            "Epoch: 1173, Train Loss: 0.4794, Train auroc: 0.7491, Valid Loss: 0.4779, Valid Auroc: 0.7374\n",
            "Epoch: 1174, Train Loss: 0.4702, Train auroc: 0.7520, Valid Loss: 0.4766, Valid Auroc: 0.7361\n",
            "Epoch: 1175, Train Loss: 0.4764, Train auroc: 0.7496, Valid Loss: 0.4888, Valid Auroc: 0.7320\n",
            "Epoch: 1176, Train Loss: 0.4724, Train auroc: 0.7517, Valid Loss: 0.4787, Valid Auroc: 0.7370\n",
            "Epoch: 1177, Train Loss: 0.4759, Train auroc: 0.7510, Valid Loss: 0.4745, Valid Auroc: 0.7403\n",
            "Epoch: 1178, Train Loss: 0.4736, Train auroc: 0.7525, Valid Loss: 0.4758, Valid Auroc: 0.7389\n",
            "Epoch: 1179, Train Loss: 0.4751, Train auroc: 0.7500, Valid Loss: 0.4870, Valid Auroc: 0.7303\n",
            "Epoch: 1180, Train Loss: 0.4741, Train auroc: 0.7507, Valid Loss: 0.4858, Valid Auroc: 0.7325\n",
            "Epoch: 1181, Train Loss: 0.4745, Train auroc: 0.7527, Valid Loss: 0.4746, Valid Auroc: 0.7413\n",
            "Epoch: 1182, Train Loss: 0.4815, Train auroc: 0.7494, Valid Loss: 0.4770, Valid Auroc: 0.7412\n",
            "Epoch: 1183, Train Loss: 0.4696, Train auroc: 0.7532, Valid Loss: 0.4760, Valid Auroc: 0.7383\n",
            "Epoch: 1184, Train Loss: 0.4813, Train auroc: 0.7482, Valid Loss: 0.4977, Valid Auroc: 0.7273\n",
            "Epoch: 1185, Train Loss: 0.4693, Train auroc: 0.7522, Valid Loss: 0.4789, Valid Auroc: 0.7341\n",
            "Epoch: 1186, Train Loss: 0.4754, Train auroc: 0.7517, Valid Loss: 0.4745, Valid Auroc: 0.7400\n",
            "Epoch: 1187, Train Loss: 0.4738, Train auroc: 0.7527, Valid Loss: 0.4732, Valid Auroc: 0.7418\n",
            "Epoch: 1188, Train Loss: 0.4732, Train auroc: 0.7539, Valid Loss: 0.4806, Valid Auroc: 0.7397\n",
            "Epoch: 1189, Train Loss: 0.4724, Train auroc: 0.7518, Valid Loss: 0.4835, Valid Auroc: 0.7336\n",
            "Epoch: 1190, Train Loss: 0.4795, Train auroc: 0.7486, Valid Loss: 0.4880, Valid Auroc: 0.7309\n",
            "Epoch: 1191, Train Loss: 0.4700, Train auroc: 0.7547, Valid Loss: 0.4717, Valid Auroc: 0.7420\n",
            "Epoch: 1192, Train Loss: 0.4782, Train auroc: 0.7503, Valid Loss: 0.4851, Valid Auroc: 0.7369\n",
            "Epoch: 1193, Train Loss: 0.4743, Train auroc: 0.7515, Valid Loss: 0.4817, Valid Auroc: 0.7369\n",
            "Epoch: 1194, Train Loss: 0.4726, Train auroc: 0.7516, Valid Loss: 0.4784, Valid Auroc: 0.7365\n",
            "Epoch: 1195, Train Loss: 0.4803, Train auroc: 0.7474, Valid Loss: 0.4870, Valid Auroc: 0.7310\n",
            "Epoch: 1196, Train Loss: 0.4716, Train auroc: 0.7517, Valid Loss: 0.4806, Valid Auroc: 0.7344\n",
            "Epoch: 1197, Train Loss: 0.4749, Train auroc: 0.7511, Valid Loss: 0.4826, Valid Auroc: 0.7357\n",
            "Epoch: 1198, Train Loss: 0.4741, Train auroc: 0.7504, Valid Loss: 0.4764, Valid Auroc: 0.7382\n",
            "Epoch: 1199, Train Loss: 0.4736, Train auroc: 0.7513, Valid Loss: 0.4724, Valid Auroc: 0.7402\n",
            "Epoch: 1200, Train Loss: 0.4710, Train auroc: 0.7515, Valid Loss: 0.4757, Valid Auroc: 0.7371\n",
            "Epoch: 1201, Train Loss: 0.4764, Train auroc: 0.7476, Valid Loss: 0.4895, Valid Auroc: 0.7283\n",
            "Epoch: 1202, Train Loss: 0.4698, Train auroc: 0.7522, Valid Loss: 0.4799, Valid Auroc: 0.7341\n",
            "Epoch: 1203, Train Loss: 0.4736, Train auroc: 0.7528, Valid Loss: 0.4732, Valid Auroc: 0.7409\n",
            "Epoch: 1204, Train Loss: 0.4714, Train auroc: 0.7541, Valid Loss: 0.4724, Valid Auroc: 0.7423\n",
            "Epoch: 1205, Train Loss: 0.4698, Train auroc: 0.7548, Valid Loss: 0.4789, Valid Auroc: 0.7392\n",
            "Epoch: 1206, Train Loss: 0.4707, Train auroc: 0.7537, Valid Loss: 0.4827, Valid Auroc: 0.7359\n",
            "Epoch: 1207, Train Loss: 0.4715, Train auroc: 0.7528, Valid Loss: 0.4777, Valid Auroc: 0.7376\n",
            "Epoch: 1208, Train Loss: 0.4699, Train auroc: 0.7551, Valid Loss: 0.4723, Valid Auroc: 0.7420\n",
            "Epoch: 1209, Train Loss: 0.4697, Train auroc: 0.7560, Valid Loss: 0.4778, Valid Auroc: 0.7405\n",
            "Epoch: 1210, Train Loss: 0.4707, Train auroc: 0.7546, Valid Loss: 0.4804, Valid Auroc: 0.7383\n",
            "Epoch: 1211, Train Loss: 0.4711, Train auroc: 0.7559, Valid Loss: 0.4771, Valid Auroc: 0.7399\n",
            "Epoch: 1212, Train Loss: 0.4682, Train auroc: 0.7572, Valid Loss: 0.4726, Valid Auroc: 0.7424\n",
            "Epoch: 1213, Train Loss: 0.4681, Train auroc: 0.7570, Valid Loss: 0.4771, Valid Auroc: 0.7402\n",
            "Epoch: 1214, Train Loss: 0.4679, Train auroc: 0.7568, Valid Loss: 0.4782, Valid Auroc: 0.7394\n",
            "Epoch: 1215, Train Loss: 0.4677, Train auroc: 0.7572, Valid Loss: 0.4734, Valid Auroc: 0.7419\n",
            "Epoch: 1216, Train Loss: 0.4713, Train auroc: 0.7564, Valid Loss: 0.4747, Valid Auroc: 0.7421\n",
            "Epoch: 1217, Train Loss: 0.4649, Train auroc: 0.7588, Valid Loss: 0.4751, Valid Auroc: 0.7411\n",
            "Epoch: 1218, Train Loss: 0.4730, Train auroc: 0.7549, Valid Loss: 0.4869, Valid Auroc: 0.7356\n",
            "Epoch: 1219, Train Loss: 0.4676, Train auroc: 0.7570, Valid Loss: 0.4742, Valid Auroc: 0.7400\n",
            "Epoch: 1220, Train Loss: 0.4733, Train auroc: 0.7560, Valid Loss: 0.4757, Valid Auroc: 0.7415\n",
            "Epoch: 1221, Train Loss: 0.4690, Train auroc: 0.7573, Valid Loss: 0.4764, Valid Auroc: 0.7407\n",
            "Epoch: 1222, Train Loss: 0.4734, Train auroc: 0.7551, Valid Loss: 0.4875, Valid Auroc: 0.7363\n",
            "Epoch: 1223, Train Loss: 0.4711, Train auroc: 0.7539, Valid Loss: 0.4783, Valid Auroc: 0.7379\n",
            "Epoch: 1224, Train Loss: 0.4738, Train auroc: 0.7549, Valid Loss: 0.4748, Valid Auroc: 0.7422\n",
            "Epoch: 1225, Train Loss: 0.4723, Train auroc: 0.7554, Valid Loss: 0.4770, Valid Auroc: 0.7392\n",
            "Epoch: 1226, Train Loss: 0.4743, Train auroc: 0.7529, Valid Loss: 0.4875, Valid Auroc: 0.7330\n",
            "Epoch: 1227, Train Loss: 0.4754, Train auroc: 0.7495, Valid Loss: 0.4852, Valid Auroc: 0.7310\n",
            "Epoch: 1228, Train Loss: 0.4705, Train auroc: 0.7544, Valid Loss: 0.4714, Valid Auroc: 0.7408\n",
            "Epoch: 1229, Train Loss: 0.4709, Train auroc: 0.7564, Valid Loss: 0.4723, Valid Auroc: 0.7427\n",
            "Epoch: 1230, Train Loss: 0.4680, Train auroc: 0.7567, Valid Loss: 0.4767, Valid Auroc: 0.7390\n",
            "Epoch: 1231, Train Loss: 0.4703, Train auroc: 0.7536, Valid Loss: 0.4850, Valid Auroc: 0.7324\n",
            "Epoch: 1232, Train Loss: 0.4674, Train auroc: 0.7558, Valid Loss: 0.4757, Valid Auroc: 0.7388\n",
            "Epoch: 1233, Train Loss: 0.4693, Train auroc: 0.7550, Valid Loss: 0.4733, Valid Auroc: 0.7418\n",
            "Epoch: 1234, Train Loss: 0.4681, Train auroc: 0.7571, Valid Loss: 0.4716, Valid Auroc: 0.7439\n",
            "Epoch: 1235, Train Loss: 0.4663, Train auroc: 0.7578, Valid Loss: 0.4754, Valid Auroc: 0.7409\n",
            "Epoch: 1236, Train Loss: 0.4702, Train auroc: 0.7550, Valid Loss: 0.4846, Valid Auroc: 0.7342\n",
            "Epoch: 1237, Train Loss: 0.4657, Train auroc: 0.7586, Valid Loss: 0.4749, Valid Auroc: 0.7412\n",
            "Epoch: 1238, Train Loss: 0.4679, Train auroc: 0.7573, Valid Loss: 0.4710, Valid Auroc: 0.7439\n",
            "Epoch: 1239, Train Loss: 0.4679, Train auroc: 0.7584, Valid Loss: 0.4705, Valid Auroc: 0.7447\n",
            "Epoch: 1240, Train Loss: 0.4652, Train auroc: 0.7588, Valid Loss: 0.4759, Valid Auroc: 0.7411\n",
            "Epoch: 1241, Train Loss: 0.4657, Train auroc: 0.7578, Valid Loss: 0.4766, Valid Auroc: 0.7392\n",
            "Epoch: 1242, Train Loss: 0.4636, Train auroc: 0.7606, Valid Loss: 0.4701, Valid Auroc: 0.7446\n",
            "Epoch: 1243, Train Loss: 0.4638, Train auroc: 0.7615, Valid Loss: 0.4693, Valid Auroc: 0.7464\n",
            "Epoch: 1244, Train Loss: 0.4638, Train auroc: 0.7612, Valid Loss: 0.4751, Valid Auroc: 0.7443\n",
            "Epoch: 1245, Train Loss: 0.4639, Train auroc: 0.7605, Valid Loss: 0.4728, Valid Auroc: 0.7431\n",
            "Epoch: 1246, Train Loss: 0.4651, Train auroc: 0.7610, Valid Loss: 0.4727, Valid Auroc: 0.7440\n",
            "Epoch: 1247, Train Loss: 0.4646, Train auroc: 0.7613, Valid Loss: 0.4739, Valid Auroc: 0.7451\n",
            "Epoch: 1248, Train Loss: 0.4656, Train auroc: 0.7613, Valid Loss: 0.4746, Valid Auroc: 0.7456\n",
            "Epoch: 1249, Train Loss: 0.4667, Train auroc: 0.7599, Valid Loss: 0.4716, Valid Auroc: 0.7448\n",
            "Epoch: 1250, Train Loss: 0.4670, Train auroc: 0.7595, Valid Loss: 0.4771, Valid Auroc: 0.7416\n",
            "Epoch: 1251, Train Loss: 0.4709, Train auroc: 0.7593, Valid Loss: 0.4865, Valid Auroc: 0.7405\n",
            "Epoch: 1252, Train Loss: 0.4657, Train auroc: 0.7611, Valid Loss: 0.4706, Valid Auroc: 0.7470\n",
            "Epoch: 1253, Train Loss: 0.4782, Train auroc: 0.7559, Valid Loss: 0.4775, Valid Auroc: 0.7446\n",
            "Epoch: 1254, Train Loss: 0.4666, Train auroc: 0.7597, Valid Loss: 0.4745, Valid Auroc: 0.7421\n",
            "Epoch: 1255, Train Loss: 0.4857, Train auroc: 0.7512, Valid Loss: 0.5040, Valid Auroc: 0.7300\n",
            "Epoch: 1256, Train Loss: 0.4673, Train auroc: 0.7573, Valid Loss: 0.4799, Valid Auroc: 0.7377\n",
            "Epoch: 1257, Train Loss: 0.4846, Train auroc: 0.7510, Valid Loss: 0.4844, Valid Auroc: 0.7388\n",
            "Epoch: 1258, Train Loss: 0.4812, Train auroc: 0.7509, Valid Loss: 0.4785, Valid Auroc: 0.7418\n",
            "Epoch: 1259, Train Loss: 0.4718, Train auroc: 0.7556, Valid Loss: 0.4777, Valid Auroc: 0.7412\n",
            "Epoch: 1260, Train Loss: 0.4840, Train auroc: 0.7496, Valid Loss: 0.4995, Valid Auroc: 0.7288\n",
            "Epoch: 1261, Train Loss: 0.4766, Train auroc: 0.7492, Valid Loss: 0.4905, Valid Auroc: 0.7279\n",
            "Epoch: 1262, Train Loss: 0.4794, Train auroc: 0.7516, Valid Loss: 0.4791, Valid Auroc: 0.7395\n",
            "Epoch: 1263, Train Loss: 0.4795, Train auroc: 0.7494, Valid Loss: 0.4770, Valid Auroc: 0.7392\n",
            "Epoch: 1264, Train Loss: 0.4785, Train auroc: 0.7513, Valid Loss: 0.4848, Valid Auroc: 0.7381\n",
            "Epoch: 1265, Train Loss: 0.4738, Train auroc: 0.7524, Valid Loss: 0.4836, Valid Auroc: 0.7350\n",
            "Epoch: 1266, Train Loss: 0.4827, Train auroc: 0.7459, Valid Loss: 0.4942, Valid Auroc: 0.7256\n",
            "Epoch: 1267, Train Loss: 0.4751, Train auroc: 0.7524, Valid Loss: 0.4812, Valid Auroc: 0.7370\n",
            "Epoch: 1268, Train Loss: 0.4726, Train auroc: 0.7525, Valid Loss: 0.4763, Valid Auroc: 0.7397\n",
            "Epoch: 1269, Train Loss: 0.4768, Train auroc: 0.7512, Valid Loss: 0.4842, Valid Auroc: 0.7376\n",
            "Epoch: 1270, Train Loss: 0.4740, Train auroc: 0.7524, Valid Loss: 0.4789, Valid Auroc: 0.7390\n",
            "Epoch: 1271, Train Loss: 0.4739, Train auroc: 0.7514, Valid Loss: 0.4789, Valid Auroc: 0.7372\n",
            "Epoch: 1272, Train Loss: 0.4724, Train auroc: 0.7521, Valid Loss: 0.4814, Valid Auroc: 0.7359\n",
            "Epoch: 1273, Train Loss: 0.4743, Train auroc: 0.7518, Valid Loss: 0.4851, Valid Auroc: 0.7352\n",
            "Epoch: 1274, Train Loss: 0.4724, Train auroc: 0.7536, Valid Loss: 0.4780, Valid Auroc: 0.7402\n",
            "Epoch: 1275, Train Loss: 0.4713, Train auroc: 0.7539, Valid Loss: 0.4712, Valid Auroc: 0.7425\n",
            "Epoch: 1276, Train Loss: 0.4732, Train auroc: 0.7515, Valid Loss: 0.4733, Valid Auroc: 0.7402\n",
            "Epoch: 1277, Train Loss: 0.4733, Train auroc: 0.7502, Valid Loss: 0.4782, Valid Auroc: 0.7368\n",
            "Epoch: 1278, Train Loss: 0.4704, Train auroc: 0.7528, Valid Loss: 0.4776, Valid Auroc: 0.7377\n",
            "Epoch: 1279, Train Loss: 0.4715, Train auroc: 0.7541, Valid Loss: 0.4768, Valid Auroc: 0.7401\n",
            "Epoch: 1280, Train Loss: 0.4699, Train auroc: 0.7554, Valid Loss: 0.4717, Valid Auroc: 0.7432\n",
            "Epoch: 1281, Train Loss: 0.4675, Train auroc: 0.7564, Valid Loss: 0.4728, Valid Auroc: 0.7421\n",
            "Epoch: 1282, Train Loss: 0.4714, Train auroc: 0.7549, Valid Loss: 0.4765, Valid Auroc: 0.7401\n",
            "Epoch: 1283, Train Loss: 0.4682, Train auroc: 0.7562, Valid Loss: 0.4719, Valid Auroc: 0.7427\n",
            "Epoch: 1284, Train Loss: 0.4678, Train auroc: 0.7564, Valid Loss: 0.4699, Valid Auroc: 0.7443\n",
            "Epoch: 1285, Train Loss: 0.4703, Train auroc: 0.7552, Valid Loss: 0.4752, Valid Auroc: 0.7421\n",
            "Epoch: 1286, Train Loss: 0.4664, Train auroc: 0.7571, Valid Loss: 0.4740, Valid Auroc: 0.7418\n",
            "Epoch: 1287, Train Loss: 0.4693, Train auroc: 0.7565, Valid Loss: 0.4740, Valid Auroc: 0.7410\n",
            "Epoch: 1288, Train Loss: 0.4665, Train auroc: 0.7584, Valid Loss: 0.4700, Valid Auroc: 0.7433\n",
            "Epoch: 1289, Train Loss: 0.4691, Train auroc: 0.7575, Valid Loss: 0.4757, Valid Auroc: 0.7432\n",
            "Epoch: 1290, Train Loss: 0.4656, Train auroc: 0.7592, Valid Loss: 0.4705, Valid Auroc: 0.7455\n",
            "Epoch: 1291, Train Loss: 0.4718, Train auroc: 0.7570, Valid Loss: 0.4761, Valid Auroc: 0.7432\n",
            "Epoch: 1292, Train Loss: 0.4651, Train auroc: 0.7596, Valid Loss: 0.4708, Valid Auroc: 0.7445\n",
            "Epoch: 1293, Train Loss: 0.4743, Train auroc: 0.7560, Valid Loss: 0.4858, Valid Auroc: 0.7399\n",
            "Epoch: 1294, Train Loss: 0.4650, Train auroc: 0.7594, Valid Loss: 0.4699, Valid Auroc: 0.7447\n",
            "Epoch: 1295, Train Loss: 0.4763, Train auroc: 0.7560, Valid Loss: 0.4772, Valid Auroc: 0.7427\n",
            "Epoch: 1296, Train Loss: 0.4724, Train auroc: 0.7557, Valid Loss: 0.4787, Valid Auroc: 0.7397\n",
            "Epoch: 1297, Train Loss: 0.4711, Train auroc: 0.7552, Valid Loss: 0.4802, Valid Auroc: 0.7383\n",
            "Epoch: 1298, Train Loss: 0.4754, Train auroc: 0.7543, Valid Loss: 0.4827, Valid Auroc: 0.7391\n",
            "Epoch: 1299, Train Loss: 0.4700, Train auroc: 0.7555, Valid Loss: 0.4708, Valid Auroc: 0.7429\n",
            "Epoch: 1300, Train Loss: 0.4743, Train auroc: 0.7554, Valid Loss: 0.4751, Valid Auroc: 0.7426\n",
            "Epoch: 1301, Train Loss: 0.4707, Train auroc: 0.7559, Valid Loss: 0.4808, Valid Auroc: 0.7380\n",
            "Epoch: 1302, Train Loss: 0.4744, Train auroc: 0.7534, Valid Loss: 0.4856, Valid Auroc: 0.7351\n",
            "Epoch: 1303, Train Loss: 0.4738, Train auroc: 0.7532, Valid Loss: 0.4778, Valid Auroc: 0.7385\n",
            "Epoch: 1304, Train Loss: 0.4706, Train auroc: 0.7564, Valid Loss: 0.4709, Valid Auroc: 0.7432\n",
            "Epoch: 1305, Train Loss: 0.4735, Train auroc: 0.7543, Valid Loss: 0.4764, Valid Auroc: 0.7396\n",
            "Epoch: 1306, Train Loss: 0.4736, Train auroc: 0.7529, Valid Loss: 0.4823, Valid Auroc: 0.7357\n",
            "Epoch: 1307, Train Loss: 0.4676, Train auroc: 0.7560, Valid Loss: 0.4758, Valid Auroc: 0.7395\n",
            "Epoch: 1308, Train Loss: 0.4731, Train auroc: 0.7562, Valid Loss: 0.4755, Valid Auroc: 0.7433\n",
            "Epoch: 1309, Train Loss: 0.4683, Train auroc: 0.7573, Valid Loss: 0.4693, Valid Auroc: 0.7447\n",
            "Epoch: 1310, Train Loss: 0.4670, Train auroc: 0.7577, Valid Loss: 0.4740, Valid Auroc: 0.7422\n",
            "Epoch: 1311, Train Loss: 0.4717, Train auroc: 0.7558, Valid Loss: 0.4808, Valid Auroc: 0.7388\n",
            "Epoch: 1312, Train Loss: 0.4678, Train auroc: 0.7562, Valid Loss: 0.4726, Valid Auroc: 0.7413\n",
            "Epoch: 1313, Train Loss: 0.4666, Train auroc: 0.7585, Valid Loss: 0.4695, Valid Auroc: 0.7446\n",
            "Epoch: 1314, Train Loss: 0.4698, Train auroc: 0.7576, Valid Loss: 0.4776, Valid Auroc: 0.7430\n",
            "Epoch: 1315, Train Loss: 0.4648, Train auroc: 0.7582, Valid Loss: 0.4724, Valid Auroc: 0.7435\n",
            "Epoch: 1316, Train Loss: 0.4703, Train auroc: 0.7569, Valid Loss: 0.4784, Valid Auroc: 0.7393\n",
            "Epoch: 1317, Train Loss: 0.4652, Train auroc: 0.7606, Valid Loss: 0.4723, Valid Auroc: 0.7439\n",
            "Epoch: 1318, Train Loss: 0.4701, Train auroc: 0.7576, Valid Loss: 0.4757, Valid Auroc: 0.7434\n",
            "Epoch: 1319, Train Loss: 0.4667, Train auroc: 0.7585, Valid Loss: 0.4693, Valid Auroc: 0.7456\n",
            "Epoch: 1320, Train Loss: 0.4696, Train auroc: 0.7596, Valid Loss: 0.4716, Valid Auroc: 0.7466\n",
            "Epoch: 1321, Train Loss: 0.4666, Train auroc: 0.7583, Valid Loss: 0.4751, Valid Auroc: 0.7418\n",
            "Epoch: 1322, Train Loss: 0.4711, Train auroc: 0.7581, Valid Loss: 0.4822, Valid Auroc: 0.7411\n",
            "Epoch: 1323, Train Loss: 0.4627, Train auroc: 0.7619, Valid Loss: 0.4672, Valid Auroc: 0.7469\n",
            "Epoch: 1324, Train Loss: 0.4721, Train auroc: 0.7583, Valid Loss: 0.4705, Valid Auroc: 0.7463\n",
            "Epoch: 1325, Train Loss: 0.4647, Train auroc: 0.7610, Valid Loss: 0.4687, Valid Auroc: 0.7466\n",
            "Epoch: 1326, Train Loss: 0.4697, Train auroc: 0.7589, Valid Loss: 0.4830, Valid Auroc: 0.7398\n",
            "Epoch: 1327, Train Loss: 0.4642, Train auroc: 0.7587, Valid Loss: 0.4732, Valid Auroc: 0.7415\n",
            "Epoch: 1328, Train Loss: 0.4686, Train auroc: 0.7591, Valid Loss: 0.4690, Valid Auroc: 0.7468\n",
            "Epoch: 1329, Train Loss: 0.4646, Train auroc: 0.7617, Valid Loss: 0.4668, Valid Auroc: 0.7488\n",
            "Epoch: 1330, Train Loss: 0.4670, Train auroc: 0.7604, Valid Loss: 0.4747, Valid Auroc: 0.7448\n",
            "Epoch: 1331, Train Loss: 0.4676, Train auroc: 0.7590, Valid Loss: 0.4771, Valid Auroc: 0.7409\n",
            "Epoch: 1332, Train Loss: 0.4680, Train auroc: 0.7601, Valid Loss: 0.4730, Valid Auroc: 0.7443\n",
            "Epoch: 1333, Train Loss: 0.4650, Train auroc: 0.7616, Valid Loss: 0.4679, Valid Auroc: 0.7480\n",
            "Epoch: 1334, Train Loss: 0.4713, Train auroc: 0.7592, Valid Loss: 0.4806, Valid Auroc: 0.7441\n",
            "Epoch: 1335, Train Loss: 0.4633, Train auroc: 0.7618, Valid Loss: 0.4684, Valid Auroc: 0.7462\n",
            "Epoch: 1336, Train Loss: 0.4751, Train auroc: 0.7577, Valid Loss: 0.4809, Valid Auroc: 0.7411\n",
            "Epoch: 1337, Train Loss: 0.4639, Train auroc: 0.7615, Valid Loss: 0.4702, Valid Auroc: 0.7463\n",
            "Epoch: 1338, Train Loss: 0.4721, Train auroc: 0.7597, Valid Loss: 0.4805, Valid Auroc: 0.7447\n",
            "Epoch: 1339, Train Loss: 0.4661, Train auroc: 0.7604, Valid Loss: 0.4696, Valid Auroc: 0.7471\n",
            "Epoch: 1340, Train Loss: 0.4716, Train auroc: 0.7584, Valid Loss: 0.4722, Valid Auroc: 0.7456\n",
            "Epoch: 1341, Train Loss: 0.4669, Train auroc: 0.7598, Valid Loss: 0.4770, Valid Auroc: 0.7412\n",
            "Epoch: 1342, Train Loss: 0.4746, Train auroc: 0.7568, Valid Loss: 0.4884, Valid Auroc: 0.7378\n",
            "Epoch: 1343, Train Loss: 0.4638, Train auroc: 0.7617, Valid Loss: 0.4686, Valid Auroc: 0.7473\n",
            "Epoch: 1344, Train Loss: 0.4718, Train auroc: 0.7586, Valid Loss: 0.4699, Valid Auroc: 0.7481\n",
            "Epoch: 1345, Train Loss: 0.4650, Train auroc: 0.7617, Valid Loss: 0.4688, Valid Auroc: 0.7478\n",
            "Epoch: 1346, Train Loss: 0.4713, Train auroc: 0.7590, Valid Loss: 0.4874, Valid Auroc: 0.7387\n",
            "Epoch: 1347, Train Loss: 0.4646, Train auroc: 0.7609, Valid Loss: 0.4749, Valid Auroc: 0.7424\n",
            "Epoch: 1348, Train Loss: 0.4724, Train auroc: 0.7598, Valid Loss: 0.4741, Valid Auroc: 0.7467\n",
            "Epoch: 1349, Train Loss: 0.4708, Train auroc: 0.7604, Valid Loss: 0.4700, Valid Auroc: 0.7490\n",
            "Epoch: 1350, Train Loss: 0.4639, Train auroc: 0.7624, Valid Loss: 0.4709, Valid Auroc: 0.7469\n",
            "Epoch: 1351, Train Loss: 0.4749, Train auroc: 0.7585, Valid Loss: 0.4892, Valid Auroc: 0.7393\n",
            "Epoch: 1352, Train Loss: 0.4644, Train auroc: 0.7611, Valid Loss: 0.4746, Valid Auroc: 0.7425\n",
            "Epoch: 1353, Train Loss: 0.4719, Train auroc: 0.7605, Valid Loss: 0.4746, Valid Auroc: 0.7465\n",
            "Epoch: 1354, Train Loss: 0.4682, Train auroc: 0.7594, Valid Loss: 0.4687, Valid Auroc: 0.7485\n",
            "Epoch: 1355, Train Loss: 0.4663, Train auroc: 0.7614, Valid Loss: 0.4723, Valid Auroc: 0.7479\n",
            "Epoch: 1356, Train Loss: 0.4635, Train auroc: 0.7617, Valid Loss: 0.4721, Valid Auroc: 0.7453\n",
            "Epoch: 1357, Train Loss: 0.4639, Train auroc: 0.7613, Valid Loss: 0.4703, Valid Auroc: 0.7456\n",
            "Epoch: 1358, Train Loss: 0.4611, Train auroc: 0.7644, Valid Loss: 0.4664, Valid Auroc: 0.7492\n",
            "Epoch: 1359, Train Loss: 0.4632, Train auroc: 0.7641, Valid Loss: 0.4694, Valid Auroc: 0.7487\n",
            "Epoch: 1360, Train Loss: 0.4606, Train auroc: 0.7655, Valid Loss: 0.4679, Valid Auroc: 0.7491\n",
            "Epoch: 1361, Train Loss: 0.4634, Train auroc: 0.7646, Valid Loss: 0.4702, Valid Auroc: 0.7479\n",
            "Epoch: 1362, Train Loss: 0.4592, Train auroc: 0.7661, Valid Loss: 0.4647, Valid Auroc: 0.7505\n",
            "Epoch: 1363, Train Loss: 0.4637, Train auroc: 0.7653, Valid Loss: 0.4715, Valid Auroc: 0.7495\n",
            "Epoch: 1364, Train Loss: 0.4581, Train auroc: 0.7670, Valid Loss: 0.4642, Valid Auroc: 0.7508\n",
            "Epoch: 1365, Train Loss: 0.4628, Train auroc: 0.7662, Valid Loss: 0.4701, Valid Auroc: 0.7486\n",
            "Epoch: 1366, Train Loss: 0.4577, Train auroc: 0.7686, Valid Loss: 0.4668, Valid Auroc: 0.7507\n",
            "Epoch: 1367, Train Loss: 0.4628, Train auroc: 0.7663, Valid Loss: 0.4734, Valid Auroc: 0.7493\n",
            "Epoch: 1368, Train Loss: 0.4639, Train auroc: 0.7651, Valid Loss: 0.4664, Valid Auroc: 0.7498\n",
            "Epoch: 1369, Train Loss: 0.4592, Train auroc: 0.7683, Valid Loss: 0.4676, Valid Auroc: 0.7508\n",
            "Epoch: 1370, Train Loss: 0.4668, Train auroc: 0.7658, Valid Loss: 0.4809, Valid Auroc: 0.7473\n",
            "Epoch: 1371, Train Loss: 0.4594, Train auroc: 0.7679, Valid Loss: 0.4665, Valid Auroc: 0.7512\n",
            "Epoch: 1372, Train Loss: 0.4682, Train auroc: 0.7648, Valid Loss: 0.4708, Valid Auroc: 0.7505\n",
            "Epoch: 1373, Train Loss: 0.4619, Train auroc: 0.7668, Valid Loss: 0.4686, Valid Auroc: 0.7504\n",
            "Epoch: 1374, Train Loss: 0.4705, Train auroc: 0.7633, Valid Loss: 0.4863, Valid Auroc: 0.7439\n",
            "Epoch: 1375, Train Loss: 0.4638, Train auroc: 0.7638, Valid Loss: 0.4738, Valid Auroc: 0.7454\n",
            "Epoch: 1376, Train Loss: 0.4719, Train auroc: 0.7620, Valid Loss: 0.4768, Valid Auroc: 0.7460\n",
            "Epoch: 1377, Train Loss: 0.4714, Train auroc: 0.7628, Valid Loss: 0.4752, Valid Auroc: 0.7485\n",
            "Epoch: 1378, Train Loss: 0.4662, Train auroc: 0.7641, Valid Loss: 0.4743, Valid Auroc: 0.7486\n",
            "Epoch: 1379, Train Loss: 0.4742, Train auroc: 0.7577, Valid Loss: 0.4847, Valid Auroc: 0.7407\n",
            "Epoch: 1380, Train Loss: 0.4721, Train auroc: 0.7583, Valid Loss: 0.4787, Valid Auroc: 0.7411\n",
            "Epoch: 1381, Train Loss: 0.4711, Train auroc: 0.7603, Valid Loss: 0.4759, Valid Auroc: 0.7441\n",
            "Epoch: 1382, Train Loss: 0.4728, Train auroc: 0.7594, Valid Loss: 0.4791, Valid Auroc: 0.7433\n",
            "Epoch: 1383, Train Loss: 0.4728, Train auroc: 0.7604, Valid Loss: 0.4832, Valid Auroc: 0.7434\n",
            "Epoch: 1384, Train Loss: 0.4673, Train auroc: 0.7593, Valid Loss: 0.4730, Valid Auroc: 0.7438\n",
            "Epoch: 1385, Train Loss: 0.4799, Train auroc: 0.7549, Valid Loss: 0.4817, Valid Auroc: 0.7410\n",
            "Epoch: 1386, Train Loss: 0.4659, Train auroc: 0.7613, Valid Loss: 0.4686, Valid Auroc: 0.7460\n",
            "Epoch: 1387, Train Loss: 0.4701, Train auroc: 0.7592, Valid Loss: 0.4794, Valid Auroc: 0.7434\n",
            "Epoch: 1388, Train Loss: 0.4729, Train auroc: 0.7589, Valid Loss: 0.4801, Valid Auroc: 0.7438\n",
            "Epoch: 1389, Train Loss: 0.4658, Train auroc: 0.7618, Valid Loss: 0.4721, Valid Auroc: 0.7458\n",
            "Epoch: 1390, Train Loss: 0.4721, Train auroc: 0.7607, Valid Loss: 0.4756, Valid Auroc: 0.7459\n",
            "Epoch: 1391, Train Loss: 0.4676, Train auroc: 0.7607, Valid Loss: 0.4703, Valid Auroc: 0.7469\n",
            "Epoch: 1392, Train Loss: 0.4690, Train auroc: 0.7615, Valid Loss: 0.4786, Valid Auroc: 0.7451\n",
            "Epoch: 1393, Train Loss: 0.4658, Train auroc: 0.7614, Valid Loss: 0.4711, Valid Auroc: 0.7467\n",
            "Epoch: 1394, Train Loss: 0.4677, Train auroc: 0.7610, Valid Loss: 0.4727, Valid Auroc: 0.7462\n",
            "Epoch: 1395, Train Loss: 0.4629, Train auroc: 0.7636, Valid Loss: 0.4706, Valid Auroc: 0.7471\n",
            "Epoch: 1396, Train Loss: 0.4645, Train auroc: 0.7630, Valid Loss: 0.4737, Valid Auroc: 0.7461\n",
            "Epoch: 1397, Train Loss: 0.4616, Train auroc: 0.7643, Valid Loss: 0.4692, Valid Auroc: 0.7475\n",
            "Epoch: 1398, Train Loss: 0.4634, Train auroc: 0.7648, Valid Loss: 0.4680, Valid Auroc: 0.7493\n",
            "Epoch: 1399, Train Loss: 0.4596, Train auroc: 0.7664, Valid Loss: 0.4668, Valid Auroc: 0.7504\n",
            "Epoch: 1400, Train Loss: 0.4632, Train auroc: 0.7657, Valid Loss: 0.4739, Valid Auroc: 0.7484\n",
            "Epoch: 1401, Train Loss: 0.4592, Train auroc: 0.7670, Valid Loss: 0.4671, Valid Auroc: 0.7498\n",
            "Epoch: 1402, Train Loss: 0.4639, Train auroc: 0.7664, Valid Loss: 0.4686, Valid Auroc: 0.7498\n",
            "Epoch: 1403, Train Loss: 0.4595, Train auroc: 0.7686, Valid Loss: 0.4646, Valid Auroc: 0.7527\n",
            "Epoch: 1404, Train Loss: 0.4642, Train auroc: 0.7661, Valid Loss: 0.4737, Valid Auroc: 0.7502\n",
            "Epoch: 1405, Train Loss: 0.4578, Train auroc: 0.7684, Valid Loss: 0.4647, Valid Auroc: 0.7528\n",
            "Epoch: 1406, Train Loss: 0.4618, Train auroc: 0.7678, Valid Loss: 0.4670, Valid Auroc: 0.7513\n",
            "Epoch: 1407, Train Loss: 0.4574, Train auroc: 0.7686, Valid Loss: 0.4659, Valid Auroc: 0.7508\n",
            "Epoch: 1408, Train Loss: 0.4591, Train auroc: 0.7690, Valid Loss: 0.4709, Valid Auroc: 0.7504\n",
            "Epoch: 1409, Train Loss: 0.4554, Train auroc: 0.7708, Valid Loss: 0.4626, Valid Auroc: 0.7546\n",
            "Epoch: 1410, Train Loss: 0.4600, Train auroc: 0.7694, Valid Loss: 0.4644, Valid Auroc: 0.7546\n",
            "Epoch: 1411, Train Loss: 0.4550, Train auroc: 0.7711, Valid Loss: 0.4629, Valid Auroc: 0.7542\n",
            "Epoch: 1412, Train Loss: 0.4613, Train auroc: 0.7693, Valid Loss: 0.4751, Valid Auroc: 0.7500\n",
            "Epoch: 1413, Train Loss: 0.4550, Train auroc: 0.7714, Valid Loss: 0.4643, Valid Auroc: 0.7532\n",
            "Epoch: 1414, Train Loss: 0.4632, Train auroc: 0.7693, Valid Loss: 0.4690, Valid Auroc: 0.7523\n",
            "Epoch: 1415, Train Loss: 0.4556, Train auroc: 0.7722, Valid Loss: 0.4611, Valid Auroc: 0.7566\n",
            "Epoch: 1416, Train Loss: 0.4653, Train auroc: 0.7695, Valid Loss: 0.4777, Valid Auroc: 0.7516\n",
            "Epoch: 1417, Train Loss: 0.4554, Train auroc: 0.7712, Valid Loss: 0.4653, Valid Auroc: 0.7523\n",
            "Epoch: 1418, Train Loss: 0.4702, Train auroc: 0.7657, Valid Loss: 0.4765, Valid Auroc: 0.7478\n",
            "Epoch: 1419, Train Loss: 0.4607, Train auroc: 0.7696, Valid Loss: 0.4655, Valid Auroc: 0.7547\n",
            "Epoch: 1420, Train Loss: 0.4674, Train auroc: 0.7675, Valid Loss: 0.4781, Valid Auroc: 0.7518\n",
            "Epoch: 1421, Train Loss: 0.4632, Train auroc: 0.7688, Valid Loss: 0.4765, Valid Auroc: 0.7511\n",
            "Epoch: 1422, Train Loss: 0.4679, Train auroc: 0.7647, Valid Loss: 0.4717, Valid Auroc: 0.7496\n",
            "Epoch: 1423, Train Loss: 0.4678, Train auroc: 0.7652, Valid Loss: 0.4702, Valid Auroc: 0.7508\n",
            "Epoch: 1424, Train Loss: 0.4646, Train auroc: 0.7654, Valid Loss: 0.4734, Valid Auroc: 0.7481\n",
            "Epoch: 1425, Train Loss: 0.4738, Train auroc: 0.7618, Valid Loss: 0.4894, Valid Auroc: 0.7421\n",
            "Epoch: 1426, Train Loss: 0.4637, Train auroc: 0.7643, Valid Loss: 0.4737, Valid Auroc: 0.7447\n",
            "Epoch: 1427, Train Loss: 0.4745, Train auroc: 0.7628, Valid Loss: 0.4752, Valid Auroc: 0.7491\n",
            "Epoch: 1428, Train Loss: 0.4642, Train auroc: 0.7652, Valid Loss: 0.4673, Valid Auroc: 0.7511\n",
            "Epoch: 1429, Train Loss: 0.4688, Train auroc: 0.7641, Valid Loss: 0.4831, Valid Auroc: 0.7463\n",
            "Epoch: 1430, Train Loss: 0.4626, Train auroc: 0.7657, Valid Loss: 0.4757, Valid Auroc: 0.7463\n",
            "Epoch: 1431, Train Loss: 0.4618, Train auroc: 0.7675, Valid Loss: 0.4669, Valid Auroc: 0.7514\n",
            "Epoch: 1432, Train Loss: 0.4643, Train auroc: 0.7662, Valid Loss: 0.4663, Valid Auroc: 0.7527\n",
            "Epoch: 1433, Train Loss: 0.4589, Train auroc: 0.7690, Valid Loss: 0.4667, Valid Auroc: 0.7523\n",
            "Epoch: 1434, Train Loss: 0.4639, Train auroc: 0.7667, Valid Loss: 0.4779, Valid Auroc: 0.7463\n",
            "Epoch: 1435, Train Loss: 0.4587, Train auroc: 0.7698, Valid Loss: 0.4678, Valid Auroc: 0.7515\n",
            "Epoch: 1436, Train Loss: 0.4640, Train auroc: 0.7673, Valid Loss: 0.4660, Valid Auroc: 0.7527\n",
            "Epoch: 1437, Train Loss: 0.4607, Train auroc: 0.7685, Valid Loss: 0.4664, Valid Auroc: 0.7526\n",
            "Epoch: 1438, Train Loss: 0.4605, Train auroc: 0.7691, Valid Loss: 0.4746, Valid Auroc: 0.7486\n",
            "Epoch: 1439, Train Loss: 0.4636, Train auroc: 0.7670, Valid Loss: 0.4781, Valid Auroc: 0.7463\n",
            "Epoch: 1440, Train Loss: 0.4559, Train auroc: 0.7711, Valid Loss: 0.4616, Valid Auroc: 0.7556\n",
            "Epoch: 1441, Train Loss: 0.4642, Train auroc: 0.7666, Valid Loss: 0.4689, Valid Auroc: 0.7521\n",
            "Epoch: 1442, Train Loss: 0.4568, Train auroc: 0.7701, Valid Loss: 0.4633, Valid Auroc: 0.7535\n",
            "Epoch: 1443, Train Loss: 0.4613, Train auroc: 0.7683, Valid Loss: 0.4733, Valid Auroc: 0.7471\n",
            "Epoch: 1444, Train Loss: 0.4594, Train auroc: 0.7689, Valid Loss: 0.4695, Valid Auroc: 0.7501\n",
            "Epoch: 1445, Train Loss: 0.4575, Train auroc: 0.7706, Valid Loss: 0.4623, Valid Auroc: 0.7560\n",
            "Epoch: 1446, Train Loss: 0.4561, Train auroc: 0.7705, Valid Loss: 0.4619, Valid Auroc: 0.7554\n",
            "Epoch: 1447, Train Loss: 0.4547, Train auroc: 0.7716, Valid Loss: 0.4646, Valid Auroc: 0.7541\n",
            "Epoch: 1448, Train Loss: 0.4550, Train auroc: 0.7722, Valid Loss: 0.4660, Valid Auroc: 0.7531\n",
            "Epoch: 1449, Train Loss: 0.4534, Train auroc: 0.7735, Valid Loss: 0.4615, Valid Auroc: 0.7553\n",
            "Epoch: 1450, Train Loss: 0.4541, Train auroc: 0.7736, Valid Loss: 0.4599, Valid Auroc: 0.7573\n",
            "Epoch: 1451, Train Loss: 0.4528, Train auroc: 0.7742, Valid Loss: 0.4604, Valid Auroc: 0.7575\n",
            "Epoch: 1452, Train Loss: 0.4531, Train auroc: 0.7745, Valid Loss: 0.4640, Valid Auroc: 0.7556\n",
            "Epoch: 1453, Train Loss: 0.4523, Train auroc: 0.7745, Valid Loss: 0.4634, Valid Auroc: 0.7555\n",
            "Epoch: 1454, Train Loss: 0.4518, Train auroc: 0.7756, Valid Loss: 0.4598, Valid Auroc: 0.7580\n",
            "Epoch: 1455, Train Loss: 0.4520, Train auroc: 0.7760, Valid Loss: 0.4574, Valid Auroc: 0.7599\n",
            "Epoch: 1456, Train Loss: 0.4513, Train auroc: 0.7763, Valid Loss: 0.4596, Valid Auroc: 0.7586\n",
            "Epoch: 1457, Train Loss: 0.4523, Train auroc: 0.7757, Valid Loss: 0.4646, Valid Auroc: 0.7558\n",
            "Epoch: 1458, Train Loss: 0.4510, Train auroc: 0.7762, Valid Loss: 0.4591, Valid Auroc: 0.7581\n",
            "Epoch: 1459, Train Loss: 0.4523, Train auroc: 0.7764, Valid Loss: 0.4576, Valid Auroc: 0.7599\n",
            "Epoch: 1460, Train Loss: 0.4521, Train auroc: 0.7768, Valid Loss: 0.4625, Valid Auroc: 0.7589\n",
            "Epoch: 1461, Train Loss: 0.4525, Train auroc: 0.7767, Valid Loss: 0.4650, Valid Auroc: 0.7567\n",
            "Epoch: 1462, Train Loss: 0.4543, Train auroc: 0.7759, Valid Loss: 0.4616, Valid Auroc: 0.7576\n",
            "Epoch: 1463, Train Loss: 0.4544, Train auroc: 0.7757, Valid Loss: 0.4632, Valid Auroc: 0.7582\n",
            "Epoch: 1464, Train Loss: 0.4545, Train auroc: 0.7759, Valid Loss: 0.4658, Valid Auroc: 0.7577\n",
            "Epoch: 1465, Train Loss: 0.4594, Train auroc: 0.7734, Valid Loss: 0.4699, Valid Auroc: 0.7543\n",
            "Epoch: 1466, Train Loss: 0.4558, Train auroc: 0.7746, Valid Loss: 0.4656, Valid Auroc: 0.7557\n",
            "Epoch: 1467, Train Loss: 0.4612, Train auroc: 0.7708, Valid Loss: 0.4727, Valid Auroc: 0.7526\n",
            "Epoch: 1468, Train Loss: 0.4608, Train auroc: 0.7724, Valid Loss: 0.4667, Valid Auroc: 0.7556\n",
            "Epoch: 1469, Train Loss: 0.4655, Train auroc: 0.7687, Valid Loss: 0.4738, Valid Auroc: 0.7501\n",
            "Epoch: 1470, Train Loss: 0.4621, Train auroc: 0.7700, Valid Loss: 0.4727, Valid Auroc: 0.7522\n",
            "Epoch: 1471, Train Loss: 0.4652, Train auroc: 0.7678, Valid Loss: 0.4765, Valid Auroc: 0.7507\n",
            "Epoch: 1472, Train Loss: 0.4665, Train auroc: 0.7678, Valid Loss: 0.4693, Valid Auroc: 0.7529\n",
            "Epoch: 1473, Train Loss: 0.4688, Train auroc: 0.7660, Valid Loss: 0.4745, Valid Auroc: 0.7493\n",
            "Epoch: 1474, Train Loss: 0.4675, Train auroc: 0.7641, Valid Loss: 0.4773, Valid Auroc: 0.7457\n",
            "Epoch: 1475, Train Loss: 0.4660, Train auroc: 0.7655, Valid Loss: 0.4793, Valid Auroc: 0.7455\n",
            "Epoch: 1476, Train Loss: 0.4685, Train auroc: 0.7615, Valid Loss: 0.4711, Valid Auroc: 0.7467\n",
            "Epoch: 1477, Train Loss: 0.4678, Train auroc: 0.7639, Valid Loss: 0.4678, Valid Auroc: 0.7505\n",
            "Epoch: 1478, Train Loss: 0.4636, Train auroc: 0.7663, Valid Loss: 0.4733, Valid Auroc: 0.7496\n",
            "Epoch: 1479, Train Loss: 0.4678, Train auroc: 0.7638, Valid Loss: 0.4797, Valid Auroc: 0.7460\n",
            "Epoch: 1480, Train Loss: 0.4675, Train auroc: 0.7632, Valid Loss: 0.4761, Valid Auroc: 0.7441\n",
            "Epoch: 1481, Train Loss: 0.4876, Train auroc: 0.7598, Valid Loss: 0.4746, Valid Auroc: 0.7440\n",
            "Epoch: 1482, Train Loss: 0.4828, Train auroc: 0.7541, Valid Loss: 0.4781, Valid Auroc: 0.7441\n",
            "Epoch: 1483, Train Loss: 0.4766, Train auroc: 0.7488, Valid Loss: 0.4743, Valid Auroc: 0.7418\n",
            "Epoch: 1484, Train Loss: 0.4737, Train auroc: 0.7557, Valid Loss: 0.4795, Valid Auroc: 0.7428\n",
            "Epoch: 1485, Train Loss: 0.4728, Train auroc: 0.7547, Valid Loss: 0.4819, Valid Auroc: 0.7383\n",
            "Epoch: 1486, Train Loss: 0.4729, Train auroc: 0.7517, Valid Loss: 0.4769, Valid Auroc: 0.7388\n",
            "Epoch: 1487, Train Loss: 0.4724, Train auroc: 0.7537, Valid Loss: 0.4771, Valid Auroc: 0.7399\n",
            "Epoch: 1488, Train Loss: 0.4694, Train auroc: 0.7564, Valid Loss: 0.4742, Valid Auroc: 0.7400\n",
            "Epoch: 1489, Train Loss: 0.4673, Train auroc: 0.7573, Valid Loss: 0.4724, Valid Auroc: 0.7418\n",
            "Epoch: 1490, Train Loss: 0.4679, Train auroc: 0.7580, Valid Loss: 0.4720, Valid Auroc: 0.7447\n",
            "Epoch: 1491, Train Loss: 0.4649, Train auroc: 0.7607, Valid Loss: 0.4706, Valid Auroc: 0.7466\n",
            "Epoch: 1492, Train Loss: 0.4637, Train auroc: 0.7626, Valid Loss: 0.4669, Valid Auroc: 0.7482\n",
            "Epoch: 1493, Train Loss: 0.4627, Train auroc: 0.7627, Valid Loss: 0.4703, Valid Auroc: 0.7466\n",
            "Epoch: 1494, Train Loss: 0.4609, Train auroc: 0.7644, Valid Loss: 0.4693, Valid Auroc: 0.7474\n",
            "Epoch: 1495, Train Loss: 0.4606, Train auroc: 0.7658, Valid Loss: 0.4664, Valid Auroc: 0.7493\n",
            "Epoch: 1496, Train Loss: 0.4586, Train auroc: 0.7679, Valid Loss: 0.4647, Valid Auroc: 0.7517\n",
            "Epoch: 1497, Train Loss: 0.4582, Train auroc: 0.7681, Valid Loss: 0.4672, Valid Auroc: 0.7517\n",
            "Epoch: 1498, Train Loss: 0.4588, Train auroc: 0.7680, Valid Loss: 0.4656, Valid Auroc: 0.7515\n",
            "Epoch: 1499, Train Loss: 0.4569, Train auroc: 0.7699, Valid Loss: 0.4643, Valid Auroc: 0.7528\n",
            "Epoch: 1500, Train Loss: 0.4591, Train auroc: 0.7699, Valid Loss: 0.4691, Valid Auroc: 0.7529\n",
            "Epoch: 1501, Train Loss: 0.4563, Train auroc: 0.7711, Valid Loss: 0.4626, Valid Auroc: 0.7549\n",
            "Epoch: 1502, Train Loss: 0.4607, Train auroc: 0.7703, Valid Loss: 0.4640, Valid Auroc: 0.7538\n",
            "Epoch: 1503, Train Loss: 0.4578, Train auroc: 0.7706, Valid Loss: 0.4717, Valid Auroc: 0.7511\n",
            "Epoch: 1504, Train Loss: 0.4593, Train auroc: 0.7712, Valid Loss: 0.4725, Valid Auroc: 0.7543\n",
            "Epoch: 1505, Train Loss: 0.4650, Train auroc: 0.7689, Valid Loss: 0.4642, Valid Auroc: 0.7569\n",
            "Epoch: 1506, Train Loss: 0.4635, Train auroc: 0.7697, Valid Loss: 0.4679, Valid Auroc: 0.7540\n",
            "Epoch: 1507, Train Loss: 0.4636, Train auroc: 0.7683, Valid Loss: 0.4777, Valid Auroc: 0.7484\n",
            "Epoch: 1508, Train Loss: 0.4648, Train auroc: 0.7686, Valid Loss: 0.4807, Valid Auroc: 0.7495\n",
            "Epoch: 1509, Train Loss: 0.4750, Train auroc: 0.7653, Valid Loss: 0.4743, Valid Auroc: 0.7517\n",
            "Epoch: 1510, Train Loss: 0.4643, Train auroc: 0.7680, Valid Loss: 0.4672, Valid Auroc: 0.7538\n",
            "Epoch: 1511, Train Loss: 0.4690, Train auroc: 0.7649, Valid Loss: 0.4777, Valid Auroc: 0.7494\n",
            "Epoch: 1512, Train Loss: 0.4637, Train auroc: 0.7677, Valid Loss: 0.4743, Valid Auroc: 0.7499\n",
            "Epoch: 1513, Train Loss: 0.4614, Train auroc: 0.7680, Valid Loss: 0.4651, Valid Auroc: 0.7517\n",
            "Epoch: 1514, Train Loss: 0.4670, Train auroc: 0.7666, Valid Loss: 0.4659, Valid Auroc: 0.7531\n",
            "Epoch: 1515, Train Loss: 0.4594, Train auroc: 0.7681, Valid Loss: 0.4675, Valid Auroc: 0.7523\n",
            "Epoch: 1516, Train Loss: 0.4663, Train auroc: 0.7672, Valid Loss: 0.4808, Valid Auroc: 0.7488\n",
            "Epoch: 1517, Train Loss: 0.4616, Train auroc: 0.7682, Valid Loss: 0.4708, Valid Auroc: 0.7496\n",
            "Epoch: 1518, Train Loss: 0.4696, Train auroc: 0.7658, Valid Loss: 0.4716, Valid Auroc: 0.7511\n",
            "Epoch: 1519, Train Loss: 0.4601, Train auroc: 0.7703, Valid Loss: 0.4636, Valid Auroc: 0.7562\n",
            "Epoch: 1520, Train Loss: 0.4677, Train auroc: 0.7675, Valid Loss: 0.4826, Valid Auroc: 0.7502\n",
            "Epoch: 1521, Train Loss: 0.4635, Train auroc: 0.7677, Valid Loss: 0.4768, Valid Auroc: 0.7477\n",
            "Epoch: 1522, Train Loss: 0.4685, Train auroc: 0.7670, Valid Loss: 0.4753, Valid Auroc: 0.7495\n",
            "Epoch: 1523, Train Loss: 0.4654, Train auroc: 0.7667, Valid Loss: 0.4662, Valid Auroc: 0.7538\n",
            "Epoch: 1524, Train Loss: 0.4716, Train auroc: 0.7642, Valid Loss: 0.4785, Valid Auroc: 0.7501\n",
            "Epoch: 1525, Train Loss: 0.4646, Train auroc: 0.7667, Valid Loss: 0.4767, Valid Auroc: 0.7474\n",
            "Epoch: 1526, Train Loss: 0.4709, Train auroc: 0.7627, Valid Loss: 0.4856, Valid Auroc: 0.7387\n",
            "Epoch: 1527, Train Loss: 0.4714, Train auroc: 0.7662, Valid Loss: 0.4747, Valid Auroc: 0.7501\n",
            "Epoch: 1528, Train Loss: 0.4627, Train auroc: 0.7682, Valid Loss: 0.4653, Valid Auroc: 0.7539\n",
            "Epoch: 1529, Train Loss: 0.4755, Train auroc: 0.7623, Valid Loss: 0.4872, Valid Auroc: 0.7458\n",
            "Epoch: 1530, Train Loss: 0.4659, Train auroc: 0.7639, Valid Loss: 0.4740, Valid Auroc: 0.7472\n",
            "Epoch: 1531, Train Loss: 0.4693, Train auroc: 0.7646, Valid Loss: 0.4763, Valid Auroc: 0.7462\n",
            "Epoch: 1532, Train Loss: 0.4729, Train auroc: 0.7634, Valid Loss: 0.4781, Valid Auroc: 0.7469\n",
            "Epoch: 1533, Train Loss: 0.4646, Train auroc: 0.7661, Valid Loss: 0.4711, Valid Auroc: 0.7517\n",
            "Epoch: 1534, Train Loss: 0.4673, Train auroc: 0.7653, Valid Loss: 0.4748, Valid Auroc: 0.7508\n",
            "Epoch: 1535, Train Loss: 0.4650, Train auroc: 0.7643, Valid Loss: 0.4719, Valid Auroc: 0.7483\n",
            "Epoch: 1536, Train Loss: 0.4708, Train auroc: 0.7619, Valid Loss: 0.4708, Valid Auroc: 0.7465\n",
            "Epoch: 1537, Train Loss: 0.4634, Train auroc: 0.7660, Valid Loss: 0.4665, Valid Auroc: 0.7509\n",
            "Epoch: 1538, Train Loss: 0.4599, Train auroc: 0.7679, Valid Loss: 0.4687, Valid Auroc: 0.7516\n",
            "Epoch: 1539, Train Loss: 0.4600, Train auroc: 0.7683, Valid Loss: 0.4687, Valid Auroc: 0.7527\n",
            "Epoch: 1540, Train Loss: 0.4593, Train auroc: 0.7687, Valid Loss: 0.4644, Valid Auroc: 0.7544\n",
            "Epoch: 1541, Train Loss: 0.4579, Train auroc: 0.7695, Valid Loss: 0.4647, Valid Auroc: 0.7540\n",
            "Epoch: 1542, Train Loss: 0.4599, Train auroc: 0.7689, Valid Loss: 0.4689, Valid Auroc: 0.7515\n",
            "Epoch: 1543, Train Loss: 0.4565, Train auroc: 0.7701, Valid Loss: 0.4635, Valid Auroc: 0.7536\n",
            "Epoch: 1544, Train Loss: 0.4587, Train auroc: 0.7711, Valid Loss: 0.4633, Valid Auroc: 0.7562\n",
            "Epoch: 1545, Train Loss: 0.4552, Train auroc: 0.7729, Valid Loss: 0.4596, Valid Auroc: 0.7579\n",
            "Epoch: 1546, Train Loss: 0.4568, Train auroc: 0.7717, Valid Loss: 0.4661, Valid Auroc: 0.7560\n",
            "Epoch: 1547, Train Loss: 0.4556, Train auroc: 0.7727, Valid Loss: 0.4643, Valid Auroc: 0.7563\n",
            "Epoch: 1548, Train Loss: 0.4548, Train auroc: 0.7741, Valid Loss: 0.4614, Valid Auroc: 0.7577\n",
            "Epoch: 1549, Train Loss: 0.4556, Train auroc: 0.7743, Valid Loss: 0.4618, Valid Auroc: 0.7587\n",
            "Epoch: 1550, Train Loss: 0.4558, Train auroc: 0.7744, Valid Loss: 0.4645, Valid Auroc: 0.7580\n",
            "Epoch: 1551, Train Loss: 0.4531, Train auroc: 0.7752, Valid Loss: 0.4610, Valid Auroc: 0.7581\n",
            "Epoch: 1552, Train Loss: 0.4583, Train auroc: 0.7732, Valid Loss: 0.4651, Valid Auroc: 0.7557\n",
            "Epoch: 1553, Train Loss: 0.4516, Train auroc: 0.7768, Valid Loss: 0.4608, Valid Auroc: 0.7593\n",
            "Epoch: 1554, Train Loss: 0.4596, Train auroc: 0.7744, Valid Loss: 0.4707, Valid Auroc: 0.7580\n",
            "Epoch: 1555, Train Loss: 0.4536, Train auroc: 0.7760, Valid Loss: 0.4580, Valid Auroc: 0.7609\n",
            "Epoch: 1556, Train Loss: 0.4596, Train auroc: 0.7738, Valid Loss: 0.4656, Valid Auroc: 0.7567\n",
            "Epoch: 1557, Train Loss: 0.4554, Train auroc: 0.7746, Valid Loss: 0.4689, Valid Auroc: 0.7542\n",
            "Epoch: 1558, Train Loss: 0.4593, Train auroc: 0.7741, Valid Loss: 0.4766, Valid Auroc: 0.7561\n",
            "Epoch: 1559, Train Loss: 0.4588, Train auroc: 0.7738, Valid Loss: 0.4614, Valid Auroc: 0.7609\n",
            "Epoch: 1560, Train Loss: 0.4616, Train auroc: 0.7732, Valid Loss: 0.4650, Valid Auroc: 0.7588\n",
            "Epoch: 1561, Train Loss: 0.4595, Train auroc: 0.7712, Valid Loss: 0.4727, Valid Auroc: 0.7497\n",
            "Epoch: 1562, Train Loss: 0.4654, Train auroc: 0.7720, Valid Loss: 0.4820, Valid Auroc: 0.7507\n",
            "Epoch: 1563, Train Loss: 0.4551, Train auroc: 0.7744, Valid Loss: 0.4609, Valid Auroc: 0.7592\n",
            "Epoch: 1564, Train Loss: 0.4697, Train auroc: 0.7684, Valid Loss: 0.4686, Valid Auroc: 0.7570\n",
            "Epoch: 1565, Train Loss: 0.4601, Train auroc: 0.7724, Valid Loss: 0.4653, Valid Auroc: 0.7580\n",
            "Epoch: 1566, Train Loss: 0.4637, Train auroc: 0.7700, Valid Loss: 0.4766, Valid Auroc: 0.7499\n",
            "Epoch: 1567, Train Loss: 0.4653, Train auroc: 0.7680, Valid Loss: 0.4787, Valid Auroc: 0.7468\n",
            "Epoch: 1568, Train Loss: 0.4655, Train auroc: 0.7671, Valid Loss: 0.4673, Valid Auroc: 0.7513\n",
            "Epoch: 1569, Train Loss: 0.4660, Train auroc: 0.7699, Valid Loss: 0.4657, Valid Auroc: 0.7561\n",
            "Epoch: 1570, Train Loss: 0.4620, Train auroc: 0.7693, Valid Loss: 0.4679, Valid Auroc: 0.7548\n",
            "Epoch: 1571, Train Loss: 0.4661, Train auroc: 0.7678, Valid Loss: 0.4797, Valid Auroc: 0.7520\n",
            "Epoch: 1572, Train Loss: 0.4621, Train auroc: 0.7695, Valid Loss: 0.4669, Valid Auroc: 0.7549\n",
            "Epoch: 1573, Train Loss: 0.4589, Train auroc: 0.7715, Valid Loss: 0.4620, Valid Auroc: 0.7567\n",
            "Epoch: 1574, Train Loss: 0.4585, Train auroc: 0.7717, Valid Loss: 0.4699, Valid Auroc: 0.7542\n",
            "Epoch: 1575, Train Loss: 0.4570, Train auroc: 0.7733, Valid Loss: 0.4693, Valid Auroc: 0.7550\n",
            "Epoch: 1576, Train Loss: 0.4576, Train auroc: 0.7745, Valid Loss: 0.4643, Valid Auroc: 0.7576\n",
            "Epoch: 1577, Train Loss: 0.4567, Train auroc: 0.7750, Valid Loss: 0.4597, Valid Auroc: 0.7600\n",
            "Epoch: 1578, Train Loss: 0.4572, Train auroc: 0.7744, Valid Loss: 0.4689, Valid Auroc: 0.7571\n",
            "Epoch: 1579, Train Loss: 0.4561, Train auroc: 0.7748, Valid Loss: 0.4695, Valid Auroc: 0.7562\n",
            "Epoch: 1580, Train Loss: 0.4606, Train auroc: 0.7731, Valid Loss: 0.4691, Valid Auroc: 0.7546\n",
            "Epoch: 1581, Train Loss: 0.4584, Train auroc: 0.7752, Valid Loss: 0.4632, Valid Auroc: 0.7598\n",
            "Epoch: 1582, Train Loss: 0.4547, Train auroc: 0.7758, Valid Loss: 0.4624, Valid Auroc: 0.7605\n",
            "Epoch: 1583, Train Loss: 0.4612, Train auroc: 0.7737, Valid Loss: 0.4767, Valid Auroc: 0.7558\n",
            "Epoch: 1584, Train Loss: 0.4542, Train auroc: 0.7762, Valid Loss: 0.4624, Valid Auroc: 0.7584\n",
            "Epoch: 1585, Train Loss: 0.4646, Train auroc: 0.7713, Valid Loss: 0.4709, Valid Auroc: 0.7539\n",
            "Epoch: 1586, Train Loss: 0.4551, Train auroc: 0.7753, Valid Loss: 0.4635, Valid Auroc: 0.7577\n",
            "Epoch: 1587, Train Loss: 0.4653, Train auroc: 0.7718, Valid Loss: 0.4776, Valid Auroc: 0.7539\n",
            "Epoch: 1588, Train Loss: 0.4569, Train auroc: 0.7735, Valid Loss: 0.4655, Valid Auroc: 0.7563\n",
            "Epoch: 1589, Train Loss: 0.4628, Train auroc: 0.7728, Valid Loss: 0.4664, Valid Auroc: 0.7566\n",
            "Epoch: 1590, Train Loss: 0.4621, Train auroc: 0.7733, Valid Loss: 0.4689, Valid Auroc: 0.7558\n",
            "Epoch: 1591, Train Loss: 0.4572, Train auroc: 0.7752, Valid Loss: 0.4709, Valid Auroc: 0.7561\n",
            "Epoch: 1592, Train Loss: 0.4651, Train auroc: 0.7708, Valid Loss: 0.4773, Valid Auroc: 0.7530\n",
            "Epoch: 1593, Train Loss: 0.4591, Train auroc: 0.7727, Valid Loss: 0.4634, Valid Auroc: 0.7568\n",
            "Epoch: 1594, Train Loss: 0.4605, Train auroc: 0.7719, Valid Loss: 0.4641, Valid Auroc: 0.7557\n",
            "Epoch: 1595, Train Loss: 0.4633, Train auroc: 0.7703, Valid Loss: 0.4739, Valid Auroc: 0.7512\n",
            "Epoch: 1596, Train Loss: 0.4622, Train auroc: 0.7714, Valid Loss: 0.4781, Valid Auroc: 0.7512\n",
            "Epoch: 1597, Train Loss: 0.4544, Train auroc: 0.7748, Valid Loss: 0.4618, Valid Auroc: 0.7586\n",
            "Epoch: 1598, Train Loss: 0.4673, Train auroc: 0.7711, Valid Loss: 0.4685, Valid Auroc: 0.7580\n",
            "Epoch: 1599, Train Loss: 0.4552, Train auroc: 0.7757, Valid Loss: 0.4599, Valid Auroc: 0.7603\n",
            "Epoch: 1600, Train Loss: 0.4607, Train auroc: 0.7724, Valid Loss: 0.4758, Valid Auroc: 0.7522\n",
            "Epoch: 1601, Train Loss: 0.4609, Train auroc: 0.7724, Valid Loss: 0.4754, Valid Auroc: 0.7524\n",
            "Epoch: 1602, Train Loss: 0.4576, Train auroc: 0.7740, Valid Loss: 0.4613, Valid Auroc: 0.7599\n",
            "Epoch: 1603, Train Loss: 0.4613, Train auroc: 0.7724, Valid Loss: 0.4619, Valid Auroc: 0.7594\n",
            "Epoch: 1604, Train Loss: 0.4593, Train auroc: 0.7730, Valid Loss: 0.4683, Valid Auroc: 0.7563\n",
            "Epoch: 1605, Train Loss: 0.4585, Train auroc: 0.7737, Valid Loss: 0.4739, Valid Auroc: 0.7533\n",
            "Epoch: 1606, Train Loss: 0.4602, Train auroc: 0.7727, Valid Loss: 0.4724, Valid Auroc: 0.7511\n",
            "Epoch: 1607, Train Loss: 0.4612, Train auroc: 0.7741, Valid Loss: 0.4647, Valid Auroc: 0.7586\n",
            "Epoch: 1608, Train Loss: 0.4543, Train auroc: 0.7751, Valid Loss: 0.4601, Valid Auroc: 0.7601\n",
            "Epoch: 1609, Train Loss: 0.4666, Train auroc: 0.7713, Valid Loss: 0.4827, Valid Auroc: 0.7531\n",
            "Epoch: 1610, Train Loss: 0.4547, Train auroc: 0.7756, Valid Loss: 0.4642, Valid Auroc: 0.7571\n",
            "Epoch: 1611, Train Loss: 0.4618, Train auroc: 0.7723, Valid Loss: 0.4684, Valid Auroc: 0.7542\n",
            "Epoch: 1612, Train Loss: 0.4619, Train auroc: 0.7730, Valid Loss: 0.4668, Valid Auroc: 0.7566\n",
            "Epoch: 1613, Train Loss: 0.4580, Train auroc: 0.7750, Valid Loss: 0.4709, Valid Auroc: 0.7568\n",
            "Epoch: 1614, Train Loss: 0.4599, Train auroc: 0.7724, Valid Loss: 0.4714, Valid Auroc: 0.7549\n",
            "Epoch: 1615, Train Loss: 0.4624, Train auroc: 0.7727, Valid Loss: 0.4644, Valid Auroc: 0.7591\n",
            "Epoch: 1616, Train Loss: 0.4551, Train auroc: 0.7757, Valid Loss: 0.4585, Valid Auroc: 0.7607\n",
            "Epoch: 1617, Train Loss: 0.4543, Train auroc: 0.7748, Valid Loss: 0.4646, Valid Auroc: 0.7564\n",
            "Epoch: 1618, Train Loss: 0.4564, Train auroc: 0.7732, Valid Loss: 0.4724, Valid Auroc: 0.7520\n",
            "Epoch: 1619, Train Loss: 0.4528, Train auroc: 0.7772, Valid Loss: 0.4621, Valid Auroc: 0.7587\n",
            "Epoch: 1620, Train Loss: 0.4539, Train auroc: 0.7768, Valid Loss: 0.4572, Valid Auroc: 0.7622\n",
            "Epoch: 1621, Train Loss: 0.4548, Train auroc: 0.7766, Valid Loss: 0.4623, Valid Auroc: 0.7607\n",
            "Epoch: 1622, Train Loss: 0.4526, Train auroc: 0.7787, Valid Loss: 0.4654, Valid Auroc: 0.7596\n",
            "Epoch: 1623, Train Loss: 0.4539, Train auroc: 0.7763, Valid Loss: 0.4663, Valid Auroc: 0.7556\n",
            "Epoch: 1624, Train Loss: 0.4547, Train auroc: 0.7770, Valid Loss: 0.4614, Valid Auroc: 0.7597\n",
            "Epoch: 1625, Train Loss: 0.4543, Train auroc: 0.7781, Valid Loss: 0.4631, Valid Auroc: 0.7615\n",
            "Epoch: 1626, Train Loss: 0.4533, Train auroc: 0.7784, Valid Loss: 0.4627, Valid Auroc: 0.7617\n",
            "Epoch: 1627, Train Loss: 0.4590, Train auroc: 0.7762, Valid Loss: 0.4695, Valid Auroc: 0.7581\n",
            "Epoch: 1628, Train Loss: 0.4505, Train auroc: 0.7798, Valid Loss: 0.4582, Valid Auroc: 0.7627\n",
            "Epoch: 1629, Train Loss: 0.4604, Train auroc: 0.7740, Valid Loss: 0.4737, Valid Auroc: 0.7570\n",
            "Epoch: 1630, Train Loss: 0.4543, Train auroc: 0.7777, Valid Loss: 0.4663, Valid Auroc: 0.7603\n",
            "Epoch: 1631, Train Loss: 0.4550, Train auroc: 0.7768, Valid Loss: 0.4635, Valid Auroc: 0.7597\n",
            "Epoch: 1632, Train Loss: 0.4628, Train auroc: 0.7734, Valid Loss: 0.4725, Valid Auroc: 0.7560\n",
            "Epoch: 1633, Train Loss: 0.4509, Train auroc: 0.7792, Valid Loss: 0.4585, Valid Auroc: 0.7633\n",
            "Epoch: 1634, Train Loss: 0.4596, Train auroc: 0.7749, Valid Loss: 0.4740, Valid Auroc: 0.7583\n",
            "Epoch: 1635, Train Loss: 0.4574, Train auroc: 0.7738, Valid Loss: 0.4652, Valid Auroc: 0.7587\n",
            "Epoch: 1636, Train Loss: 0.4536, Train auroc: 0.7775, Valid Loss: 0.4611, Valid Auroc: 0.7604\n",
            "Epoch: 1637, Train Loss: 0.4579, Train auroc: 0.7740, Valid Loss: 0.4702, Valid Auroc: 0.7543\n",
            "Epoch: 1638, Train Loss: 0.4548, Train auroc: 0.7755, Valid Loss: 0.4631, Valid Auroc: 0.7590\n",
            "Epoch: 1639, Train Loss: 0.4539, Train auroc: 0.7783, Valid Loss: 0.4576, Valid Auroc: 0.7637\n",
            "Epoch: 1640, Train Loss: 0.4506, Train auroc: 0.7785, Valid Loss: 0.4612, Valid Auroc: 0.7612\n",
            "Epoch: 1641, Train Loss: 0.4525, Train auroc: 0.7775, Valid Loss: 0.4670, Valid Auroc: 0.7588\n",
            "Epoch: 1642, Train Loss: 0.4509, Train auroc: 0.7789, Valid Loss: 0.4582, Valid Auroc: 0.7616\n",
            "Epoch: 1643, Train Loss: 0.4503, Train auroc: 0.7803, Valid Loss: 0.4563, Valid Auroc: 0.7638\n",
            "Epoch: 1644, Train Loss: 0.4522, Train auroc: 0.7793, Valid Loss: 0.4628, Valid Auroc: 0.7618\n",
            "Epoch: 1645, Train Loss: 0.4503, Train auroc: 0.7806, Valid Loss: 0.4640, Valid Auroc: 0.7611\n",
            "Epoch: 1646, Train Loss: 0.4535, Train auroc: 0.7802, Valid Loss: 0.4597, Valid Auroc: 0.7617\n",
            "Epoch: 1647, Train Loss: 0.4479, Train auroc: 0.7818, Valid Loss: 0.4557, Valid Auroc: 0.7637\n",
            "Epoch: 1648, Train Loss: 0.4543, Train auroc: 0.7806, Valid Loss: 0.4735, Valid Auroc: 0.7604\n",
            "Epoch: 1649, Train Loss: 0.4476, Train auroc: 0.7825, Valid Loss: 0.4569, Valid Auroc: 0.7653\n",
            "Epoch: 1650, Train Loss: 0.4581, Train auroc: 0.7786, Valid Loss: 0.4649, Valid Auroc: 0.7612\n",
            "Epoch: 1651, Train Loss: 0.4491, Train auroc: 0.7819, Valid Loss: 0.4593, Valid Auroc: 0.7621\n",
            "Epoch: 1652, Train Loss: 0.4606, Train auroc: 0.7785, Valid Loss: 0.4809, Valid Auroc: 0.7577\n",
            "Epoch: 1653, Train Loss: 0.4496, Train auroc: 0.7820, Valid Loss: 0.4602, Valid Auroc: 0.7638\n",
            "Epoch: 1654, Train Loss: 0.4635, Train auroc: 0.7775, Valid Loss: 0.4659, Valid Auroc: 0.7628\n",
            "Epoch: 1655, Train Loss: 0.4575, Train auroc: 0.7787, Valid Loss: 0.4665, Valid Auroc: 0.7604\n",
            "Epoch: 1656, Train Loss: 0.4589, Train auroc: 0.7774, Valid Loss: 0.4752, Valid Auroc: 0.7579\n",
            "Epoch: 1657, Train Loss: 0.4598, Train auroc: 0.7779, Valid Loss: 0.4753, Valid Auroc: 0.7587\n",
            "Epoch: 1658, Train Loss: 0.4652, Train auroc: 0.7758, Valid Loss: 0.4648, Valid Auroc: 0.7609\n",
            "Epoch: 1659, Train Loss: 0.4569, Train auroc: 0.7784, Valid Loss: 0.4633, Valid Auroc: 0.7609\n",
            "Epoch: 1660, Train Loss: 0.4597, Train auroc: 0.7771, Valid Loss: 0.4782, Valid Auroc: 0.7573\n",
            "Epoch: 1661, Train Loss: 0.4570, Train auroc: 0.7779, Valid Loss: 0.4715, Valid Auroc: 0.7606\n",
            "Epoch: 1662, Train Loss: 0.4582, Train auroc: 0.7759, Valid Loss: 0.4607, Valid Auroc: 0.7620\n",
            "Epoch: 1663, Train Loss: 0.4631, Train auroc: 0.7742, Valid Loss: 0.4710, Valid Auroc: 0.7552\n",
            "Epoch: 1664, Train Loss: 0.4545, Train auroc: 0.7764, Valid Loss: 0.4679, Valid Auroc: 0.7549\n",
            "Epoch: 1665, Train Loss: 0.4652, Train auroc: 0.7735, Valid Loss: 0.4838, Valid Auroc: 0.7535\n",
            "Epoch: 1666, Train Loss: 0.4558, Train auroc: 0.7759, Valid Loss: 0.4612, Valid Auroc: 0.7610\n",
            "Epoch: 1667, Train Loss: 0.4626, Train auroc: 0.7749, Valid Loss: 0.4616, Valid Auroc: 0.7630\n",
            "Epoch: 1668, Train Loss: 0.4609, Train auroc: 0.7750, Valid Loss: 0.4717, Valid Auroc: 0.7548\n",
            "Epoch: 1669, Train Loss: 0.4594, Train auroc: 0.7737, Valid Loss: 0.4759, Valid Auroc: 0.7512\n",
            "Epoch: 1670, Train Loss: 0.4610, Train auroc: 0.7749, Valid Loss: 0.4740, Valid Auroc: 0.7567\n",
            "Epoch: 1671, Train Loss: 0.4597, Train auroc: 0.7739, Valid Loss: 0.4619, Valid Auroc: 0.7599\n",
            "Epoch: 1672, Train Loss: 0.4623, Train auroc: 0.7728, Valid Loss: 0.4625, Valid Auroc: 0.7603\n",
            "Epoch: 1673, Train Loss: 0.4606, Train auroc: 0.7734, Valid Loss: 0.4712, Valid Auroc: 0.7542\n",
            "Epoch: 1674, Train Loss: 0.4619, Train auroc: 0.7726, Valid Loss: 0.4790, Valid Auroc: 0.7488\n",
            "Epoch: 1675, Train Loss: 0.4579, Train auroc: 0.7736, Valid Loss: 0.4663, Valid Auroc: 0.7553\n",
            "Epoch: 1676, Train Loss: 0.4642, Train auroc: 0.7716, Valid Loss: 0.4654, Valid Auroc: 0.7580\n",
            "Epoch: 1677, Train Loss: 0.4517, Train auroc: 0.7793, Valid Loss: 0.4571, Valid Auroc: 0.7638\n",
            "Epoch: 1678, Train Loss: 0.4566, Train auroc: 0.7745, Valid Loss: 0.4665, Valid Auroc: 0.7576\n",
            "Epoch: 1679, Train Loss: 0.4577, Train auroc: 0.7738, Valid Loss: 0.4699, Valid Auroc: 0.7552\n",
            "Epoch: 1680, Train Loss: 0.4512, Train auroc: 0.7787, Valid Loss: 0.4573, Valid Auroc: 0.7621\n",
            "Epoch: 1681, Train Loss: 0.4545, Train auroc: 0.7782, Valid Loss: 0.4577, Valid Auroc: 0.7628\n",
            "Epoch: 1682, Train Loss: 0.4535, Train auroc: 0.7786, Valid Loss: 0.4603, Valid Auroc: 0.7618\n",
            "Epoch: 1683, Train Loss: 0.4521, Train auroc: 0.7791, Valid Loss: 0.4657, Valid Auroc: 0.7582\n",
            "Epoch: 1684, Train Loss: 0.4534, Train auroc: 0.7783, Valid Loss: 0.4657, Valid Auroc: 0.7575\n",
            "Epoch: 1685, Train Loss: 0.4530, Train auroc: 0.7786, Valid Loss: 0.4576, Valid Auroc: 0.7629\n",
            "Epoch: 1686, Train Loss: 0.4503, Train auroc: 0.7803, Valid Loss: 0.4569, Valid Auroc: 0.7640\n",
            "Epoch: 1687, Train Loss: 0.4533, Train auroc: 0.7803, Valid Loss: 0.4655, Valid Auroc: 0.7613\n",
            "Epoch: 1688, Train Loss: 0.4501, Train auroc: 0.7793, Valid Loss: 0.4615, Valid Auroc: 0.7590\n",
            "Epoch: 1689, Train Loss: 0.4530, Train auroc: 0.7792, Valid Loss: 0.4625, Valid Auroc: 0.7594\n",
            "Epoch: 1690, Train Loss: 0.4513, Train auroc: 0.7811, Valid Loss: 0.4599, Valid Auroc: 0.7637\n",
            "Epoch: 1691, Train Loss: 0.4510, Train auroc: 0.7803, Valid Loss: 0.4615, Valid Auroc: 0.7628\n",
            "Epoch: 1692, Train Loss: 0.4517, Train auroc: 0.7808, Valid Loss: 0.4604, Valid Auroc: 0.7633\n",
            "Epoch: 1693, Train Loss: 0.4509, Train auroc: 0.7815, Valid Loss: 0.4617, Valid Auroc: 0.7623\n",
            "Epoch: 1694, Train Loss: 0.4498, Train auroc: 0.7810, Valid Loss: 0.4619, Valid Auroc: 0.7624\n",
            "Epoch: 1695, Train Loss: 0.4548, Train auroc: 0.7796, Valid Loss: 0.4700, Valid Auroc: 0.7606\n",
            "Epoch: 1696, Train Loss: 0.4470, Train auroc: 0.7834, Valid Loss: 0.4552, Valid Auroc: 0.7658\n",
            "Epoch: 1697, Train Loss: 0.4542, Train auroc: 0.7807, Valid Loss: 0.4615, Valid Auroc: 0.7637\n",
            "Epoch: 1698, Train Loss: 0.4496, Train auroc: 0.7822, Valid Loss: 0.4590, Valid Auroc: 0.7645\n",
            "Epoch: 1699, Train Loss: 0.4506, Train auroc: 0.7824, Valid Loss: 0.4664, Valid Auroc: 0.7633\n",
            "Epoch: 1700, Train Loss: 0.4508, Train auroc: 0.7815, Valid Loss: 0.4600, Valid Auroc: 0.7638\n",
            "Epoch: 1701, Train Loss: 0.4526, Train auroc: 0.7817, Valid Loss: 0.4597, Valid Auroc: 0.7641\n",
            "Epoch: 1702, Train Loss: 0.4495, Train auroc: 0.7827, Valid Loss: 0.4615, Valid Auroc: 0.7628\n",
            "Epoch: 1703, Train Loss: 0.4566, Train auroc: 0.7806, Valid Loss: 0.4699, Valid Auroc: 0.7621\n",
            "Epoch: 1704, Train Loss: 0.4487, Train auroc: 0.7827, Valid Loss: 0.4546, Valid Auroc: 0.7669\n",
            "Epoch: 1705, Train Loss: 0.4564, Train auroc: 0.7797, Valid Loss: 0.4605, Valid Auroc: 0.7635\n",
            "Epoch: 1706, Train Loss: 0.4507, Train auroc: 0.7805, Valid Loss: 0.4651, Valid Auroc: 0.7586\n",
            "Epoch: 1707, Train Loss: 0.4576, Train auroc: 0.7784, Valid Loss: 0.4765, Valid Auroc: 0.7553\n",
            "Epoch: 1708, Train Loss: 0.4531, Train auroc: 0.7806, Valid Loss: 0.4621, Valid Auroc: 0.7618\n",
            "Epoch: 1709, Train Loss: 0.4568, Train auroc: 0.7814, Valid Loss: 0.4602, Valid Auroc: 0.7653\n",
            "Epoch: 1710, Train Loss: 0.4515, Train auroc: 0.7805, Valid Loss: 0.4571, Valid Auroc: 0.7641\n",
            "Epoch: 1711, Train Loss: 0.4608, Train auroc: 0.7773, Valid Loss: 0.4821, Valid Auroc: 0.7570\n",
            "Epoch: 1712, Train Loss: 0.4510, Train auroc: 0.7813, Valid Loss: 0.4630, Valid Auroc: 0.7623\n",
            "Epoch: 1713, Train Loss: 0.4603, Train auroc: 0.7757, Valid Loss: 0.4671, Valid Auroc: 0.7588\n",
            "Epoch: 1714, Train Loss: 0.4621, Train auroc: 0.7760, Valid Loss: 0.4664, Valid Auroc: 0.7604\n",
            "Epoch: 1715, Train Loss: 0.4530, Train auroc: 0.7803, Valid Loss: 0.4616, Valid Auroc: 0.7632\n",
            "Epoch: 1716, Train Loss: 0.4569, Train auroc: 0.7784, Valid Loss: 0.4716, Valid Auroc: 0.7586\n",
            "Epoch: 1717, Train Loss: 0.4606, Train auroc: 0.7729, Valid Loss: 0.4665, Valid Auroc: 0.7557\n",
            "Epoch: 1718, Train Loss: 0.4498, Train auroc: 0.7803, Valid Loss: 0.4595, Valid Auroc: 0.7625\n",
            "Epoch: 1719, Train Loss: 0.4556, Train auroc: 0.7786, Valid Loss: 0.4698, Valid Auroc: 0.7595\n",
            "Epoch: 1720, Train Loss: 0.4519, Train auroc: 0.7796, Valid Loss: 0.4603, Valid Auroc: 0.7618\n",
            "Epoch: 1721, Train Loss: 0.4573, Train auroc: 0.7787, Valid Loss: 0.4653, Valid Auroc: 0.7610\n",
            "Epoch: 1722, Train Loss: 0.4481, Train auroc: 0.7818, Valid Loss: 0.4558, Valid Auroc: 0.7658\n",
            "Epoch: 1723, Train Loss: 0.4584, Train auroc: 0.7786, Valid Loss: 0.4728, Valid Auroc: 0.7617\n",
            "Epoch: 1724, Train Loss: 0.4479, Train auroc: 0.7816, Valid Loss: 0.4548, Valid Auroc: 0.7659\n",
            "Epoch: 1725, Train Loss: 0.4586, Train auroc: 0.7779, Valid Loss: 0.4657, Valid Auroc: 0.7610\n",
            "Epoch: 1726, Train Loss: 0.4503, Train auroc: 0.7810, Valid Loss: 0.4604, Valid Auroc: 0.7612\n",
            "Epoch: 1727, Train Loss: 0.4570, Train auroc: 0.7790, Valid Loss: 0.4694, Valid Auroc: 0.7602\n",
            "Epoch: 1728, Train Loss: 0.4534, Train auroc: 0.7804, Valid Loss: 0.4636, Valid Auroc: 0.7625\n",
            "Epoch: 1729, Train Loss: 0.4521, Train auroc: 0.7822, Valid Loss: 0.4568, Valid Auroc: 0.7660\n",
            "Epoch: 1730, Train Loss: 0.4587, Train auroc: 0.7788, Valid Loss: 0.4673, Valid Auroc: 0.7605\n",
            "Epoch: 1731, Train Loss: 0.4495, Train auroc: 0.7819, Valid Loss: 0.4620, Valid Auroc: 0.7627\n",
            "Epoch: 1732, Train Loss: 0.4589, Train auroc: 0.7801, Valid Loss: 0.4720, Valid Auroc: 0.7614\n",
            "Epoch: 1733, Train Loss: 0.4530, Train auroc: 0.7801, Valid Loss: 0.4571, Valid Auroc: 0.7643\n",
            "Epoch: 1734, Train Loss: 0.4552, Train auroc: 0.7805, Valid Loss: 0.4640, Valid Auroc: 0.7613\n",
            "Epoch: 1735, Train Loss: 0.4546, Train auroc: 0.7791, Valid Loss: 0.4685, Valid Auroc: 0.7574\n",
            "Epoch: 1736, Train Loss: 0.4516, Train auroc: 0.7813, Valid Loss: 0.4649, Valid Auroc: 0.7626\n",
            "Epoch: 1737, Train Loss: 0.4549, Train auroc: 0.7790, Valid Loss: 0.4612, Valid Auroc: 0.7632\n",
            "Epoch: 1738, Train Loss: 0.4528, Train auroc: 0.7794, Valid Loss: 0.4562, Valid Auroc: 0.7650\n",
            "Epoch: 1739, Train Loss: 0.4520, Train auroc: 0.7807, Valid Loss: 0.4632, Valid Auroc: 0.7613\n",
            "Epoch: 1740, Train Loss: 0.4568, Train auroc: 0.7778, Valid Loss: 0.4728, Valid Auroc: 0.7552\n",
            "Epoch: 1741, Train Loss: 0.4488, Train auroc: 0.7833, Valid Loss: 0.4579, Valid Auroc: 0.7643\n",
            "Epoch: 1742, Train Loss: 0.4532, Train auroc: 0.7820, Valid Loss: 0.4575, Valid Auroc: 0.7654\n",
            "Epoch: 1743, Train Loss: 0.4484, Train auroc: 0.7824, Valid Loss: 0.4582, Valid Auroc: 0.7642\n",
            "Epoch: 1744, Train Loss: 0.4500, Train auroc: 0.7832, Valid Loss: 0.4661, Valid Auroc: 0.7629\n",
            "Epoch: 1745, Train Loss: 0.4499, Train auroc: 0.7833, Valid Loss: 0.4624, Valid Auroc: 0.7638\n",
            "Epoch: 1746, Train Loss: 0.4484, Train auroc: 0.7842, Valid Loss: 0.4550, Valid Auroc: 0.7671\n",
            "Epoch: 1747, Train Loss: 0.4511, Train auroc: 0.7830, Valid Loss: 0.4577, Valid Auroc: 0.7655\n",
            "Epoch: 1748, Train Loss: 0.4485, Train auroc: 0.7845, Valid Loss: 0.4642, Valid Auroc: 0.7639\n",
            "Epoch: 1749, Train Loss: 0.4489, Train auroc: 0.7837, Valid Loss: 0.4616, Valid Auroc: 0.7634\n",
            "Epoch: 1750, Train Loss: 0.4516, Train auroc: 0.7828, Valid Loss: 0.4608, Valid Auroc: 0.7642\n",
            "Epoch: 1751, Train Loss: 0.4468, Train auroc: 0.7851, Valid Loss: 0.4566, Valid Auroc: 0.7667\n",
            "Epoch: 1752, Train Loss: 0.4549, Train auroc: 0.7808, Valid Loss: 0.4710, Valid Auroc: 0.7609\n",
            "Epoch: 1753, Train Loss: 0.4469, Train auroc: 0.7852, Valid Loss: 0.4567, Valid Auroc: 0.7654\n",
            "Epoch: 1754, Train Loss: 0.4546, Train auroc: 0.7814, Valid Loss: 0.4675, Valid Auroc: 0.7594\n",
            "Epoch: 1755, Train Loss: 0.4485, Train auroc: 0.7841, Valid Loss: 0.4603, Valid Auroc: 0.7644\n",
            "Epoch: 1756, Train Loss: 0.4530, Train auroc: 0.7823, Valid Loss: 0.4674, Valid Auroc: 0.7641\n",
            "Epoch: 1757, Train Loss: 0.4519, Train auroc: 0.7806, Valid Loss: 0.4588, Valid Auroc: 0.7643\n",
            "Epoch: 1758, Train Loss: 0.4510, Train auroc: 0.7831, Valid Loss: 0.4560, Valid Auroc: 0.7668\n",
            "Epoch: 1759, Train Loss: 0.4533, Train auroc: 0.7798, Valid Loss: 0.4707, Valid Auroc: 0.7558\n",
            "Epoch: 1760, Train Loss: 0.4518, Train auroc: 0.7807, Valid Loss: 0.4675, Valid Auroc: 0.7571\n",
            "Epoch: 1761, Train Loss: 0.4466, Train auroc: 0.7858, Valid Loss: 0.4526, Valid Auroc: 0.7692\n",
            "Epoch: 1762, Train Loss: 0.4522, Train auroc: 0.7816, Valid Loss: 0.4546, Valid Auroc: 0.7671\n",
            "Epoch: 1763, Train Loss: 0.4464, Train auroc: 0.7843, Valid Loss: 0.4603, Valid Auroc: 0.7646\n",
            "Epoch: 1764, Train Loss: 0.4488, Train auroc: 0.7838, Valid Loss: 0.4669, Valid Auroc: 0.7607\n",
            "Epoch: 1765, Train Loss: 0.4521, Train auroc: 0.7827, Valid Loss: 0.4630, Valid Auroc: 0.7622\n",
            "Epoch: 1766, Train Loss: 0.4452, Train auroc: 0.7864, Valid Loss: 0.4535, Valid Auroc: 0.7692\n",
            "Epoch: 1767, Train Loss: 0.4506, Train auroc: 0.7847, Valid Loss: 0.4633, Valid Auroc: 0.7671\n",
            "Epoch: 1768, Train Loss: 0.4451, Train auroc: 0.7854, Valid Loss: 0.4532, Valid Auroc: 0.7686\n",
            "Epoch: 1769, Train Loss: 0.4506, Train auroc: 0.7842, Valid Loss: 0.4584, Valid Auroc: 0.7663\n",
            "Epoch: 1770, Train Loss: 0.4457, Train auroc: 0.7860, Valid Loss: 0.4604, Valid Auroc: 0.7650\n",
            "Epoch: 1771, Train Loss: 0.4473, Train auroc: 0.7850, Valid Loss: 0.4606, Valid Auroc: 0.7646\n",
            "Epoch: 1772, Train Loss: 0.4455, Train auroc: 0.7874, Valid Loss: 0.4529, Valid Auroc: 0.7689\n",
            "Epoch: 1773, Train Loss: 0.4427, Train auroc: 0.7883, Valid Loss: 0.4521, Valid Auroc: 0.7705\n",
            "Epoch: 1774, Train Loss: 0.4441, Train auroc: 0.7869, Valid Loss: 0.4576, Valid Auroc: 0.7684\n",
            "Epoch: 1775, Train Loss: 0.4429, Train auroc: 0.7875, Valid Loss: 0.4518, Valid Auroc: 0.7691\n",
            "Epoch: 1776, Train Loss: 0.4415, Train auroc: 0.7892, Valid Loss: 0.4516, Valid Auroc: 0.7695\n",
            "Epoch: 1777, Train Loss: 0.4432, Train auroc: 0.7889, Valid Loss: 0.4568, Valid Auroc: 0.7678\n",
            "Epoch: 1778, Train Loss: 0.4416, Train auroc: 0.7898, Valid Loss: 0.4541, Valid Auroc: 0.7692\n",
            "Epoch: 1779, Train Loss: 0.4432, Train auroc: 0.7894, Valid Loss: 0.4516, Valid Auroc: 0.7705\n",
            "Epoch: 1780, Train Loss: 0.4407, Train auroc: 0.7899, Valid Loss: 0.4520, Valid Auroc: 0.7707\n",
            "Epoch: 1781, Train Loss: 0.4456, Train auroc: 0.7889, Valid Loss: 0.4652, Valid Auroc: 0.7677\n",
            "Epoch: 1782, Train Loss: 0.4409, Train auroc: 0.7907, Valid Loss: 0.4513, Valid Auroc: 0.7708\n",
            "Epoch: 1783, Train Loss: 0.4485, Train auroc: 0.7880, Valid Loss: 0.4569, Valid Auroc: 0.7692\n",
            "Epoch: 1784, Train Loss: 0.4431, Train auroc: 0.7903, Valid Loss: 0.4572, Valid Auroc: 0.7702\n",
            "Epoch: 1785, Train Loss: 0.4493, Train auroc: 0.7880, Valid Loss: 0.4693, Valid Auroc: 0.7667\n",
            "Epoch: 1786, Train Loss: 0.4483, Train auroc: 0.7875, Valid Loss: 0.4547, Valid Auroc: 0.7702\n",
            "Epoch: 1787, Train Loss: 0.4520, Train auroc: 0.7861, Valid Loss: 0.4587, Valid Auroc: 0.7687\n",
            "Epoch: 1788, Train Loss: 0.4512, Train auroc: 0.7861, Valid Loss: 0.4679, Valid Auroc: 0.7650\n",
            "Epoch: 1789, Train Loss: 0.4585, Train auroc: 0.7826, Valid Loss: 0.4795, Valid Auroc: 0.7604\n",
            "Epoch: 1790, Train Loss: 0.4524, Train auroc: 0.7846, Valid Loss: 0.4596, Valid Auroc: 0.7653\n",
            "Epoch: 1791, Train Loss: 0.4689, Train auroc: 0.7788, Valid Loss: 0.4722, Valid Auroc: 0.7623\n",
            "Epoch: 1792, Train Loss: 0.4469, Train auroc: 0.7851, Valid Loss: 0.4568, Valid Auroc: 0.7682\n",
            "Epoch: 1793, Train Loss: 0.4738, Train auroc: 0.7779, Valid Loss: 0.4991, Valid Auroc: 0.7575\n",
            "Epoch: 1794, Train Loss: 0.4508, Train auroc: 0.7828, Valid Loss: 0.4593, Valid Auroc: 0.7652\n",
            "Epoch: 1795, Train Loss: 0.4710, Train auroc: 0.7721, Valid Loss: 0.4746, Valid Auroc: 0.7565\n",
            "Epoch: 1796, Train Loss: 0.4707, Train auroc: 0.7704, Valid Loss: 0.4786, Valid Auroc: 0.7517\n",
            "Epoch: 1797, Train Loss: 0.4552, Train auroc: 0.7764, Valid Loss: 0.4706, Valid Auroc: 0.7553\n",
            "Epoch: 1798, Train Loss: 0.4670, Train auroc: 0.7723, Valid Loss: 0.4817, Valid Auroc: 0.7539\n",
            "Epoch: 1799, Train Loss: 0.4623, Train auroc: 0.7727, Valid Loss: 0.4707, Valid Auroc: 0.7556\n",
            "Epoch: 1800, Train Loss: 0.4566, Train auroc: 0.7769, Valid Loss: 0.4601, Valid Auroc: 0.7613\n",
            "Epoch: 1801, Train Loss: 0.4615, Train auroc: 0.7751, Valid Loss: 0.4701, Valid Auroc: 0.7569\n",
            "Epoch: 1802, Train Loss: 0.4595, Train auroc: 0.7734, Valid Loss: 0.4751, Valid Auroc: 0.7507\n",
            "Epoch: 1803, Train Loss: 0.4553, Train auroc: 0.7742, Valid Loss: 0.4674, Valid Auroc: 0.7540\n",
            "Epoch: 1804, Train Loss: 0.4599, Train auroc: 0.7743, Valid Loss: 0.4651, Valid Auroc: 0.7577\n",
            "Epoch: 1805, Train Loss: 0.4533, Train auroc: 0.7791, Valid Loss: 0.4587, Valid Auroc: 0.7628\n",
            "Epoch: 1806, Train Loss: 0.4514, Train auroc: 0.7792, Valid Loss: 0.4611, Valid Auroc: 0.7617\n",
            "Epoch: 1807, Train Loss: 0.4569, Train auroc: 0.7766, Valid Loss: 0.4696, Valid Auroc: 0.7574\n",
            "Epoch: 1808, Train Loss: 0.4496, Train auroc: 0.7802, Valid Loss: 0.4592, Valid Auroc: 0.7613\n",
            "Epoch: 1809, Train Loss: 0.4518, Train auroc: 0.7805, Valid Loss: 0.4573, Valid Auroc: 0.7632\n",
            "Epoch: 1810, Train Loss: 0.4515, Train auroc: 0.7818, Valid Loss: 0.4598, Valid Auroc: 0.7639\n",
            "Epoch: 1811, Train Loss: 0.4478, Train auroc: 0.7840, Valid Loss: 0.4599, Valid Auroc: 0.7647\n",
            "Epoch: 1812, Train Loss: 0.4492, Train auroc: 0.7834, Valid Loss: 0.4616, Valid Auroc: 0.7628\n",
            "Epoch: 1813, Train Loss: 0.4495, Train auroc: 0.7837, Valid Loss: 0.4582, Valid Auroc: 0.7648\n",
            "Epoch: 1814, Train Loss: 0.4456, Train auroc: 0.7847, Valid Loss: 0.4569, Valid Auroc: 0.7650\n",
            "Epoch: 1815, Train Loss: 0.4512, Train auroc: 0.7834, Valid Loss: 0.4663, Valid Auroc: 0.7626\n",
            "Epoch: 1816, Train Loss: 0.4455, Train auroc: 0.7867, Valid Loss: 0.4561, Valid Auroc: 0.7666\n",
            "Epoch: 1817, Train Loss: 0.4475, Train auroc: 0.7862, Valid Loss: 0.4575, Valid Auroc: 0.7665\n",
            "Epoch: 1818, Train Loss: 0.4483, Train auroc: 0.7864, Valid Loss: 0.4593, Valid Auroc: 0.7671\n",
            "Epoch: 1819, Train Loss: 0.4461, Train auroc: 0.7870, Valid Loss: 0.4622, Valid Auroc: 0.7666\n",
            "Epoch: 1820, Train Loss: 0.4470, Train auroc: 0.7865, Valid Loss: 0.4564, Valid Auroc: 0.7675\n",
            "Epoch: 1821, Train Loss: 0.4488, Train auroc: 0.7868, Valid Loss: 0.4548, Valid Auroc: 0.7692\n",
            "Epoch: 1822, Train Loss: 0.4446, Train auroc: 0.7878, Valid Loss: 0.4542, Valid Auroc: 0.7688\n",
            "Epoch: 1823, Train Loss: 0.4521, Train auroc: 0.7849, Valid Loss: 0.4700, Valid Auroc: 0.7626\n",
            "Epoch: 1824, Train Loss: 0.4464, Train auroc: 0.7859, Valid Loss: 0.4567, Valid Auroc: 0.7657\n",
            "Epoch: 1825, Train Loss: 0.4491, Train auroc: 0.7863, Valid Loss: 0.4533, Valid Auroc: 0.7700\n",
            "Epoch: 1826, Train Loss: 0.4445, Train auroc: 0.7879, Valid Loss: 0.4539, Valid Auroc: 0.7696\n",
            "Epoch: 1827, Train Loss: 0.4464, Train auroc: 0.7877, Valid Loss: 0.4620, Valid Auroc: 0.7677\n",
            "Epoch: 1828, Train Loss: 0.4454, Train auroc: 0.7873, Valid Loss: 0.4583, Valid Auroc: 0.7668\n",
            "Epoch: 1829, Train Loss: 0.4481, Train auroc: 0.7870, Valid Loss: 0.4576, Valid Auroc: 0.7675\n",
            "Epoch: 1830, Train Loss: 0.4438, Train auroc: 0.7885, Valid Loss: 0.4535, Valid Auroc: 0.7708\n",
            "Epoch: 1831, Train Loss: 0.4516, Train auroc: 0.7868, Valid Loss: 0.4669, Valid Auroc: 0.7675\n",
            "Epoch: 1832, Train Loss: 0.4424, Train auroc: 0.7892, Valid Loss: 0.4516, Valid Auroc: 0.7702\n",
            "Epoch: 1833, Train Loss: 0.4551, Train auroc: 0.7838, Valid Loss: 0.4660, Valid Auroc: 0.7631\n",
            "Epoch: 1834, Train Loss: 0.4446, Train auroc: 0.7887, Valid Loss: 0.4592, Valid Auroc: 0.7681\n",
            "Epoch: 1835, Train Loss: 0.4550, Train auroc: 0.7842, Valid Loss: 0.4711, Valid Auroc: 0.7658\n",
            "Epoch: 1836, Train Loss: 0.4486, Train auroc: 0.7860, Valid Loss: 0.4538, Valid Auroc: 0.7701\n",
            "Epoch: 1837, Train Loss: 0.4545, Train auroc: 0.7852, Valid Loss: 0.4598, Valid Auroc: 0.7683\n",
            "Epoch: 1838, Train Loss: 0.4498, Train auroc: 0.7832, Valid Loss: 0.4645, Valid Auroc: 0.7612\n",
            "Epoch: 1839, Train Loss: 0.4585, Train auroc: 0.7810, Valid Loss: 0.4780, Valid Auroc: 0.7579\n",
            "Epoch: 1840, Train Loss: 0.4490, Train auroc: 0.7852, Valid Loss: 0.4543, Valid Auroc: 0.7672\n",
            "Epoch: 1841, Train Loss: 0.4525, Train auroc: 0.7843, Valid Loss: 0.4567, Valid Auroc: 0.7687\n",
            "Epoch: 1842, Train Loss: 0.4467, Train auroc: 0.7852, Valid Loss: 0.4577, Valid Auroc: 0.7680\n",
            "Epoch: 1843, Train Loss: 0.4483, Train auroc: 0.7863, Valid Loss: 0.4631, Valid Auroc: 0.7677\n",
            "Epoch: 1844, Train Loss: 0.4466, Train auroc: 0.7858, Valid Loss: 0.4551, Valid Auroc: 0.7679\n",
            "Epoch: 1845, Train Loss: 0.4474, Train auroc: 0.7862, Valid Loss: 0.4570, Valid Auroc: 0.7664\n",
            "Epoch: 1846, Train Loss: 0.4467, Train auroc: 0.7868, Valid Loss: 0.4584, Valid Auroc: 0.7669\n",
            "Epoch: 1847, Train Loss: 0.4483, Train auroc: 0.7869, Valid Loss: 0.4618, Valid Auroc: 0.7675\n",
            "Epoch: 1848, Train Loss: 0.4466, Train auroc: 0.7872, Valid Loss: 0.4536, Valid Auroc: 0.7699\n",
            "Epoch: 1849, Train Loss: 0.4480, Train auroc: 0.7869, Valid Loss: 0.4578, Valid Auroc: 0.7678\n",
            "Epoch: 1850, Train Loss: 0.4478, Train auroc: 0.7851, Valid Loss: 0.4656, Valid Auroc: 0.7622\n",
            "Epoch: 1851, Train Loss: 0.4492, Train auroc: 0.7862, Valid Loss: 0.4644, Valid Auroc: 0.7654\n",
            "Epoch: 1852, Train Loss: 0.4480, Train auroc: 0.7881, Valid Loss: 0.4520, Valid Auroc: 0.7714\n",
            "Epoch: 1853, Train Loss: 0.4505, Train auroc: 0.7863, Valid Loss: 0.4569, Valid Auroc: 0.7683\n",
            "Epoch: 1854, Train Loss: 0.4507, Train auroc: 0.7844, Valid Loss: 0.4667, Valid Auroc: 0.7618\n",
            "Epoch: 1855, Train Loss: 0.4488, Train auroc: 0.7878, Valid Loss: 0.4671, Valid Auroc: 0.7660\n",
            "Epoch: 1856, Train Loss: 0.4520, Train auroc: 0.7856, Valid Loss: 0.4572, Valid Auroc: 0.7694\n",
            "Epoch: 1857, Train Loss: 0.4556, Train auroc: 0.7841, Valid Loss: 0.4602, Valid Auroc: 0.7682\n",
            "Epoch: 1858, Train Loss: 0.4463, Train auroc: 0.7869, Valid Loss: 0.4572, Valid Auroc: 0.7668\n",
            "Epoch: 1859, Train Loss: 0.4627, Train auroc: 0.7811, Valid Loss: 0.4838, Valid Auroc: 0.7589\n",
            "Epoch: 1860, Train Loss: 0.4453, Train auroc: 0.7866, Valid Loss: 0.4565, Valid Auroc: 0.7680\n",
            "Epoch: 1861, Train Loss: 0.4595, Train auroc: 0.7832, Valid Loss: 0.4611, Valid Auroc: 0.7692\n",
            "Epoch: 1862, Train Loss: 0.4521, Train auroc: 0.7831, Valid Loss: 0.4581, Valid Auroc: 0.7663\n",
            "Epoch: 1863, Train Loss: 0.4619, Train auroc: 0.7782, Valid Loss: 0.4770, Valid Auroc: 0.7581\n",
            "Epoch: 1864, Train Loss: 0.4490, Train auroc: 0.7841, Valid Loss: 0.4633, Valid Auroc: 0.7620\n",
            "Epoch: 1865, Train Loss: 0.4528, Train auroc: 0.7834, Valid Loss: 0.4581, Valid Auroc: 0.7653\n",
            "Epoch: 1866, Train Loss: 0.4523, Train auroc: 0.7830, Valid Loss: 0.4569, Valid Auroc: 0.7665\n",
            "Epoch: 1867, Train Loss: 0.4488, Train auroc: 0.7843, Valid Loss: 0.4621, Valid Auroc: 0.7653\n",
            "Epoch: 1868, Train Loss: 0.4502, Train auroc: 0.7855, Valid Loss: 0.4663, Valid Auroc: 0.7655\n",
            "Epoch: 1869, Train Loss: 0.4495, Train auroc: 0.7841, Valid Loss: 0.4580, Valid Auroc: 0.7663\n",
            "Epoch: 1870, Train Loss: 0.4528, Train auroc: 0.7838, Valid Loss: 0.4596, Valid Auroc: 0.7663\n",
            "Epoch: 1871, Train Loss: 0.4455, Train auroc: 0.7873, Valid Loss: 0.4543, Valid Auroc: 0.7695\n",
            "Epoch: 1872, Train Loss: 0.4549, Train auroc: 0.7824, Valid Loss: 0.4702, Valid Auroc: 0.7629\n",
            "Epoch: 1873, Train Loss: 0.4465, Train auroc: 0.7857, Valid Loss: 0.4582, Valid Auroc: 0.7662\n",
            "Epoch: 1874, Train Loss: 0.4526, Train auroc: 0.7856, Valid Loss: 0.4600, Valid Auroc: 0.7683\n",
            "Epoch: 1875, Train Loss: 0.4478, Train auroc: 0.7860, Valid Loss: 0.4552, Valid Auroc: 0.7696\n",
            "Epoch: 1876, Train Loss: 0.4547, Train auroc: 0.7837, Valid Loss: 0.4696, Valid Auroc: 0.7644\n",
            "Epoch: 1877, Train Loss: 0.4495, Train auroc: 0.7858, Valid Loss: 0.4626, Valid Auroc: 0.7648\n",
            "Epoch: 1878, Train Loss: 0.4516, Train auroc: 0.7864, Valid Loss: 0.4600, Valid Auroc: 0.7675\n",
            "Epoch: 1879, Train Loss: 0.4496, Train auroc: 0.7854, Valid Loss: 0.4557, Valid Auroc: 0.7699\n",
            "Epoch: 1880, Train Loss: 0.4518, Train auroc: 0.7851, Valid Loss: 0.4689, Valid Auroc: 0.7665\n",
            "Epoch: 1881, Train Loss: 0.4453, Train auroc: 0.7873, Valid Loss: 0.4558, Valid Auroc: 0.7678\n",
            "Epoch: 1882, Train Loss: 0.4524, Train auroc: 0.7846, Valid Loss: 0.4650, Valid Auroc: 0.7635\n",
            "Epoch: 1883, Train Loss: 0.4432, Train auroc: 0.7888, Valid Loss: 0.4534, Valid Auroc: 0.7694\n",
            "Epoch: 1884, Train Loss: 0.4530, Train auroc: 0.7856, Valid Loss: 0.4676, Valid Auroc: 0.7664\n",
            "Epoch: 1885, Train Loss: 0.4434, Train auroc: 0.7890, Valid Loss: 0.4521, Valid Auroc: 0.7717\n",
            "Epoch: 1886, Train Loss: 0.4507, Train auroc: 0.7860, Valid Loss: 0.4592, Valid Auroc: 0.7683\n",
            "Epoch: 1887, Train Loss: 0.4474, Train auroc: 0.7873, Valid Loss: 0.4607, Valid Auroc: 0.7660\n",
            "Epoch: 1888, Train Loss: 0.4471, Train auroc: 0.7879, Valid Loss: 0.4619, Valid Auroc: 0.7669\n",
            "Epoch: 1889, Train Loss: 0.4477, Train auroc: 0.7870, Valid Loss: 0.4571, Valid Auroc: 0.7676\n",
            "Epoch: 1890, Train Loss: 0.4419, Train auroc: 0.7909, Valid Loss: 0.4503, Valid Auroc: 0.7719\n",
            "Epoch: 1891, Train Loss: 0.4456, Train auroc: 0.7891, Valid Loss: 0.4584, Valid Auroc: 0.7682\n",
            "Epoch: 1892, Train Loss: 0.4435, Train auroc: 0.7895, Valid Loss: 0.4576, Valid Auroc: 0.7688\n",
            "Epoch: 1893, Train Loss: 0.4424, Train auroc: 0.7898, Valid Loss: 0.4553, Valid Auroc: 0.7708\n",
            "Epoch: 1894, Train Loss: 0.4462, Train auroc: 0.7890, Valid Loss: 0.4533, Valid Auroc: 0.7715\n",
            "Epoch: 1895, Train Loss: 0.4404, Train auroc: 0.7920, Valid Loss: 0.4508, Valid Auroc: 0.7720\n",
            "Epoch: 1896, Train Loss: 0.4484, Train auroc: 0.7868, Valid Loss: 0.4647, Valid Auroc: 0.7627\n",
            "Epoch: 1897, Train Loss: 0.4435, Train auroc: 0.7899, Valid Loss: 0.4590, Valid Auroc: 0.7677\n",
            "Epoch: 1898, Train Loss: 0.4449, Train auroc: 0.7896, Valid Loss: 0.4524, Valid Auroc: 0.7723\n",
            "Epoch: 1899, Train Loss: 0.4467, Train auroc: 0.7883, Valid Loss: 0.4525, Valid Auroc: 0.7719\n",
            "Epoch: 1900, Train Loss: 0.4420, Train auroc: 0.7909, Valid Loss: 0.4564, Valid Auroc: 0.7699\n",
            "Epoch: 1901, Train Loss: 0.4499, Train auroc: 0.7871, Valid Loss: 0.4682, Valid Auroc: 0.7627\n",
            "Epoch: 1902, Train Loss: 0.4419, Train auroc: 0.7913, Valid Loss: 0.4549, Valid Auroc: 0.7694\n",
            "Epoch: 1903, Train Loss: 0.4471, Train auroc: 0.7895, Valid Loss: 0.4540, Valid Auroc: 0.7718\n",
            "Epoch: 1904, Train Loss: 0.4470, Train auroc: 0.7885, Valid Loss: 0.4582, Valid Auroc: 0.7705\n",
            "Epoch: 1905, Train Loss: 0.4422, Train auroc: 0.7910, Valid Loss: 0.4562, Valid Auroc: 0.7710\n",
            "Epoch: 1906, Train Loss: 0.4516, Train auroc: 0.7860, Valid Loss: 0.4673, Valid Auroc: 0.7634\n",
            "Epoch: 1907, Train Loss: 0.4417, Train auroc: 0.7907, Valid Loss: 0.4537, Valid Auroc: 0.7695\n",
            "Epoch: 1908, Train Loss: 0.4504, Train auroc: 0.7852, Valid Loss: 0.4626, Valid Auroc: 0.7673\n",
            "Epoch: 1909, Train Loss: 0.4525, Train auroc: 0.7820, Valid Loss: 0.4571, Valid Auroc: 0.7684\n",
            "Epoch: 1910, Train Loss: 0.4460, Train auroc: 0.7868, Valid Loss: 0.4522, Valid Auroc: 0.7715\n",
            "Epoch: 1911, Train Loss: 0.4539, Train auroc: 0.7806, Valid Loss: 0.4636, Valid Auroc: 0.7627\n",
            "Epoch: 1912, Train Loss: 0.4496, Train auroc: 0.7824, Valid Loss: 0.4618, Valid Auroc: 0.7624\n",
            "Epoch: 1913, Train Loss: 0.4467, Train auroc: 0.7845, Valid Loss: 0.4548, Valid Auroc: 0.7667\n",
            "Epoch: 1914, Train Loss: 0.4468, Train auroc: 0.7856, Valid Loss: 0.4535, Valid Auroc: 0.7694\n",
            "Epoch: 1915, Train Loss: 0.4446, Train auroc: 0.7868, Valid Loss: 0.4538, Valid Auroc: 0.7700\n",
            "Epoch: 1916, Train Loss: 0.4451, Train auroc: 0.7866, Valid Loss: 0.4564, Valid Auroc: 0.7683\n",
            "Epoch: 1917, Train Loss: 0.4429, Train auroc: 0.7882, Valid Loss: 0.4526, Valid Auroc: 0.7701\n",
            "Epoch: 1918, Train Loss: 0.4438, Train auroc: 0.7887, Valid Loss: 0.4521, Valid Auroc: 0.7710\n",
            "Epoch: 1919, Train Loss: 0.4423, Train auroc: 0.7895, Valid Loss: 0.4528, Valid Auroc: 0.7701\n",
            "Epoch: 1920, Train Loss: 0.4428, Train auroc: 0.7894, Valid Loss: 0.4543, Valid Auroc: 0.7694\n",
            "Epoch: 1921, Train Loss: 0.4411, Train auroc: 0.7908, Valid Loss: 0.4520, Valid Auroc: 0.7718\n",
            "Epoch: 1922, Train Loss: 0.4400, Train auroc: 0.7914, Valid Loss: 0.4495, Valid Auroc: 0.7733\n",
            "Epoch: 1923, Train Loss: 0.4397, Train auroc: 0.7917, Valid Loss: 0.4514, Valid Auroc: 0.7732\n",
            "Epoch: 1924, Train Loss: 0.4387, Train auroc: 0.7928, Valid Loss: 0.4503, Valid Auroc: 0.7732\n",
            "Epoch: 1925, Train Loss: 0.4386, Train auroc: 0.7933, Valid Loss: 0.4491, Valid Auroc: 0.7736\n",
            "Epoch: 1926, Train Loss: 0.4383, Train auroc: 0.7936, Valid Loss: 0.4504, Valid Auroc: 0.7739\n",
            "Epoch: 1927, Train Loss: 0.4380, Train auroc: 0.7941, Valid Loss: 0.4507, Valid Auroc: 0.7743\n",
            "Epoch: 1928, Train Loss: 0.4390, Train auroc: 0.7936, Valid Loss: 0.4493, Valid Auroc: 0.7736\n",
            "Epoch: 1929, Train Loss: 0.4376, Train auroc: 0.7941, Valid Loss: 0.4511, Valid Auroc: 0.7731\n",
            "Epoch: 1930, Train Loss: 0.4402, Train auroc: 0.7944, Valid Loss: 0.4548, Valid Auroc: 0.7742\n",
            "Epoch: 1931, Train Loss: 0.4393, Train auroc: 0.7945, Valid Loss: 0.4477, Valid Auroc: 0.7759\n",
            "Epoch: 1932, Train Loss: 0.4402, Train auroc: 0.7938, Valid Loss: 0.4509, Valid Auroc: 0.7731\n",
            "Epoch: 1933, Train Loss: 0.4449, Train auroc: 0.7923, Valid Loss: 0.4661, Valid Auroc: 0.7696\n",
            "Epoch: 1934, Train Loss: 0.4412, Train auroc: 0.7938, Valid Loss: 0.4513, Valid Auroc: 0.7752\n",
            "Epoch: 1935, Train Loss: 0.4523, Train auroc: 0.7898, Valid Loss: 0.4583, Valid Auroc: 0.7726\n",
            "Epoch: 1936, Train Loss: 0.4434, Train auroc: 0.7918, Valid Loss: 0.4555, Valid Auroc: 0.7714\n",
            "Epoch: 1937, Train Loss: 0.4606, Train auroc: 0.7876, Valid Loss: 0.4850, Valid Auroc: 0.7646\n",
            "Epoch: 1938, Train Loss: 0.4464, Train auroc: 0.7904, Valid Loss: 0.4574, Valid Auroc: 0.7703\n",
            "Epoch: 1939, Train Loss: 0.4730, Train auroc: 0.7807, Valid Loss: 0.4765, Valid Auroc: 0.7637\n",
            "Epoch: 1940, Train Loss: 0.4475, Train auroc: 0.7871, Valid Loss: 0.4529, Valid Auroc: 0.7716\n",
            "Epoch: 1941, Train Loss: 0.4802, Train auroc: 0.7782, Valid Loss: 0.5039, Valid Auroc: 0.7575\n",
            "Epoch: 1942, Train Loss: 0.4514, Train auroc: 0.7823, Valid Loss: 0.4655, Valid Auroc: 0.7614\n",
            "Epoch: 1943, Train Loss: 0.4768, Train auroc: 0.7737, Valid Loss: 0.4871, Valid Auroc: 0.7530\n",
            "Epoch: 1944, Train Loss: 0.4713, Train auroc: 0.7782, Valid Loss: 0.4726, Valid Auroc: 0.7638\n",
            "Epoch: 1945, Train Loss: 0.4541, Train auroc: 0.7790, Valid Loss: 0.4587, Valid Auroc: 0.7660\n",
            "Epoch: 1946, Train Loss: 0.4815, Train auroc: 0.7729, Valid Loss: 0.4968, Valid Auroc: 0.7556\n",
            "Epoch: 1947, Train Loss: 0.4558, Train auroc: 0.7789, Valid Loss: 0.4651, Valid Auroc: 0.7610\n",
            "Epoch: 1948, Train Loss: 0.4697, Train auroc: 0.7724, Valid Loss: 0.4737, Valid Auroc: 0.7580\n",
            "Epoch: 1949, Train Loss: 0.4712, Train auroc: 0.7725, Valid Loss: 0.4721, Valid Auroc: 0.7596\n",
            "Epoch: 1950, Train Loss: 0.4572, Train auroc: 0.7759, Valid Loss: 0.4642, Valid Auroc: 0.7591\n",
            "Epoch: 1951, Train Loss: 0.4662, Train auroc: 0.7698, Valid Loss: 0.4776, Valid Auroc: 0.7508\n",
            "Epoch: 1952, Train Loss: 0.4672, Train auroc: 0.7684, Valid Loss: 0.4810, Valid Auroc: 0.7465\n",
            "Epoch: 1953, Train Loss: 0.4560, Train auroc: 0.7773, Valid Loss: 0.4651, Valid Auroc: 0.7567\n",
            "Epoch: 1954, Train Loss: 0.4592, Train auroc: 0.7776, Valid Loss: 0.4625, Valid Auroc: 0.7629\n",
            "Epoch: 1955, Train Loss: 0.4613, Train auroc: 0.7789, Valid Loss: 0.4652, Valid Auroc: 0.7638\n",
            "Epoch: 1956, Train Loss: 0.4489, Train auroc: 0.7823, Valid Loss: 0.4589, Valid Auroc: 0.7627\n",
            "Epoch: 1957, Train Loss: 0.4572, Train auroc: 0.7760, Valid Loss: 0.4719, Valid Auroc: 0.7536\n",
            "Epoch: 1958, Train Loss: 0.4563, Train auroc: 0.7783, Valid Loss: 0.4715, Valid Auroc: 0.7557\n",
            "Epoch: 1959, Train Loss: 0.4462, Train auroc: 0.7858, Valid Loss: 0.4566, Valid Auroc: 0.7669\n",
            "Epoch: 1960, Train Loss: 0.4524, Train auroc: 0.7835, Valid Loss: 0.4584, Valid Auroc: 0.7682\n",
            "Epoch: 1961, Train Loss: 0.4501, Train auroc: 0.7851, Valid Loss: 0.4566, Valid Auroc: 0.7693\n",
            "Epoch: 1962, Train Loss: 0.4442, Train auroc: 0.7870, Valid Loss: 0.4539, Valid Auroc: 0.7684\n",
            "Epoch: 1963, Train Loss: 0.4509, Train auroc: 0.7829, Valid Loss: 0.4665, Valid Auroc: 0.7610\n",
            "Epoch: 1964, Train Loss: 0.4450, Train auroc: 0.7877, Valid Loss: 0.4593, Valid Auroc: 0.7656\n",
            "Epoch: 1965, Train Loss: 0.4441, Train auroc: 0.7893, Valid Loss: 0.4528, Valid Auroc: 0.7710\n",
            "Epoch: 1966, Train Loss: 0.4494, Train auroc: 0.7869, Valid Loss: 0.4556, Valid Auroc: 0.7709\n",
            "Epoch: 1967, Train Loss: 0.4423, Train auroc: 0.7902, Valid Loss: 0.4509, Valid Auroc: 0.7731\n",
            "Epoch: 1968, Train Loss: 0.4439, Train auroc: 0.7890, Valid Loss: 0.4577, Valid Auroc: 0.7692\n",
            "Epoch: 1969, Train Loss: 0.4459, Train auroc: 0.7875, Valid Loss: 0.4610, Valid Auroc: 0.7659\n",
            "Epoch: 1970, Train Loss: 0.4399, Train auroc: 0.7916, Valid Loss: 0.4516, Valid Auroc: 0.7714\n",
            "Epoch: 1971, Train Loss: 0.4448, Train auroc: 0.7908, Valid Loss: 0.4531, Valid Auroc: 0.7733\n",
            "Epoch: 1972, Train Loss: 0.4415, Train auroc: 0.7924, Valid Loss: 0.4501, Valid Auroc: 0.7747\n",
            "Epoch: 1973, Train Loss: 0.4407, Train auroc: 0.7920, Valid Loss: 0.4525, Valid Auroc: 0.7721\n",
            "Epoch: 1974, Train Loss: 0.4454, Train auroc: 0.7891, Valid Loss: 0.4615, Valid Auroc: 0.7668\n",
            "Epoch: 1975, Train Loss: 0.4391, Train auroc: 0.7936, Valid Loss: 0.4548, Valid Auroc: 0.7720\n",
            "Epoch: 1976, Train Loss: 0.4428, Train auroc: 0.7926, Valid Loss: 0.4521, Valid Auroc: 0.7743\n",
            "Epoch: 1977, Train Loss: 0.4444, Train auroc: 0.7913, Valid Loss: 0.4510, Valid Auroc: 0.7745\n",
            "Epoch: 1978, Train Loss: 0.4423, Train auroc: 0.7900, Valid Loss: 0.4540, Valid Auroc: 0.7709\n",
            "Epoch: 1979, Train Loss: 0.4506, Train auroc: 0.7862, Valid Loss: 0.4686, Valid Auroc: 0.7643\n",
            "Epoch: 1980, Train Loss: 0.4477, Train auroc: 0.7864, Valid Loss: 0.4588, Valid Auroc: 0.7660\n",
            "Epoch: 1981, Train Loss: 0.4459, Train auroc: 0.7886, Valid Loss: 0.4549, Valid Auroc: 0.7726\n",
            "Epoch: 1982, Train Loss: 0.4446, Train auroc: 0.7887, Valid Loss: 0.4571, Valid Auroc: 0.7717\n",
            "Epoch: 1983, Train Loss: 0.4454, Train auroc: 0.7882, Valid Loss: 0.4557, Valid Auroc: 0.7705\n",
            "Epoch: 1984, Train Loss: 0.4439, Train auroc: 0.7894, Valid Loss: 0.4559, Valid Auroc: 0.7702\n",
            "Epoch: 1985, Train Loss: 0.4455, Train auroc: 0.7895, Valid Loss: 0.4614, Valid Auroc: 0.7702\n",
            "Epoch: 1986, Train Loss: 0.4446, Train auroc: 0.7898, Valid Loss: 0.4549, Valid Auroc: 0.7725\n",
            "Epoch: 1987, Train Loss: 0.4456, Train auroc: 0.7903, Valid Loss: 0.4524, Valid Auroc: 0.7735\n",
            "Epoch: 1988, Train Loss: 0.4444, Train auroc: 0.7898, Valid Loss: 0.4555, Valid Auroc: 0.7702\n",
            "Epoch: 1989, Train Loss: 0.4476, Train auroc: 0.7890, Valid Loss: 0.4706, Valid Auroc: 0.7667\n",
            "Epoch: 1990, Train Loss: 0.4437, Train auroc: 0.7914, Valid Loss: 0.4567, Valid Auroc: 0.7720\n",
            "Epoch: 1991, Train Loss: 0.4495, Train auroc: 0.7881, Valid Loss: 0.4579, Valid Auroc: 0.7713\n",
            "Epoch: 1992, Train Loss: 0.4496, Train auroc: 0.7882, Valid Loss: 0.4584, Valid Auroc: 0.7712\n",
            "Epoch: 1993, Train Loss: 0.4469, Train auroc: 0.7895, Valid Loss: 0.4605, Valid Auroc: 0.7709\n",
            "Epoch: 1994, Train Loss: 0.4506, Train auroc: 0.7873, Valid Loss: 0.4689, Valid Auroc: 0.7667\n",
            "Epoch: 1995, Train Loss: 0.4526, Train auroc: 0.7861, Valid Loss: 0.4609, Valid Auroc: 0.7684\n",
            "Epoch: 1996, Train Loss: 0.4483, Train auroc: 0.7883, Valid Loss: 0.4569, Valid Auroc: 0.7713\n",
            "Epoch: 1997, Train Loss: 0.4565, Train auroc: 0.7844, Valid Loss: 0.4663, Valid Auroc: 0.7678\n",
            "Epoch: 1998, Train Loss: 0.4505, Train auroc: 0.7866, Valid Loss: 0.4643, Valid Auroc: 0.7679\n",
            "Epoch: 1999, Train Loss: 0.4498, Train auroc: 0.7861, Valid Loss: 0.4585, Valid Auroc: 0.7682\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import Javascript  # Restrict height of output cell.\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=5e-7)\n",
        "\n",
        "epoch_num = 2000\n",
        "for epoch in range(1, epoch_num):\n",
        "    train()\n",
        "    train_loss, train_auroc = gnn_test(model, loss_fn, train_loader, device)\n",
        "    val_loss, val_auroc = gnn_test(model, loss_fn, val_loader, device)\n",
        "    print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Train auroc: {train_auroc:.4f}, Valid Loss: {val_loss:.4f}, Valid Auroc: {val_auroc:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lpinwrFF_1-"
      },
      "source": [
        "### Save the model\n",
        "Save the NodeGCN model to file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSbkEBaCF_1-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5a4aece-a7f3-44b1-a5c5-24fa59fdf006"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved PyTorch Model State to node_gcn.pth\n"
          ]
        }
      ],
      "source": [
        "torch.save(model.state_dict(), \"node_gcn.pth\")\n",
        "print(\"Saved PyTorch Model State to node_gcn.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1KEX6ouF_1-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dfda3ca-8471-404c-b0ae-da7d134b7f51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val_loss: 0.458481, val_auroc: 0.768188\n"
          ]
        }
      ],
      "source": [
        "model = NodeGCN().to(device)\n",
        "model.load_state_dict(torch.load(\"node_gcn.pth\"))\n",
        "val_loss, val_auroc = gnn_test(model, loss_fn, val_loader, device)\n",
        "print(\"val_loss: %f, val_auroc: %f\" % (val_loss, val_auroc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qcq7iy63F_1_"
      },
      "source": [
        "## Graph Classification\n",
        "\n",
        "The upcoming section will focus on the application of Graph Neural Networks (GNNs) to graph classification, which involves classifying complete graphs (rather than individual nodes) based on structural graph characteristics using a dataset of graphs. In this scenario, the objective is to embed complete graphs in a manner that makes them linearly separable for a given task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvSi9BC0F_1_"
      },
      "source": [
        "Graph classification is often used for predicting molecular properties. In this context, molecules are represented as graphs, and the goal may be to determine if a molecule inhibits the HIV virus or not.\n",
        "\n",
        "One source of graph classification datasets is the `TUDatasets` collection, curated by TU Dortmund University. These datasets are available through the torch_geometric.datasets.TUDataset module in PyTorch Geometric. For the purpose of this tutorial, we will load a subset of the TUDatasets: the protein graphs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TS1rDuUHF_1_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e94d26d-43ad-4456-83a4-7b8129751140"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://www.chrsmrrs.com/graphkerneldatasets/PROTEINS_full.zip\n",
            "Extracting data/TUDataset/PROTEINS_full/PROTEINS_full.zip\n",
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset: PROTEINS_full(1113):\n",
            "Number of graphs: 1113\n",
            "Number of features: 3\n",
            "Number of classes: 2\n",
            "\n",
            "Data(edge_index=[2, 162], x=[42, 3], y=[1])\n",
            "Number of nodes: 42\n",
            "Number of edges: 162\n",
            "Is undirected: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.datasets import TUDataset\n",
        "\n",
        "dataset = TUDataset(root='data/TUDataset', name='PROTEINS_full')\n",
        "\n",
        "print()\n",
        "print(f'Dataset: {dataset}:')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "\n",
        "data = dataset[0]\n",
        "\n",
        "print()\n",
        "print(data)\n",
        "\n",
        "print(f'Number of nodes: {data.num_nodes}')\n",
        "print(f'Number of edges: {data.num_edges}')\n",
        "print(f'Is undirected: {data.is_undirected()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRWzaZZQF_1_"
      },
      "source": [
        "This dataset provides about one thousand graphs. This is a binary classfication task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYOoW451F_1_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df171133-c334-4d90-8a3a-8cba4df81337"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training graphs: 779\n",
            "Number of valid graphs: 168\n"
          ]
        }
      ],
      "source": [
        "### DO NOT CHANGE THE CODE IN THIS CELL ###\n",
        "\n",
        "# We split the dataset into train set, valid set and test set\n",
        "torch.manual_seed(12345678)\n",
        "dataset = dataset.shuffle()\n",
        "test_ratio = 0.15\n",
        "test_len = int(len(dataset) * test_ratio)\n",
        "train_valid_len = len(dataset) - test_len\n",
        "train_ratio = 0.7\n",
        "train_len = int(len(dataset) * train_ratio)\n",
        "valid_len = len(dataset) - train_len - test_len\n",
        "train_dataset = dataset[:train_len]\n",
        "valid_dataset = dataset[train_len:train_len + valid_len]\n",
        "test_dataset = dataset[train_len + valid_len:]\n",
        "print(f'Number of training graphs: {len(train_dataset)}')\n",
        "print(f'Number of valid graphs: {len(valid_dataset)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iH_1GxvpF_2A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d902439c-3fd8-487a-a36f-433b6ac40488"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "DataBatch(edge_index=[2, 9522], x=[2532, 3], y=[64], batch=[2532], ptr=[65])\n",
            "\n",
            "Step 2:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "DataBatch(edge_index=[2, 7766], x=[2084, 3], y=[64], batch=[2084], ptr=[65])\n",
            "\n",
            "Step 3:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "DataBatch(edge_index=[2, 9156], x=[2506, 3], y=[64], batch=[2506], ptr=[65])\n",
            "\n",
            "Step 4:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "DataBatch(edge_index=[2, 9588], x=[2548, 3], y=[64], batch=[2548], ptr=[65])\n",
            "\n",
            "Step 5:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "DataBatch(edge_index=[2, 11064], x=[3050, 3], y=[64], batch=[3050], ptr=[65])\n",
            "\n",
            "Step 6:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "DataBatch(edge_index=[2, 8890], x=[2383, 3], y=[64], batch=[2383], ptr=[65])\n",
            "\n",
            "Step 7:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "DataBatch(edge_index=[2, 9172], x=[2368, 3], y=[64], batch=[2368], ptr=[65])\n",
            "\n",
            "Step 8:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "DataBatch(edge_index=[2, 9748], x=[2452, 3], y=[64], batch=[2452], ptr=[65])\n",
            "\n",
            "Step 9:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "DataBatch(edge_index=[2, 9776], x=[2568, 3], y=[64], batch=[2568], ptr=[65])\n",
            "\n",
            "Step 10:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "DataBatch(edge_index=[2, 7708], x=[2041, 3], y=[64], batch=[2041], ptr=[65])\n",
            "\n",
            "Step 11:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "DataBatch(edge_index=[2, 8476], x=[2317, 3], y=[64], batch=[2317], ptr=[65])\n",
            "\n",
            "Step 12:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "DataBatch(edge_index=[2, 9372], x=[2586, 3], y=[64], batch=[2586], ptr=[65])\n",
            "\n",
            "Step 13:\n",
            "=======\n",
            "Number of graphs in the current batch: 11\n",
            "DataBatch(edge_index=[2, 1036], x=[266, 3], y=[11], batch=[266], ptr=[12])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "batch_size=64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "for step, data in enumerate(train_loader):\n",
        "    print(f'Step {step + 1}:')\n",
        "    print('=======')\n",
        "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
        "    print(data)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6t5uQ8b7F_2A"
      },
      "source": [
        "To train a GNN for graph classification, we typically follow a three-step recipe:\n",
        "\n",
        "- Embedding each node by performing multiple rounds of message passing.\n",
        "- Aggregating the node embeddings into a single graph embedding, which is also known as the readout layer.\n",
        "- Training a classifier on the graph embedding.\n",
        "\n",
        "There are several readout layer options available, but the most common one is to take the average of node embeddings:\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_{\\mathcal{G}} = \\frac{1}{|\\mathcal{V}|} \\sum_{v \\in \\mathcal{V}} \\mathcal{x}^{(L)}_v\n",
        "$$\n",
        "\n",
        "This functionality is provided by PyTorch Geometric via [`torch_geometric.nn.global_mean_pool`](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.glob.global_mean_pool). Given the node embeddings of all nodes in the mini-batch and the `batch` assignment vector, this function computes a graph embedding of size `[batch_size, hidden_channels]` for each graph in the batch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYnjPPDPF_2A"
      },
      "source": [
        "## Task 4: Graph Classfication using GCNConv [5 pts]\n",
        "\n",
        "Please design a GNN model based on GCNConv to do this graph classification job. Complete the following class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDWOaNUzF_2A"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "### DO NOT CHANGE ANY CODE ABOVE THIS LINE IN THIS CELL ###\n",
        "\n",
        "\n",
        "class GraphGCN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GraphGCN, self).__init__()\n",
        "        # TODO: Please design your MLP layers\n",
        "        self.conv1 = GCNConv(3, 16)\n",
        "        self.conv2 = GCNConv(16, 32)\n",
        "        self.conv3 = GCNConv(32, 64)\n",
        "        self.conv4 = GCNConv(64, 128)\n",
        "        self.conv5 = GCNConv(128, 256)\n",
        "        self.conv6 = GCNConv(256, 512)\n",
        "        self.conv7 = GCNConv(512, 1024)\n",
        "        self.fc = Linear(1024, 2)\n",
        "        pass\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        # TODO: Finish this function\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = F.relu(self.conv3(x, edge_index))\n",
        "        x = F.relu(self.conv4(x, edge_index))\n",
        "        x = F.relu(self.conv5(x, edge_index))\n",
        "        x = F.relu(self.conv6(x, edge_index))\n",
        "        x = F.relu(self.conv7(x, edge_index))\n",
        "        x = global_mean_pool(x, batch)\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "    \n",
        "######################################################################\n",
        "########## DON'T WRITE ANY CODE OUTSIDE THE CLASS! ###################\n",
        "######## IF YOU WANT TO CALL OR TEST IT CREATE A NEW CELL ############\n",
        "######################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLPPVE1iF_2A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e43a0ea-d920-4ddd-8799-381d62ea255d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GraphGCN(\n",
            "  (conv1): GCNConv(3, 16)\n",
            "  (conv2): GCNConv(16, 32)\n",
            "  (conv3): GCNConv(32, 64)\n",
            "  (conv4): GCNConv(64, 128)\n",
            "  (conv5): GCNConv(128, 256)\n",
            "  (conv6): GCNConv(256, 512)\n",
            "  (conv7): GCNConv(512, 1024)\n",
            "  (fc): Linear(in_features=1024, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = GraphGCN().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pZsRhR1hY2N"
      },
      "source": [
        "## Task 5: Finish the `graph_test` function and compute the ROC AUC score [2 pts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCKJ1QYxWn8v"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "from torchmetrics.classification import MulticlassAUROC\n",
        "### DO NOT CHANGE ANY CODE ABOVE THIS LINE IN THIS CELL ###\n",
        "\n",
        "def graph_test(model, loss_fn, loader, device):\n",
        "     \"\"\"\n",
        "     model: pytorch GNN model\n",
        "     loss_fn: loss function\n",
        "     loader: DataLoader\n",
        "     device: device eused to bind the model and tensor\n",
        "     \"\"\"\n",
        "     model.eval()\n",
        "     auroc_score = 0.0\n",
        "     # TODO: Finish this function to calculate AUROC score\n",
        "     auroc = MulticlassAUROC(num_classes=2)\n",
        "     with torch.no_grad():\n",
        "       for data in loader:\n",
        "         data.x, data.y = data.x.to(device), data.y.to(device)\n",
        "         out = model(data.x, data.edge_index.to(device), data.batch.to(device))\n",
        "         auroc.update(out, data.y.to(torch.long))\n",
        "\n",
        "     auroc_score = auroc.compute().mean().item()\n",
        "     return auroc_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJej3ikjF_2A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "049dcae2-d63e-45a1-fe19-fcf171a4bfd1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 000, Train Loss: 0.67906, Train Auc: 0.6703, Valid Auc: 0.7168\n",
            "Epoch: 001, Train Loss: 0.67859, Train Auc: 0.6354, Valid Auc: 0.6745\n",
            "Epoch: 002, Train Loss: 0.66859, Train Auc: 0.6828, Valid Auc: 0.7404\n",
            "Epoch: 003, Train Loss: 0.64000, Train Auc: 0.6944, Valid Auc: 0.7423\n",
            "Epoch: 004, Train Loss: 0.64209, Train Auc: 0.7015, Valid Auc: 0.7296\n",
            "Epoch: 005, Train Loss: 0.62255, Train Auc: 0.6996, Valid Auc: 0.7341\n",
            "Epoch: 006, Train Loss: 0.62197, Train Auc: 0.6909, Valid Auc: 0.7139\n",
            "Epoch: 007, Train Loss: 0.60804, Train Auc: 0.6638, Valid Auc: 0.7243\n",
            "Epoch: 008, Train Loss: 0.61907, Train Auc: 0.6918, Valid Auc: 0.6940\n",
            "Epoch: 009, Train Loss: 0.61961, Train Auc: 0.6982, Valid Auc: 0.7037\n",
            "Epoch: 010, Train Loss: 0.61544, Train Auc: 0.7030, Valid Auc: 0.7120\n",
            "Epoch: 011, Train Loss: 0.60907, Train Auc: 0.7191, Valid Auc: 0.7314\n",
            "Epoch: 012, Train Loss: 0.60215, Train Auc: 0.6832, Valid Auc: 0.6846\n",
            "Epoch: 013, Train Loss: 0.61803, Train Auc: 0.7245, Valid Auc: 0.7386\n",
            "Epoch: 014, Train Loss: 0.61375, Train Auc: 0.7072, Valid Auc: 0.7158\n",
            "Epoch: 015, Train Loss: 0.59678, Train Auc: 0.7138, Valid Auc: 0.7275\n",
            "Epoch: 016, Train Loss: 0.61403, Train Auc: 0.7263, Valid Auc: 0.7325\n",
            "Epoch: 017, Train Loss: 0.60535, Train Auc: 0.7103, Valid Auc: 0.7097\n",
            "Epoch: 018, Train Loss: 0.58717, Train Auc: 0.7190, Valid Auc: 0.7276\n",
            "Epoch: 019, Train Loss: 0.59964, Train Auc: 0.7074, Valid Auc: 0.7230\n",
            "Epoch: 020, Train Loss: 0.60351, Train Auc: 0.7234, Valid Auc: 0.7265\n",
            "Epoch: 021, Train Loss: 0.60654, Train Auc: 0.7225, Valid Auc: 0.7031\n",
            "Epoch: 022, Train Loss: 0.58606, Train Auc: 0.7185, Valid Auc: 0.7309\n",
            "Epoch: 023, Train Loss: 0.59613, Train Auc: 0.7144, Valid Auc: 0.7425\n",
            "Epoch: 024, Train Loss: 0.59878, Train Auc: 0.7179, Valid Auc: 0.7164\n",
            "Epoch: 025, Train Loss: 0.58235, Train Auc: 0.7167, Valid Auc: 0.7488\n",
            "Epoch: 026, Train Loss: 0.58870, Train Auc: 0.7208, Valid Auc: 0.7244\n",
            "Epoch: 027, Train Loss: 0.60617, Train Auc: 0.7332, Valid Auc: 0.7348\n",
            "Epoch: 028, Train Loss: 0.61117, Train Auc: 0.7278, Valid Auc: 0.7205\n",
            "Epoch: 029, Train Loss: 0.60836, Train Auc: 0.7217, Valid Auc: 0.7159\n",
            "Epoch: 030, Train Loss: 0.59371, Train Auc: 0.7195, Valid Auc: 0.7188\n",
            "Epoch: 031, Train Loss: 0.60031, Train Auc: 0.7266, Valid Auc: 0.7366\n",
            "Epoch: 032, Train Loss: 0.60233, Train Auc: 0.7259, Valid Auc: 0.7208\n",
            "Epoch: 033, Train Loss: 0.61299, Train Auc: 0.7291, Valid Auc: 0.7338\n",
            "Epoch: 034, Train Loss: 0.59163, Train Auc: 0.7269, Valid Auc: 0.7266\n",
            "Epoch: 035, Train Loss: 0.58536, Train Auc: 0.7221, Valid Auc: 0.7177\n",
            "Epoch: 036, Train Loss: 0.60085, Train Auc: 0.7300, Valid Auc: 0.7276\n",
            "Epoch: 037, Train Loss: 0.58187, Train Auc: 0.7217, Valid Auc: 0.7129\n",
            "Epoch: 038, Train Loss: 0.58493, Train Auc: 0.7267, Valid Auc: 0.7138\n",
            "Epoch: 039, Train Loss: 0.60980, Train Auc: 0.7302, Valid Auc: 0.7273\n",
            "Epoch: 040, Train Loss: 0.58694, Train Auc: 0.7354, Valid Auc: 0.7393\n",
            "Epoch: 041, Train Loss: 0.58466, Train Auc: 0.7339, Valid Auc: 0.7260\n",
            "Epoch: 042, Train Loss: 0.59489, Train Auc: 0.7311, Valid Auc: 0.7308\n",
            "Epoch: 043, Train Loss: 0.60158, Train Auc: 0.7374, Valid Auc: 0.7295\n",
            "Epoch: 044, Train Loss: 0.57777, Train Auc: 0.7241, Valid Auc: 0.7118\n",
            "Epoch: 045, Train Loss: 0.59296, Train Auc: 0.7380, Valid Auc: 0.7370\n",
            "Epoch: 046, Train Loss: 0.60205, Train Auc: 0.7348, Valid Auc: 0.7342\n",
            "Epoch: 047, Train Loss: 0.58276, Train Auc: 0.7333, Valid Auc: 0.7273\n",
            "Epoch: 048, Train Loss: 0.58521, Train Auc: 0.7391, Valid Auc: 0.7354\n",
            "Epoch: 049, Train Loss: 0.59277, Train Auc: 0.7214, Valid Auc: 0.7073\n",
            "Epoch: 050, Train Loss: 0.58703, Train Auc: 0.7398, Valid Auc: 0.7387\n",
            "Epoch: 051, Train Loss: 0.58407, Train Auc: 0.7260, Valid Auc: 0.7143\n",
            "Epoch: 052, Train Loss: 0.58769, Train Auc: 0.7415, Valid Auc: 0.7262\n",
            "Epoch: 053, Train Loss: 0.58473, Train Auc: 0.7277, Valid Auc: 0.7136\n",
            "Epoch: 054, Train Loss: 0.57701, Train Auc: 0.7369, Valid Auc: 0.7386\n",
            "Epoch: 055, Train Loss: 0.59361, Train Auc: 0.7342, Valid Auc: 0.7301\n",
            "Epoch: 056, Train Loss: 0.58479, Train Auc: 0.7266, Valid Auc: 0.7021\n",
            "Epoch: 057, Train Loss: 0.59162, Train Auc: 0.7423, Valid Auc: 0.7373\n",
            "Epoch: 058, Train Loss: 0.58092, Train Auc: 0.7360, Valid Auc: 0.7154\n",
            "Epoch: 059, Train Loss: 0.59573, Train Auc: 0.7333, Valid Auc: 0.7105\n",
            "Epoch: 060, Train Loss: 0.57339, Train Auc: 0.7423, Valid Auc: 0.7435\n",
            "Epoch: 061, Train Loss: 0.57359, Train Auc: 0.7239, Valid Auc: 0.7130\n",
            "Epoch: 062, Train Loss: 0.58112, Train Auc: 0.7435, Valid Auc: 0.7259\n",
            "Epoch: 063, Train Loss: 0.57997, Train Auc: 0.7451, Valid Auc: 0.7331\n",
            "Epoch: 064, Train Loss: 0.56276, Train Auc: 0.7320, Valid Auc: 0.7162\n",
            "Epoch: 065, Train Loss: 0.58011, Train Auc: 0.7401, Valid Auc: 0.7253\n",
            "Epoch: 066, Train Loss: 0.58705, Train Auc: 0.7449, Valid Auc: 0.7458\n",
            "Epoch: 067, Train Loss: 0.58443, Train Auc: 0.7332, Valid Auc: 0.7221\n",
            "Epoch: 068, Train Loss: 0.58600, Train Auc: 0.7413, Valid Auc: 0.7305\n",
            "Epoch: 069, Train Loss: 0.59258, Train Auc: 0.7382, Valid Auc: 0.7200\n",
            "Epoch: 070, Train Loss: 0.58317, Train Auc: 0.7424, Valid Auc: 0.7416\n",
            "Epoch: 071, Train Loss: 0.57393, Train Auc: 0.7441, Valid Auc: 0.7278\n",
            "Epoch: 072, Train Loss: 0.57650, Train Auc: 0.7484, Valid Auc: 0.7443\n",
            "Epoch: 073, Train Loss: 0.58162, Train Auc: 0.7492, Valid Auc: 0.7234\n",
            "Epoch: 074, Train Loss: 0.57793, Train Auc: 0.7515, Valid Auc: 0.7217\n",
            "Epoch: 075, Train Loss: 0.57954, Train Auc: 0.7540, Valid Auc: 0.7252\n",
            "Epoch: 076, Train Loss: 0.57093, Train Auc: 0.7507, Valid Auc: 0.7344\n",
            "Epoch: 077, Train Loss: 0.56994, Train Auc: 0.7533, Valid Auc: 0.7403\n",
            "Epoch: 078, Train Loss: 0.56888, Train Auc: 0.7483, Valid Auc: 0.7497\n",
            "Epoch: 079, Train Loss: 0.56423, Train Auc: 0.7540, Valid Auc: 0.7433\n",
            "Epoch: 080, Train Loss: 0.57695, Train Auc: 0.7541, Valid Auc: 0.7265\n",
            "Epoch: 081, Train Loss: 0.58208, Train Auc: 0.7559, Valid Auc: 0.7234\n",
            "Epoch: 082, Train Loss: 0.55961, Train Auc: 0.7531, Valid Auc: 0.7383\n",
            "Epoch: 083, Train Loss: 0.57433, Train Auc: 0.7573, Valid Auc: 0.7391\n",
            "Epoch: 084, Train Loss: 0.56979, Train Auc: 0.7418, Valid Auc: 0.7214\n",
            "Epoch: 085, Train Loss: 0.57606, Train Auc: 0.7599, Valid Auc: 0.7412\n",
            "Epoch: 086, Train Loss: 0.57096, Train Auc: 0.7592, Valid Auc: 0.7465\n",
            "Epoch: 087, Train Loss: 0.57613, Train Auc: 0.7599, Valid Auc: 0.7285\n",
            "Epoch: 088, Train Loss: 0.56985, Train Auc: 0.7650, Valid Auc: 0.7469\n",
            "Epoch: 089, Train Loss: 0.56759, Train Auc: 0.7516, Valid Auc: 0.7433\n",
            "Epoch: 090, Train Loss: 0.57078, Train Auc: 0.7261, Valid Auc: 0.7373\n",
            "Epoch: 091, Train Loss: 0.60586, Train Auc: 0.7513, Valid Auc: 0.7280\n",
            "Epoch: 092, Train Loss: 0.55921, Train Auc: 0.7607, Valid Auc: 0.7491\n",
            "Epoch: 093, Train Loss: 0.56443, Train Auc: 0.7620, Valid Auc: 0.7289\n",
            "Epoch: 094, Train Loss: 0.59378, Train Auc: 0.7627, Valid Auc: 0.7416\n",
            "Epoch: 095, Train Loss: 0.58773, Train Auc: 0.7524, Valid Auc: 0.7693\n",
            "Epoch: 096, Train Loss: 0.57289, Train Auc: 0.7608, Valid Auc: 0.7188\n",
            "Epoch: 097, Train Loss: 0.55493, Train Auc: 0.7569, Valid Auc: 0.7394\n",
            "Epoch: 098, Train Loss: 0.55778, Train Auc: 0.7714, Valid Auc: 0.7351\n",
            "Epoch: 099, Train Loss: 0.55431, Train Auc: 0.7703, Valid Auc: 0.7530\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import Javascript\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "learning_rate = 1e-3\n",
        "epoch_num = 100\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
        "        out = model(data.x.to(device), data.edge_index.to(device), data.batch.to(device))  # Perform a single forward pass.\n",
        "        loss = loss_fn(out, data.y.to(device))  # Compute the loss.\n",
        "        total_loss += loss\n",
        "        loss.backward()  # Derive gradients.\n",
        "        optimizer.step()  # Update parameters based on gradients.\n",
        "        optimizer.zero_grad()  # Clear gradients.\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "for epoch in range(epoch_num):\n",
        "    train_loss = train()\n",
        "    train_auroc = graph_test(model, loss_fn, train_loader, device)\n",
        "    valid_auroc = graph_test(model, loss_fn, valid_loader, device)\n",
        "    print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.5f}, Train Auc: {train_auroc:.4f}, Valid Auc: {valid_auroc:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lA1WkJ6jF_2A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8141fb2d-8906-4eaf-c3ce-0faad21927e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved PyTorch Model State to graph_gcn.pth\n"
          ]
        }
      ],
      "source": [
        "torch.save(model.state_dict(), \"graph_gcn.pth\")\n",
        "print(\"Saved PyTorch Model State to graph_gcn.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uz1MHkMMF_2A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35a2c89e-aed9-436d-c60d-8639a2f13c93"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7529920339584351"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "model = GraphGCN().to(device)\n",
        "model.load_state_dict(torch.load(\"graph_gcn.pth\"))\n",
        "graph_test(model, loss_fn, valid_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRnvwbN2F_2B"
      },
      "source": [
        "Utilizing neighborhood normalization reduces the capability of GNNs to differentiate certain graph structures.\n",
        " \n",
        "As an alternative, [Morris et al. (2018)](https://arxiv.org/abs/1810.02244) proposes a formulation that completely eliminates neighborhood normalization and adds a simple skip-connection to the GNN layer to retain central node information:\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_v^{(\\ell+1)} = \\mathbf{W}^{(\\ell + 1)}_1 \\mathbf{x}_v^{(\\ell)} + \\mathbf{W}^{(\\ell + 1)}_2 \\sum_{w \\in \\mathcal{N}(v)} \\mathbf{x}_w^{(\\ell)}\n",
        "$$\n",
        "\n",
        "This layer is available as [`GraphConv`](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GraphConv) in PyTorch Geometric.\n",
        "\n",
        "You can use PyG's `GraphConv` instead of `GCNConv`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4d4izufF_2B"
      },
      "source": [
        "# Task 6: Graph Classification using GraphConv [4 pts]\n",
        "\n",
        "Please design a GNN model based on GraphConv. Complete the following class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYMiNSmDF_2B"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "from torch_geometric.nn import GraphConv\n",
        "### DO NOT CHANGE ANY CODE ABOVE THIS LINE IN THIS CELL ###\n",
        "\n",
        "\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNN, self).__init__()\n",
        "        # TODO: Finish the design of this GNN model\n",
        "        self.conv1 = GraphConv(3, 16)\n",
        "        self.conv2 = GraphConv(16, 32)\n",
        "        self.conv3 = GraphConv(32, 64)\n",
        "        self.conv4 = GraphConv(64, 128)\n",
        "        self.conv5 = GraphConv(128, 256)\n",
        "        self.conv6 = GraphConv(256, 512)\n",
        "        self.conv7 = GraphConv(512, 1024)\n",
        "        self.fc = torch.nn.Linear(1024, 2)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        # TODO: Finish this function\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "        x = self.conv4(x, edge_index)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "        x = self.conv5(x, edge_index)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "        x = self.conv6(x, edge_index)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "        x = self.conv7(x, edge_index)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "        x = global_mean_pool(x, batch)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "######################################################################\n",
        "########## DON'T WRITE ANY CODE OUTSIDE THE CLASS! ###################\n",
        "######## IF YOU WANT TO CALL OR TEST IT CREATE A NEW CELL ############\n",
        "######################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2KqdBBAF_2B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf20b74b-a81c-4d22-930b-0a06b8615857"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GNN(\n",
            "  (conv1): GraphConv(3, 16)\n",
            "  (conv2): GraphConv(16, 32)\n",
            "  (conv3): GraphConv(32, 64)\n",
            "  (conv4): GraphConv(64, 128)\n",
            "  (conv5): GraphConv(128, 256)\n",
            "  (conv6): GraphConv(256, 512)\n",
            "  (conv7): GraphConv(512, 1024)\n",
            "  (fc): Linear(in_features=1024, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = GNN().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSyUX6EgF_2B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "731408ef-01b2-47c3-c314-d44025ca5066"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 000, Train loss: 0.82014, Train Auc: 0.6716, Valid Auc: 0.6239\n",
            "Epoch: 001, Train loss: 0.68709, Train Auc: 0.7082, Valid Auc: 0.6809\n",
            "Epoch: 002, Train loss: 0.68435, Train Auc: 0.7059, Valid Auc: 0.6756\n",
            "Epoch: 003, Train loss: 0.65440, Train Auc: 0.6984, Valid Auc: 0.6435\n",
            "Epoch: 004, Train loss: 0.63366, Train Auc: 0.7189, Valid Auc: 0.6813\n",
            "Epoch: 005, Train loss: 0.64073, Train Auc: 0.7062, Valid Auc: 0.6655\n",
            "Epoch: 006, Train loss: 0.63618, Train Auc: 0.7214, Valid Auc: 0.6890\n",
            "Epoch: 007, Train loss: 0.62251, Train Auc: 0.7382, Valid Auc: 0.6891\n",
            "Epoch: 008, Train loss: 0.59515, Train Auc: 0.7415, Valid Auc: 0.6803\n",
            "Epoch: 009, Train loss: 0.57841, Train Auc: 0.7395, Valid Auc: 0.6975\n",
            "Epoch: 010, Train loss: 0.58263, Train Auc: 0.7448, Valid Auc: 0.6888\n",
            "Epoch: 011, Train loss: 0.59315, Train Auc: 0.7479, Valid Auc: 0.7001\n",
            "Epoch: 012, Train loss: 0.57362, Train Auc: 0.7472, Valid Auc: 0.7028\n",
            "Epoch: 013, Train loss: 0.60935, Train Auc: 0.7389, Valid Auc: 0.6862\n",
            "Epoch: 014, Train loss: 0.58824, Train Auc: 0.7529, Valid Auc: 0.6930\n",
            "Epoch: 015, Train loss: 0.61137, Train Auc: 0.7381, Valid Auc: 0.6865\n",
            "Epoch: 016, Train loss: 0.67872, Train Auc: 0.7382, Valid Auc: 0.6929\n",
            "Epoch: 017, Train loss: 0.60719, Train Auc: 0.7416, Valid Auc: 0.7198\n",
            "Epoch: 018, Train loss: 0.57687, Train Auc: 0.7575, Valid Auc: 0.7142\n",
            "Epoch: 019, Train loss: 0.57542, Train Auc: 0.7638, Valid Auc: 0.7035\n",
            "Epoch: 020, Train loss: 0.57795, Train Auc: 0.7619, Valid Auc: 0.7099\n",
            "Epoch: 021, Train loss: 0.58883, Train Auc: 0.7438, Valid Auc: 0.6857\n",
            "Epoch: 022, Train loss: 0.60491, Train Auc: 0.7421, Valid Auc: 0.7011\n",
            "Epoch: 023, Train loss: 0.58483, Train Auc: 0.7499, Valid Auc: 0.6962\n",
            "Epoch: 024, Train loss: 0.58454, Train Auc: 0.7509, Valid Auc: 0.6820\n",
            "Epoch: 025, Train loss: 0.59794, Train Auc: 0.7690, Valid Auc: 0.7214\n",
            "Epoch: 026, Train loss: 0.55798, Train Auc: 0.7780, Valid Auc: 0.7015\n",
            "Epoch: 027, Train loss: 0.55271, Train Auc: 0.7697, Valid Auc: 0.7107\n",
            "Epoch: 028, Train loss: 0.55570, Train Auc: 0.7635, Valid Auc: 0.6911\n",
            "Epoch: 029, Train loss: 0.58243, Train Auc: 0.7755, Valid Auc: 0.7125\n",
            "Epoch: 030, Train loss: 0.57675, Train Auc: 0.7792, Valid Auc: 0.7044\n",
            "Epoch: 031, Train loss: 0.57597, Train Auc: 0.7742, Valid Auc: 0.7324\n",
            "Epoch: 032, Train loss: 0.59471, Train Auc: 0.7722, Valid Auc: 0.7207\n",
            "Epoch: 033, Train loss: 0.61530, Train Auc: 0.7433, Valid Auc: 0.6881\n",
            "Epoch: 034, Train loss: 0.60475, Train Auc: 0.7787, Valid Auc: 0.7282\n",
            "Epoch: 035, Train loss: 0.55089, Train Auc: 0.7797, Valid Auc: 0.7099\n",
            "Epoch: 036, Train loss: 0.55143, Train Auc: 0.7917, Valid Auc: 0.7195\n",
            "Epoch: 037, Train loss: 0.54314, Train Auc: 0.7878, Valid Auc: 0.7223\n",
            "Epoch: 038, Train loss: 0.55849, Train Auc: 0.7865, Valid Auc: 0.7275\n",
            "Epoch: 039, Train loss: 0.56328, Train Auc: 0.7781, Valid Auc: 0.7009\n",
            "Epoch: 040, Train loss: 0.55634, Train Auc: 0.7756, Valid Auc: 0.7273\n",
            "Epoch: 041, Train loss: 0.56339, Train Auc: 0.7909, Valid Auc: 0.7240\n",
            "Epoch: 042, Train loss: 0.56739, Train Auc: 0.7652, Valid Auc: 0.6859\n",
            "Epoch: 043, Train loss: 0.67306, Train Auc: 0.7598, Valid Auc: 0.6923\n",
            "Epoch: 044, Train loss: 0.58791, Train Auc: 0.7789, Valid Auc: 0.7279\n",
            "Epoch: 045, Train loss: 0.54428, Train Auc: 0.7964, Valid Auc: 0.7129\n",
            "Epoch: 046, Train loss: 0.54642, Train Auc: 0.7964, Valid Auc: 0.7240\n",
            "Epoch: 047, Train loss: 0.55353, Train Auc: 0.8005, Valid Auc: 0.7214\n",
            "Epoch: 048, Train loss: 0.58391, Train Auc: 0.7867, Valid Auc: 0.7338\n",
            "Epoch: 049, Train loss: 0.54897, Train Auc: 0.7971, Valid Auc: 0.7282\n",
            "Epoch: 050, Train loss: 0.53933, Train Auc: 0.8011, Valid Auc: 0.7260\n",
            "Epoch: 051, Train loss: 0.54774, Train Auc: 0.7939, Valid Auc: 0.7250\n",
            "Epoch: 052, Train loss: 0.53849, Train Auc: 0.8029, Valid Auc: 0.7296\n",
            "Epoch: 053, Train loss: 0.53333, Train Auc: 0.7974, Valid Auc: 0.7355\n",
            "Epoch: 054, Train loss: 0.53931, Train Auc: 0.8017, Valid Auc: 0.7345\n",
            "Epoch: 055, Train loss: 0.51918, Train Auc: 0.8083, Valid Auc: 0.7289\n",
            "Epoch: 056, Train loss: 0.52530, Train Auc: 0.7994, Valid Auc: 0.7376\n",
            "Epoch: 057, Train loss: 0.54194, Train Auc: 0.7967, Valid Auc: 0.7105\n",
            "Epoch: 058, Train loss: 0.54140, Train Auc: 0.7987, Valid Auc: 0.7259\n",
            "Epoch: 059, Train loss: 0.55769, Train Auc: 0.8117, Valid Auc: 0.7387\n",
            "Epoch: 060, Train loss: 0.53398, Train Auc: 0.8002, Valid Auc: 0.7361\n",
            "Epoch: 061, Train loss: 0.56576, Train Auc: 0.8131, Valid Auc: 0.7371\n",
            "Epoch: 062, Train loss: 0.59380, Train Auc: 0.7918, Valid Auc: 0.7390\n",
            "Epoch: 063, Train loss: 0.52572, Train Auc: 0.8212, Valid Auc: 0.7167\n",
            "Epoch: 064, Train loss: 0.51502, Train Auc: 0.8113, Valid Auc: 0.7371\n",
            "Epoch: 065, Train loss: 0.53809, Train Auc: 0.8111, Valid Auc: 0.7217\n",
            "Epoch: 066, Train loss: 0.52510, Train Auc: 0.8115, Valid Auc: 0.7350\n",
            "Epoch: 067, Train loss: 0.53488, Train Auc: 0.8165, Valid Auc: 0.7302\n",
            "Epoch: 068, Train loss: 0.53696, Train Auc: 0.8143, Valid Auc: 0.7298\n",
            "Epoch: 069, Train loss: 0.51866, Train Auc: 0.8122, Valid Auc: 0.7270\n",
            "Epoch: 070, Train loss: 0.52240, Train Auc: 0.8174, Valid Auc: 0.7407\n",
            "Epoch: 071, Train loss: 0.51922, Train Auc: 0.8258, Valid Auc: 0.7304\n",
            "Epoch: 072, Train loss: 0.52527, Train Auc: 0.8138, Valid Auc: 0.7262\n",
            "Epoch: 073, Train loss: 0.50901, Train Auc: 0.8153, Valid Auc: 0.7305\n",
            "Epoch: 074, Train loss: 0.54102, Train Auc: 0.8171, Valid Auc: 0.7403\n",
            "Epoch: 075, Train loss: 0.53734, Train Auc: 0.8050, Valid Auc: 0.7136\n",
            "Epoch: 076, Train loss: 0.51069, Train Auc: 0.8226, Valid Auc: 0.7419\n",
            "Epoch: 077, Train loss: 0.50781, Train Auc: 0.8245, Valid Auc: 0.7360\n",
            "Epoch: 078, Train loss: 0.53967, Train Auc: 0.8163, Valid Auc: 0.7331\n",
            "Epoch: 079, Train loss: 0.52667, Train Auc: 0.8183, Valid Auc: 0.7298\n",
            "Epoch: 080, Train loss: 0.53319, Train Auc: 0.8269, Valid Auc: 0.7301\n",
            "Epoch: 081, Train loss: 0.55710, Train Auc: 0.8172, Valid Auc: 0.7387\n",
            "Epoch: 082, Train loss: 0.52137, Train Auc: 0.8195, Valid Auc: 0.7404\n",
            "Epoch: 083, Train loss: 0.51653, Train Auc: 0.8254, Valid Auc: 0.7322\n",
            "Epoch: 084, Train loss: 0.51503, Train Auc: 0.8258, Valid Auc: 0.7334\n",
            "Epoch: 085, Train loss: 0.51109, Train Auc: 0.8294, Valid Auc: 0.7275\n",
            "Epoch: 086, Train loss: 0.52981, Train Auc: 0.8242, Valid Auc: 0.7340\n",
            "Epoch: 087, Train loss: 0.49763, Train Auc: 0.8065, Valid Auc: 0.7070\n",
            "Epoch: 088, Train loss: 0.52988, Train Auc: 0.8043, Valid Auc: 0.7236\n",
            "Epoch: 089, Train loss: 0.50879, Train Auc: 0.8297, Valid Auc: 0.7348\n",
            "Epoch: 090, Train loss: 0.51203, Train Auc: 0.8292, Valid Auc: 0.7263\n",
            "Epoch: 091, Train loss: 0.50088, Train Auc: 0.8315, Valid Auc: 0.7298\n",
            "Epoch: 092, Train loss: 0.49461, Train Auc: 0.8305, Valid Auc: 0.7338\n",
            "Epoch: 093, Train loss: 0.49865, Train Auc: 0.8280, Valid Auc: 0.7371\n",
            "Epoch: 094, Train loss: 0.51593, Train Auc: 0.8317, Valid Auc: 0.7327\n",
            "Epoch: 095, Train loss: 0.49382, Train Auc: 0.8350, Valid Auc: 0.7309\n",
            "Epoch: 096, Train loss: 0.50144, Train Auc: 0.8374, Valid Auc: 0.7417\n",
            "Epoch: 097, Train loss: 0.49968, Train Auc: 0.8297, Valid Auc: 0.7381\n",
            "Epoch: 098, Train loss: 0.51996, Train Auc: 0.8195, Valid Auc: 0.7266\n",
            "Epoch: 099, Train loss: 0.51261, Train Auc: 0.8391, Valid Auc: 0.7298\n",
            "Epoch: 100, Train loss: 0.49764, Train Auc: 0.8258, Valid Auc: 0.7404\n",
            "Epoch: 101, Train loss: 0.51062, Train Auc: 0.8353, Valid Auc: 0.7425\n",
            "Epoch: 102, Train loss: 0.49174, Train Auc: 0.8384, Valid Auc: 0.7399\n",
            "Epoch: 103, Train loss: 0.51859, Train Auc: 0.8154, Valid Auc: 0.7265\n",
            "Epoch: 104, Train loss: 0.52474, Train Auc: 0.8336, Valid Auc: 0.7386\n",
            "Epoch: 105, Train loss: 0.51864, Train Auc: 0.8390, Valid Auc: 0.7324\n",
            "Epoch: 106, Train loss: 0.50560, Train Auc: 0.8182, Valid Auc: 0.7337\n",
            "Epoch: 107, Train loss: 0.50415, Train Auc: 0.8172, Valid Auc: 0.7193\n",
            "Epoch: 108, Train loss: 0.54753, Train Auc: 0.8090, Valid Auc: 0.7158\n",
            "Epoch: 109, Train loss: 0.52691, Train Auc: 0.8333, Valid Auc: 0.7461\n",
            "Epoch: 110, Train loss: 0.51483, Train Auc: 0.8400, Valid Auc: 0.7451\n",
            "Epoch: 111, Train loss: 0.52825, Train Auc: 0.8200, Valid Auc: 0.7371\n",
            "Epoch: 112, Train loss: 0.50312, Train Auc: 0.8320, Valid Auc: 0.7445\n",
            "Epoch: 113, Train loss: 0.48934, Train Auc: 0.8463, Valid Auc: 0.7351\n",
            "Epoch: 114, Train loss: 0.48966, Train Auc: 0.8371, Valid Auc: 0.7451\n",
            "Epoch: 115, Train loss: 0.48332, Train Auc: 0.8473, Valid Auc: 0.7425\n",
            "Epoch: 116, Train loss: 0.50638, Train Auc: 0.8493, Valid Auc: 0.7348\n",
            "Epoch: 117, Train loss: 0.48276, Train Auc: 0.8327, Valid Auc: 0.7478\n",
            "Epoch: 118, Train loss: 0.49114, Train Auc: 0.8463, Valid Auc: 0.7355\n",
            "Epoch: 119, Train loss: 0.49731, Train Auc: 0.8411, Valid Auc: 0.7443\n",
            "Epoch: 120, Train loss: 0.52595, Train Auc: 0.8312, Valid Auc: 0.7425\n",
            "Epoch: 121, Train loss: 0.50785, Train Auc: 0.8340, Valid Auc: 0.7335\n",
            "Epoch: 122, Train loss: 0.50424, Train Auc: 0.8459, Valid Auc: 0.7466\n",
            "Epoch: 123, Train loss: 0.49991, Train Auc: 0.8482, Valid Auc: 0.7435\n",
            "Epoch: 124, Train loss: 0.48598, Train Auc: 0.8490, Valid Auc: 0.7308\n",
            "Epoch: 125, Train loss: 0.48087, Train Auc: 0.8492, Valid Auc: 0.7420\n",
            "Epoch: 126, Train loss: 0.48151, Train Auc: 0.8369, Valid Auc: 0.7416\n",
            "Epoch: 127, Train loss: 0.47742, Train Auc: 0.8522, Valid Auc: 0.7458\n",
            "Epoch: 128, Train loss: 0.48192, Train Auc: 0.8497, Valid Auc: 0.7472\n",
            "Epoch: 129, Train loss: 0.49000, Train Auc: 0.8493, Valid Auc: 0.7477\n",
            "Epoch: 130, Train loss: 0.48389, Train Auc: 0.8505, Valid Auc: 0.7451\n",
            "Epoch: 131, Train loss: 0.48798, Train Auc: 0.8541, Valid Auc: 0.7416\n",
            "Epoch: 132, Train loss: 0.49586, Train Auc: 0.8496, Valid Auc: 0.7288\n",
            "Epoch: 133, Train loss: 0.48012, Train Auc: 0.8515, Valid Auc: 0.7400\n",
            "Epoch: 134, Train loss: 0.47909, Train Auc: 0.8322, Valid Auc: 0.7304\n",
            "Epoch: 135, Train loss: 0.52105, Train Auc: 0.8308, Valid Auc: 0.7377\n",
            "Epoch: 136, Train loss: 0.50457, Train Auc: 0.8419, Valid Auc: 0.7479\n",
            "Epoch: 137, Train loss: 0.50411, Train Auc: 0.8206, Valid Auc: 0.7441\n",
            "Epoch: 138, Train loss: 0.55581, Train Auc: 0.8476, Valid Auc: 0.7562\n",
            "Epoch: 139, Train loss: 0.49065, Train Auc: 0.8502, Valid Auc: 0.7614\n",
            "Epoch: 140, Train loss: 0.48032, Train Auc: 0.8533, Valid Auc: 0.7603\n",
            "Epoch: 141, Train loss: 0.47416, Train Auc: 0.8535, Valid Auc: 0.7441\n",
            "Epoch: 142, Train loss: 0.49079, Train Auc: 0.8592, Valid Auc: 0.7423\n",
            "Epoch: 143, Train loss: 0.48041, Train Auc: 0.8542, Valid Auc: 0.7684\n",
            "Epoch: 144, Train loss: 0.48557, Train Auc: 0.8328, Valid Auc: 0.7485\n",
            "Epoch: 145, Train loss: 0.52586, Train Auc: 0.8544, Valid Auc: 0.7520\n",
            "Epoch: 146, Train loss: 0.48275, Train Auc: 0.8585, Valid Auc: 0.7537\n",
            "Epoch: 147, Train loss: 0.47733, Train Auc: 0.8527, Valid Auc: 0.7497\n",
            "Epoch: 148, Train loss: 0.46233, Train Auc: 0.8427, Valid Auc: 0.7425\n",
            "Epoch: 149, Train loss: 0.48284, Train Auc: 0.8558, Valid Auc: 0.7554\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import Javascript\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "epoch_num = 150\n",
        "learning_rate = 1e-5\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "for epoch in range(epoch_num):\n",
        "    train_loss = train()\n",
        "    train_auroc = graph_test(model, loss_fn, train_loader, device)\n",
        "    valid_auroc = graph_test(model, loss_fn, valid_loader, device)\n",
        "    print(f'Epoch: {epoch:03d}, Train loss: {train_loss:.5f}, Train Auc: {train_auroc:.4f}, Valid Auc: {valid_auroc:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gREo4cNXF_2B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c3f387b-bc43-4f3f-9d95-da6a9b5b98c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved PyTorch Model State to graph_gnn.pth\n"
          ]
        }
      ],
      "source": [
        "torch.save(model.state_dict(), \"graph_gnn.pth\")\n",
        "print(\"Saved PyTorch Model State to graph_gnn.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXS4kLL3F_2B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2494e1b-fc5d-4b03-89ff-2d5ba1fd2919"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7554433941841125"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ],
      "source": [
        "model = GNN().to(device)\n",
        "model.load_state_dict(torch.load(\"graph_gnn.pth\"))\n",
        "graph_test(model, loss_fn, valid_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lC0F8LhLF_2C"
      },
      "source": [
        "## Submission\n",
        "\n",
        "When you have tested your code locally, you can upload your notebook with the saved model files to gradescope to get evaluated.\n",
        "\n",
        "Please submit a zip file to Gradescope that includes the following:\n",
        "\n",
        "1. The `hw3_gnn.ipynb` notebook file.\n",
        "2. The `node_gcn.pth` Node GCN saved model file.\n",
        "3. The `graph_gcn.pth` Graph GCN saved model file.\n",
        "4. The `graph_gnn.pth` Graph GNN saved model file.\n",
        "5. The `weights.txt` file required in Q2.3.\n",
        "6. The `hw3_seq.ipynb` notebook file.\n",
        "7. The `hw3_VAE.ipynb` notebook file.\n",
        "\n",
        "These saved models should have the same structure as the class of model in your notebook.\n",
        "\n",
        "Please make sure that your code runs correctly and that you have included all necessary files in the zip file.\n",
        "Please test your code before submitting to ensure that it runs correctly and all the outputs match the expected results.\n",
        "\n",
        "**IMPORTANT**: Please **do not** include a root directory in the zip file. The required files should be compressed directly."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gb5VhKnVizMQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "new_py_3_8",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}